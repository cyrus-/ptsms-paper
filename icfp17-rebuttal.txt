Thank you for the thoughtful reviews. The main concerns expressed seem to be
regarding the (1) usefulness and (2) novelty of our approach. 

With regard to usefulness, Review A states:

> "Although assessing the usefulness of a language design is difficult,
reporting practical and substantial uses of the extension mechanism would go
far in providing evidence that the language is useful."

We did just that in our prior work at ECOOP 2014, which we referred to as the
"most closely related work" and discussed in Sec. 1 and Sec. 7.  That paper
gave many realistic examples, described two implementations (one that today has
400+ stars on GitHub) and summarized some empirical data gathered from open
source projects. We drew brief attention to these aspects of the prior work in
the second paragraph of Sec. 1, and we would be happy to expand on the text in
the final version to make clear that usefulness concerns were substantially
confronted in prior work.

This paper was written as a "theory paper" that focuses on important semantic
questions and introduces refinements of the mechanism from ECOOP 2014, as
summarized in Sec. 7.

With regard to novelty, note that the first sentence of Sec. 1.3
("Contributions") makes explicit the specific claims of our paper:

> "This work introduces a system of typed syntax macros (TSMs) that gives
library providers substantially more syntactic control than existing typed
term-rewriting macro systems while maintaining the ability to reason abstractly
about types, binding and segmentation."

In the remainder of the rebuttal, we will argue, in view of this statement of
our specific claims, that the paper does in fact address all of the concerns
about novelty raised by the referees. We also respond to a few other concerns,
e.g. about the organization of the sections. We humbly ask that the scores
assigned to our paper be reconsidered.

== Review A ==

> "reminiscent of MacroML" 

MacroML implements staging macros (as stated on pg3). These are quite distinct
from term-rewriting macros and TSMs in that they do not give the macro access
to the syntax of the arguments at all (whether parsed or unparsed.) They cannot
therefore be used as a mechanism for syntactic control. Staging macros are
motivated mainly by concerns about performance, rather than syntactic cost.

> "reminiscent of TemplateHaskell in checking well-formedness after an
extension's free-form generation of ASTs"

While Template Haskell does check that each expansion is well-typed (as any
macro system in a statically typed language eventually must), there is no way
for clients to reason abstractly about *what that type is* because there are no type annotations on Template Haskell macros. 

Furthermore, Template Haskell does not maintain a hygienic binding discipline. 

Because Template Haskell achieves neither of these central goals, we neglected
to mention it in Sec. 1.2 alongside, e.g. Scala's macro system and Rust's macro
system, both of which are more disciplined. However, due to Template Haskell's
prominence in the community, we agree that it would be worth mentioning
explicitly. We say a bit more about Template Haskell and GHC in response to
Review C below.

> "The formalism to check hygiene seems novel, although with precedents in
systems that synthesize hygiene (such as Lee et al., ECOOP'12)."

The hygiene mechanism in Lee et al (2012) is interesting but formally ad hoc,
e.g. it sends queries at run-time to a compiler and reads back the error
messages to determine which variables are free in an encoding of a term. No
theoretical results are established in that work.

> "The paper doesn't describe this parsing approach in one particular section.
The description is partly at the bottom of page 4, and partly at the bottom of
page 18."

Section 2 describes how typed expansion works from start to finish.

One source of confusion may be that the paper only tersely describes the step
just before typed expansion -- context-free parsing. As stated in the paragraph
immediately under Fig. 3, and as suggested by the definition of the body type
in Sec. 2, there is nothing sophisticated about this step -- generalized
literal forms are parsed like raw string literals. Condition 4.1 (pg12)
explains how we formally handle this first step. The context-free syntax for
various generalized literal forms was given in our prior work (we would be
happy to reproduce the grammar in the supplement.)

The assertion that the description of the approach is "partly at the bottom of
page 18" is incorrect: the bottom of Page 18 is the beginning of Sec. 5, which
describes an extension of the core mechanism to handle parameterized families
of types and module parameters. Nothing in Sec. 5 is necessary to understand
prior sections, or the high-level points of the paper.

> "It's not immediately clear whether the segmentation should through colors
(in figures 3 and 4 and footnote 2) can be inferred without running the
extension's implementation, but my current understanding is that it can't.
Spelling out this design in once place and specifying the quoting convention
(which isn't described, as far as I can tell) would improve the presentation."

The splice summary/segmentation mechanism, and how it relates to the coloring
convention, is spelled out in Sec. 2.2. A segmentation cannot be produced if
typed expansion fails (error recovery is mentioned as future work in Sec. 7).

> "Along similar lines, the introduction provides considerable motivation for
the parsing choice, but it's tangled with motivation for types, so drawing out
the parsing strategy more explicitly could help."

Our approach is unabashedly rooted in the type-theoretic/ML tradition, yes, but
we did not make any particular attempt to advocate for types in the
introduction. Readers who see no need to reason statically about types can
apply the usual type theorist's trick of reducing our system to one where there
is just one type that classifies every well-scoped expression. The concerns
about responsibility, segmentation, context independence and capture would
still be relevant in that setting.

> "The paper's claim of a "sweet spot" seems to be supported only by the
authors' opinions on how a language should behave."

The informal phrase "sweet spot" appears appropriately in quotes, and only at
the end of the abstract and conclusion. The phrase was chosen because TSMs
avoid the problematic extremes of both term-rewriting macros and general syntax
definition systems, as described in Sec. 1. It is perhaps just an opinion
that being able to reason abstractly about programs is a good thing, but this
is not a controversial position in the ICFP community.

== Review B ==

> "The work is well presented and technically sound; my primary concern is on
usability particularly on composable syntaxes and complex situations where two
syntaxes might use the same symbols for starting/internal workings and the
conflict might be confusing to users of both."

TSMs can be applied within spliced terms without conflict (e.g. see the
footnote on page 4, the discussion about final expansion in Sec. 2, and also
several more complex examples in Omar et al, 2014).

It is not clear to us what is meant here by "two syntaxes might use the same
symbols for starting/internal workings". This concern is perhaps related to the
discussion in the paper on "Responsibility" -- in which case note that the
applied TSM unambiguously identifies which syntax is in use within each literal
form. Of course, different TSMs might implement similar looking syntactic
forms, but any semantic confusion that results can be resolved by carefully
following the reasoning principles detailed in the paper. Ultimately, one
cannot enforce "good taste" by purely technical means.

== Review C ==

> "However, the system presented is not novel in the ways the authors suggest.
Most significantly, it is very similar to [...] the camlp4 metaprogramming
system"

camlp4 was cited in Sec. 1.1 and suffers from *all* of the problems described
therein (conflict + no way to reason about responsibility, segmentation,
capture, context dependence or typing.)

> "Considering GHC quasiquotes, they also feature a single fixed delimiter,
arbitrary parsers, explicit invocation, and typing. Because Template Haskell,
on which quasiquotation is built, is more powerful than the system presented
here, they do not have context independence or capture properties, but a system
without those particular features would would have the relevant properties."

GHC quasiquotes address only the problems of conflict and responsibility. The
use of fixed outer delimiters to handle conflict is not claimed as a novel
contribution and clearly attributed to prior work at the top of page 4, where
we discuss strings and the work of Slind 1991; also Omar et al, 2014.  The fact
that responsibility is handled by explicit invocation is also clearly
attributed to existing macro systems in Sec. 1.2 (point 1). 

The fact that unhygienic macro systems, like Template Haskell / GHC
quasiquotes, can arbitrarily parse terms out of string literal bodies is
explicitly acknowledged in Sec. 1.2 (second paragraph on p4). The problem, to
reiterate, is (1) that no segmentation principles are available and (2) that
this approach is incompatible with hygiene constraints that are necessary to
"reason abstractly about ... binding". If you added standard hygiene
constraints to Template Haskell, it would no longer be able to support
arbitrary parsers for the reasons discussed on page 4. With regard to typing,
see the response to Review A above.

TSMs are novel not for each feature individually, but because you need not make
trade-offs between the mentioned reasoning principles.

> "Segmentation is, in the present system, an out-of-band communication to the
editor system, and could be so adapted for any extensible syntax system."

To be able to extract an accurate segmentation from an expansion, the origin of
each spliced term in the expansion must be retained. A standard representation
of ASTs (without the Spliced constructors discussed in Sec. 2, which are novel)
would not retain this information because an arbitrary function sits between
the input string and the output AST.

> "Additionally, the idea of directly generating an AST, rather than a term to
be further expanded, and using this to build DSLs, goes back at least to
Shriram Krishnamurthi's thesis (see the concept of "micros"). 

Direct AST generation emerged as the "default" solution in our development in
that adding support for TSM application within proto-expansions would have
required significant additional machinery. Krishnamurthi's work on micros is
focused on term-to-term translation from the source language to an IR encoding,
and macro expansion in the generated translation does eventually occur.

> "The approach to hygiene, which uses an auxiliary environment which maps
symbols (here called identifiers) to variables, is familiar from the literature
on hygenic macros in the Scheme community (this is known as a "renaming" in
that literature; see for example [Dybvig 91]). That this approach enforces
hygiene is proved for a significantly more expressive macro system by [Herman
and Wand]."

Our mechanism for identifier expansion is not specified in the same way as
Dybvig91 and Herman and Wand (2008, 2010) -- in our formalism, renaming is
*implicitly* necessary to discharge disjointness premises in the typed
expansion and proto-expansion validation rules we give (see Harper, 2012,
Chapters 1-2 on abstract binding trees, and Gabbay and Pitts 2001). Relying on
disjointness premises and pushing the act of renaming into the more general ABT
mechanism via an implicit identification convention a la Harper significantly
simplifies our formalism relative to approaches that require explicitly
specifying how unique names are generated during expansion. It also allows us
to state the capture avoidance conditions (in Theorems 4.5 and 5.1) in terms of
the standard capture-avoiding substitution operation on ABTs. To our knowledge,
this ABT-based approach has not been explored before.

It is not clear what is meant here by "more expressive macro system". Herman
and Wand's system, like other hygienic term-rewriting systems, does not support
parsing terms out from string arguments, as already discussed. There are some
similarities between our spliced segment references and the "tree locations"
used in their work (which originate in Gorn, 1967), in that tree locations
identify sub-terms by the location they appear in the argument list, rather
than more generally in our system by their syntactic location in the literal
body.

> "The parameterization is a somewhat odd feature. In the paper, it seems to
mostly address the overly-strong hygiene restriction, by allowing the macro to
generate references to identifiers from the module signature that is
parameterized over. However, this seems a round-about way of recovering what is
essentially lexical scope: macros should be able to reference names in their
environment, and those references should be stable when expanded. The
parameterization is independently useful, however, as the DICT syntax example
shows."

This sort of lexical scope is not technically possible for macros that operate
on non-primitive AST representations, like proto_expr, because variable
references in the expansion are merely pieces of data of type var_t (which
could very well just be a synonym for string or nat if using a de Bruijn-style
encoding.) These values could result from arbitrary computations.
Parameterization solves this problem without needing to privilege term
representations primitively. It also makes it clear which bindings are going to
be accessed by the computed expansion without examining the implementation
directly, which is in following with our goal of making it easy to reason
abstractly about binding.

> "In this setting, the paper follows [Culpepper, Owens, Flatt; GPCE 2005] very
closely (see also Owens' thesis)."

The work by Culpepper, Owens and Flatt, and by Owens in his thesis, on
packaging macros into components/units is more closely related to the brief
discussion in Sec. 6.4 (second paragraph) than to Sec. 5, and we will cite it
there. Sec. 5 would correspond to parameterizing macros over units of a
particular signature, which was not considered in their prior work.

> "The claims of the "Syntax Dialects are Unreasonable" section seems to ignore
the long history of success of the Lisp community in this area."

It is not clear what specific efforts in the Lisp community this is referring
to. Lisp has had notable success with term-rewriting macros, as discussed in
Sec. 1.2. However, a Lisp macro given a string literal would have exactly the
same problems as the Rust example described in that section. There have been
some efforts to support arbitrary (non-S-expression) syntax, e.g. with reader
macros / Racket's #lang mechanism, but these amount to syntax definition
systems that suffer from the problems described in Sec. 1.1.

> "p3:44 the systems listed here are not all "typed" in the same sense. Herman
and Wand's types, for example, describe the scoping of the resulting program,
not its type."

Yes, that's right. That point wasn't stated as clearly as it could have been.

> "Would splices be more simply represented by providing an opaque type of
strings with a "substring" operation?"

One way to guarantee that the splices are all in-bounds would indeed be to
package together body and proto_exp as abstract types and allow the Spliced
constructors to arise internally only via a checked substring operation on the
provided body. A related mechanism was used in Omar et al, 2014. However, this
would not guarantee that segments are non-overlapping so a check by the
semantics would still be needed. Simply defining body as a string and including
the bounds checks in along with the other checks is semantically simpler.
Providers who favor the abstract approach can choose to use it, since parse
functions are ML code.

> "p7:41 this type should be `string`, not `html`"

Yes, thanks.

> "The discussion of `do` notation on p8 is confused. `do` notation, and
generation of binding forms more generally, is not prohibited here by
capture-avoidance, but because (I infer, the full AST is not provided) there's
no way to take a splice and put it in the binding position of a `lam` AST."

This possibility is mentioned at p24:37. Shape types ala Herman and Wand would
be related to this mechanism. Absent such a mechanism, it is in fact capture
avoidance that prevents capture by spliced terms of variables bound in the
expansion.

> "p10:32 how would you address capture in a system which did feature binding
in patterns that could be subsequently referenced?"

The approach taken for expression TSMs should work in this situation too.

> "p24:34 This "backpatching" is done in many Lisp IDEs, see DrRacket for an
example [Florence et al, Language Workbench 2016]"

We will mention that in the future work, thanks.

----

> "Since these systems have quite different purposes, comparing them produces
nothing meaningful."

We organized our comparison by technical mechanism (syntax definition vs. term-rewriting), not by "purpose". A substantially more detailed comparison to related work, spanning 20 pages, is available in my PhD thesis [1] but obviously, this level of detail would not be appropriate for a conference paper.

> "there are two quite distinct endeavours to consider"

Comparing mechanisms of syntactic control in terms of reasoning principles, rather than simpler "expressability" concerns, is the major emphasis of our paper. From that perspective, the two proposed classes of examples are not distinct. 

In the paper, we did distinguish examples, like do-notation, that introduce bindings into sub-terms. TSMs as defined in the paper do not support these examples, though a simple modification could be made to add support. But we suggest that even if such examples were possible, it may not be worth it for the increased reasoning complexity that clients would have to contend with (because each spliced term would be associated not just with a type, but also with a list of bindings -- that is much harder to convey using, e.g., an editor.)

> "For example, can TSMs handle the SQL binding constructs such as 'AS'?"

Encodings of SQL queries, including those with 'AS' clauses, can be expressed using TSMs. If one were to somehow express the binding structure of SQL 'AS' clauses using the binding structure of the host language (a HOAS-style approach), it would not be possible -- see above.

> "the only way to determine the parsing is to expand the syntax and find
the 'spliced' sections, and a reader looking at a fragment of unexpanded
source can't tell how to parse identifiers"

The paper clearly states that the locations of spliced segments is communicated using secondary notation by the editor / pretty-printer.

> "Hygiene (capture and context dependence) is a significant part of this
work, and I'd like to see a deeper comparison between your design and
existing work on hygiene (e.g.. Dave Herman's "A Theory of Typed Hygienic
Macros")."

We state on p4 that existing hygiene mechanisms prevent splicing terms out of string literals. Our mechanism does not have that problem.

Technically, our approach to specifying hygiene is also different in that it uses an ABT-generic capture-avoiding substitution mechanism, rather than an explicit renaming approach. The rebuttal to the other reviews details this distinction.

> "I'm troubled by the lack of complete examples."

The rebuttal to the other reviews addresses this concern -- this paper is a "theory paper" that serves as a companion to our prior paper (ECOOP 2014), which did give several more examples. We did not have enough space to give a complete parser for a non-trivial syntax.

> "Since TSMs generate untyped parse trees that are type-checked on expansion
then I would be reluctant to describe them as "typed"."

We call them "typed" because type annotations serve to allow clients to reason abstractly about types.

The verification-validation distinction, which we also discussed, is important, but is not equivalent to "typed vs untyped".

> "A short English summary alongside each reasoning principle (Theorems 4.5,
4.6, 5.1) would make them much more accessible to the casual reader."

There is in fact an English summary alongside each reasoning principle in Theorem 4.5. The correspond clauses would have nearly identical descriptions for the other theorems, so we did not duplicate them for space reasons.

[1] http://www.cs.cmu.edu/~comar/omar-thesis.pdf

