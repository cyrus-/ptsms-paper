Thank you for the thoughtful reviews. We were pleased to see that no
significant technical problems were found during the review process. The main
doubts that seem to remain relate to (1) usefulness and (2) novelty.

With regard to usefulness, Review B states:

"Although assessing the usefulness of a language design is difficult, reporting
practical and substantial uses of the extension mechanism would go far in
providing evidence that the language is useful."

We agree! So much so that we made just such a case for usefulness in our
closely related work a few years ago (Omar et al, ECOOP 2014). That paper gave
more sophisticated examples and even gathered empirical data from open source
projects. The present paper is intended to be a follow-on "theory paper" that
contributes more detailed formal results and develops various functional
variants of the mechanism as was detailed in Sec. 7. Such a paper should be
well within the scope of ICFP.

With regard to novelty, the first sentence of Sec. 1.3 ("Contributions") makes
explicit the specific claims of our paper:

"This work introduces a system of typed syntax macros (TSMs) that gives library
providers substantially more syntactic control than existing typed
term-rewriting macro systems while maintaining the ability to reason abstractly
about types, binding and segmentation."

In the remainder of the rebuttal, we will argue, in view of these specific
claims, that the paper does in fact address all of the major concerns about
novelty raised by the referees. We humbly ask that the scores assigned to our
paper be reconsidered in this light.

== Review A ==

> "reminiscent of MacroML" 

MacroML implements staging macros (stated on pg3). These are quite distinct
from term-rewriting macros and TSMs in that they do not give the macro access
to the syntax of the arguments at all (whether parsed or unparsed.) They cannot
therefore be used as a mechanism for syntactic control -- staging macro
invocation syntactically coincides with function application. Staging macros
instead serve as a mechanism for staged evaluation, primarily for reasons of
performance. This is unrelated to our stated goals.

> "reminiscent of TemplateHaskell in checking well-formedness after an
extension's free-form generation of ASTs"

While Template Haskell does check that each expansion is well-typed (as any
macro system in a statically typed language eventually must), there is no way
for clients to reason abstractly about *what that type is* because there are no
annotations on Template Haskell macros. 

Furthermore, Template Haskell does not maintain any sort of hygienic binding
discipline. 

Being that Template Haskell achieves neither of these central goals, and that there
are a number of other languages that have broadly similar support for
compile-time code generation, we neglected to mention it in Sec. 1.2 alongside,
e.g. Scala's macro system, which is more disciplined. However, due to Template
Haskell's popularity in the community, we agree that it would be worth
mentioning explicitly. We say a bit more about Template Haskell in response to 
Review C below.

> "The formalism to check hygiene seems novel, although with precedents in
systems that synthesize hygiene (such as Lee et al., ECOOP'12)."

The hygiene mechanism in Lee et al (2012) is quite different, e.g. it sends
queries at run-time to a compiler and reads back the error messages to
determine which variables are free in an encoding of a term. No theoretical
results whatsoever are established in that work.

> "The paper doesn't describe this parsing approach in one particular section.
The description is partly at the bottom of page 4, and partly at the bottom of
page 18."

Section 2 seems to be exactly the "one particular section" that is sought: it
gives a thorough description of how typed expansion operates from start to
finish. 

One source of confusion may be that the paper only tersely describes how,
before typed expansion, the initial context-free parsing of generalized literal
forms works (though see the paragraph immediately under Fig. 3, and Sec. 2's
discussion of the body type.) There is nothing sophisticated about this step --
generalized literal forms are parsed like raw string literals. Our prior paper
showed examples of various other delimiters and gave a context free grammar.
Condition 4.1 (pg12) explains how we formally handle this first step.

The assertion that the description of the approach is "partly at the bottom of
page 18" is incorrect: the bottom of Page 18 is the beginning of Sec. 5, which
describes an extension of the core mechanism to handle parameterized families
of types and module parameters. Nothing in Sec. 5 is necessary to understand
prior sections, or the main conceptual points of the paper.

> "It's not immediately clear whether the segmentation should through colors
(in figures 3 and 4 and footnote 2) can be inferred without running the
extension's implementation, but my current understanding is that it can't.
Spelling out this design in once place and specifying the quoting convention
(which isn't described, as far as I can tell) would improve the presentation."

The splice summary/segmentation/antiquotation mechanism, and how it relates to
the colorings shown, is in fact spelled out in Sec. 2.2 (in particular, in the
final 3 paragraphs).

> "Along similar lines, the introduction provides considerable motivation for
the parsing choice, but it's tangled with motivation for types, so drawing out
the parsing strategy more explicitly could help."

By our reckoning, we gave little to no "motivation for types" in Sec. 1. Types
are brought up briefly in the fifth point of "Problem 2" in Sec. 1.1, and in
the third point in Sec. 1.2. Given that our research is focused on languages in
the ML tradition, and our technical contributions do ultimately relate to
types, we cannot eliminate statements about types from the introduction
completely. We could perhaps mention that you can recover an "untyped" variant
of TSMs by applying the usual type theorist's trick of defining just one type
that classifies every well-scoped expression. The concerns about
responsibility, segmentation, context independence and capture would still be
relevant in that setting.

> "The paper's claim of a "sweet spot" seems to be supported only by the
authors' opinions on how a language should behave."

The phrase "sweet spot" (which appears exclusively in quotes, and only at the
end of the abstract and conclusion) is informal. That said, the phrase was
chosen because TSMs sit between the well-studied extremes described in Sec. 1.

== Review B ==

> "The work is well presented and technically sound; my primary concern is on
usability particularly on composable syntaxes and complex situations where two
syntaxes might use the same symbols for starting/internal workings and the
conflict might be confusing to users of both."

TSMs can be applied within spliced terms without conflict (e.g. see the
footnote on page 4, the discussion about final expansion in Sec. 2, and also
many more complex examples in Omar et al, 2014).

It is not clear to us what is meant here by "two syntaxes might use the same
symbols for starting/internal workings". This concern is perhaps related to the
discussion in the paper on "Responsibility" -- in which case note that the
applied TSM unambiguously identifies which syntax is in use within each literal
form. Of course, different TSMs might implement similar looking syntactic
forms, but any semantic confusion that results can be resolved by carefully
following the reasoning principles detailed in the paper. Ultimately, one
cannot enforce "good taste" by purely technical means.

== Review C ==

> "However, the system presented is not novel in the ways the authors suggest.
Most significantly, it is very similar to [...] the camlp4 metaprogramming
system"

camlp4 was cited in Sec. 1.1 and suffers from *all* of the problems described
therein (conflict + no way to reason about responsibility, segmentation,
capture, context dependence or typing.)

> "Considering GHC quasiquotes, they also feature a single fixed delimiter,
arbitrary parsers, explicit invocation, and typing. Because Template Haskell,
on which quasiquotation is built, is more powerful than the system presented
here, they do not have context independence or capture properties, but a system
without those particular features would would have the relevant properties."

GHC quasiquotes address only the problems of conflict and responsibility. The
use of fixed outer delimiters to handle conflict is explicitly disclaimed as a
novel contribution at the top of page 4, where we discuss strings and the work
of Slind 1991; also Omar et al, 2014. The fact that responsibility is handled
by explicit invocation is also clearly attributed to existing macro systems in
Sec. 1.2 (point 1). 

The fact that unhygienic macro systems, like Template Haskell / GHC
quasiquotes, can arbitrarily parse terms out of string literal bodies is
explicitly acknowledged in Sec. 1.2 (second paragraph on p4). The problem, to
reiterate, is (1) that no segmentation principles are available and (2) that
this approach is incompatible with hygiene constraints that are necessary to
"reason abstractly about ... binding". If you added standard hygiene
constraints to Template Haskell, it would no longer be able to support
arbitrary parsers for the reasons discussed on page 4. With regard to typing,
see the response to Review A above.

TSMs are novel not for each feature individually, but because you need not make
trade-offs between these features.

> "Segmentation is, in the present system, an out-of-band communication to the
editor system, and could be so adapted for any extensible syntax system."

To be able to extract an accurate segmentation from an expansion, the origin of
each spliced term in the expansion must be retained. A standard representation
of ASTs (without the Spliced constructors discussed in Sec. 2, which are novel)
would not retain this information because an arbitrary function sits between
the input string and the output AST.

> "Additionally, the idea of directly generating an AST, rather than a term to
be further expanded, and using this to build DSLs, goes back at least to
Shriram Krishnamurthi's thesis (see the concept of "micros"). 

Direct AST generation emerged as the "default" solution in our development in
that adding support for TSM application within proto-expansions would have
required significant additional machinery. Krishnamurthi's work on micros is
focused on term-to-term translation from the source language to an IR encoding, 
and macro expansion in the generated translation does eventually occur.

> "The approach to hygiene, which uses an auxiliary environment which maps
symbols (here called identifiers) to variables, is familiar from the literature
on hygenic macros in the Scheme community (this is known as a "renaming" in
that literature; see for example [Dybvig 91]). That this approach enforces
hygiene is proved for a significantly more expressive macro system by [Herman
and Wand]."

Our mechanism for identifier expansion does not operate quite like both
Dybvig91 and Herman and Wand (2008, 2010) describe -- in our work, renaming is
*implicitly* necessary to discharge disjointness premises in the typed
expansion and proto-expansion validation rules we give (see Harper, 2012,
Chapters 1-2 on abstract binding trees, and Gabbay and Pitts 2001). Relying on
disjointness premises and pushing the act of renaming into the more general ABT
mechanism via an implicit identification convention a la Harper significantly
simplifies our formalism relative to approaches that require explicitly
specifying how unique names are generated during expansion. It also allows us
to state the capture avoidance conditions (in Theorems 4.5 and 5.1) in terms of
the standard capture-avoiding substitution operation on ABTs. To our knowledge,
this ABT-based approach has not been explored before.

It is not clear what is meant here by "more expressive macro system". Herman
and Wand's system, like other hygienic term-rewriting systems, does not support
parsing terms out from string arguments, as already discussed. There are some
similarities between our spliced segment references and the "tree locations"
used in their work (which originate in Gorn, 1967), in that tree locations
identify sub-terms by the location they appear in the argument list, rather
than more generally in our system by their syntactic location in the literal
body.

> "The parameterization is a somewhat odd feature. In the paper, it seems to
mostly address the overly-strong hygiene restriction, by allowing the macro to
generate references to identifiers from the module signature that is
parameterized over. However, this seems a round-about way of recovering what is
essentially lexical scope: macros should be able to reference names in their
environment, and those references should be stable when expanded. The
parameterization is independently useful, however, as the DICT syntax example
shows."

This sort of lexical scope is not technically possible for macros that operate
on non-primitive AST representations, like proto_expr, because variable
references in the expansion are merely pieces of data of type var_t (which
could very well just be a synonym for string or nat if using a de Bruijn-style
encoding.) These values could result from arbitrary computations.
Parameterization solves this problem without needing to privilege term
representations primitively. It also makes it clear which bindings are going to
be accessed by the computed expansion without examining the implementation
directly, which is in following with our goal of making it easy to reason
abstractly about binding.

"In this setting, the paper follows [Culpepper, Owens, Flatt; GPCE 2005] very
closely (see also Owens' thesis)."

The work by Culpepper, Owens and Flatt, and by Owens in his thesis, on
packaging macros into components/units is more closely related to the brief
discussion in Sec. 6.4 (second paragraph) than to Sec. 5, and we will cite it
there. Sec. 5 would correspond to parameterizing macros over units of a
particular signature, which was not considered in their prior work.

> "The claims of the "Syntax Dialects are Unreasonable" section seems to ignore
the long history of success of the Lisp community in this area."

It is not clear what specific efforts in the Lisp community this is referring
to. Lisp has had notable success with term-rewriting macros, as discussed in
Sec. 1.2. However, a Lisp macro given a string literal would have exactly the
same problems as the Rust example described in that section. There have been
some efforts to support arbitrary (non-S-expression) syntax, e.g. with reader
macros / Racket's #lang mechanism, but these amount to syntax definition
systems that suffer from some or all of the problems described in Sec. 1.1.

> "p3:44 the systems listed here are not all "typed" in the same sense. Herman
and Wand's types, for example, describe the scoping of the resulting program,
not its type."

Yes, that's right. That point wasn't stated as clearly as it could have been.

> "Would splices be more simply represented by providing an opaque type of
strings with a "substring" operation?"

One way to guarantee that the splices are all in-bounds would indeed be to
package together body and proto_exp as abstract types and allow the Spliced
constructors to arise internally only via a checked substring operation on the
provided body. A related mechanism was used in Omar et al, 2014. However, this
would not guarantee that segments are non-overlapping so a check by the
semantics would still be needed. Simply defining body as a string and including
the bounds checks in along with the other checks is semantically simpler.
Providers who favor the abstract approach can choose to use it, since parse
functions are ML code.

> "p7:41 this type should be `string`, not `html`"

Yes, thanks.

> "The discussion of `do` notation on p8 is confused. `do` notation, and
generation of binding forms more generally, is not prohibited here by
capture-avoidance, but because (I infer, the full AST is not provided) there's
no way to take a splice and put it in the binding position of a `lam` AST."

This possibility is mentioned at p24:37. Shape types ala Herman and Wand would
be related to this mechanism. Absent such a mechanism, it is in fact capture
avoidance that prevents capture by spliced terms of variables bound in the
expansion.

* "p10:32 how would you address capture in a system which did feature binding
  in patterns that could be subsequently referenced?"

This would be addressed exactly as it is with expression TSMs.

> "p24:34 This "backpatching" is done in many Lisp IDEs, see DrRacket for an
example [Florence et al, Language Workbench 2016]"

I will mention that in the future work, thanks.

