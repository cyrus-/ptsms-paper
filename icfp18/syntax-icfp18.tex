\PassOptionsToPackage{svgnames,dvipsnames,svgnames}{xcolor}
%% For double-blind review submission
% \documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=false,printccs=false,printacmref=false}
\documentclass[acmsmall]{acmart}
% \documentclass[sigplan,review,10pt,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
% \documentclass[acmsmall,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission
%\documentclass[acmlarge,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission
%\documentclass[acmlarge]{acmart}\settopmatter{}

%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format should change 'acmlarge' to
%% 'sigplan,10pt'.


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

%% Cyrus packages
\usepackage{microtype}
\usepackage{todonotes}
\usepackage{mdframed}
\usepackage{colortab}
\usepackage{mathpartir}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{wasysym}
\definecolor{light-gray}{gray}{0.95}

% \newtheorem{theorem}{Theorem}[chapter]
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \newtheorem{condition}[theorem]{Condition}

% \usepackage{titlesec}
% \titlespacing{\paragraph}{0pt}{10pt}{10pt}

% \usepackage{titlesec}

% \titlespacing*\section{0pt}{1pt plus 2pt minus 2pt}{2pt plus 2pt minus 2pt}
% \titlespacing*\subsection{0pt}{1pt plus 2pt minus 2pt}{2pt plus 2pt minus 2pt}
% \titlespacing*\subsubsection{0pt}{1pt plus 2pt minus 2pt}{2pt plus 2pt minus 2pt}
% \titlespacing*\paragraph{0pt}{5pt plus 2pt minus 2pt}{2pt plus 2pt minus 2pt}


\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{1ex \@plus .2ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother

% \usepackage{titlesec}
% \titleformat{\subsubsection}[runin]{\large\bfseries}{\thesubsubsection}{1em}{}

\makeatletter
\renewcommand{\subsubsection}{%
  \@startsection{subsubsection}{3}%
  {\z@}{1ex \@plus .2ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother

%   \usepackage{titlesec}

% \titleformat*{\subsubsection}{\large\bfseries}

%% Listings
\definecolor{mygray}{rgb}{0.75,0.75,0.75}
\usepackage{listings}
\lstset{tabsize=2, 
basicstyle=\ttfamily\fontsize{8pt}{1em}\selectfont, 
commentstyle=\itshape\ttfamily\color{gray},
stringstyle=\ttfamily\color{purple},
mathescape=false,escapeinside={(~}{~)},
numbers=left, numberstyle=\scriptsize\color{mygray}, language=ML,showspaces=false,showstringspaces=false,xleftmargin=0pt, numbersep=-6pt, morekeywords=[1]{try,spliced,tyfam,opfam,let,fn,val,def,casetype,objtype,metadata,of,*,string,lexer,parser,package,datatype,new,toast,notation,module,switch,where,dependencies,import,for,ana,syn,opcon,tycon,metasignature,metamodule,metasig,metamod,static,at,by,tycase,mod,macro,match,pattern,in,patterns,expressions,implicit,forall,int,exptsm,pattsm},deletekeywords={double,structure,of},classoffset=0, xleftmargin=0pt, 
aboveskip=3pt,belowskip=2pt,
moredelim=**[is][\color{red}]{SSTR}{ESTR},
moredelim=**[is][\color{Periwinkle}]{SHTML}{EHTML},
moredelim=**[is][\color{purple}]{SCSS}{ECSS},
moredelim=**[is][\color{brown}]{SSQL}{ESQL},
moredelim=**[is][\color{orange}]{SCOLOR}{ECOLOR},
moredelim=**[is][\color{magenta}]{SPCT}{EPCT}, 
moredelim=**[is][\color{gray}]{SNAT}{ENAT}, 
moredelim=**[is][\color{Periwinkle}]{SURL}{EURL},
moredelim=**[is][\color{Sealavender}]{SQT}{EQT},
moredelim=**[is][\color{Periwinkle}]{SGRM}{EGRM},
moredelim=**[is][\color{Yellowlavender}]{SID}{EID},
moredelim=**[is][\color{Sepia}]{SUS}{EUS},
moredelim=**[is][\color{black}]{SOK}{EOK},
% deletestring=[d]{"},
}
\lstset{morecomment=[n]{/*}{*/}}

\newcommand{\liv}[1]{\lstinline{#1}}
\newcommand{\li}[1]{\lstinline[basicstyle=\ttfamily\fontsize{9pt}{1em}\selectfont]{#1}}
\newcommand{\lismall}[1]{\lstinline[basicstyle=\ttfamily\fontsize{9pt}{1em}\selectfont]{#1}}
\newcommand{\lifootnote}[1]{\lstinline[basicstyle=\ttfamily\fontsize{7pt}{1em}\selectfont]{#1}}


\makeatletter\if@ACM@journal\makeatother
%% Journal information (used by PACMPL format)
%% Supplied to authors by publisher for camera-ready submission
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{1}
\acmArticle{1}
\acmYear{2018}
\acmMonth{9}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\else\makeatother
%% Conference information (used by SIGPLAN proceedings format)
%% Supplied to authors by publisher for camera-ready submission
\acmConference[ICFP'18]{International Conference on Functional Programming}{September 23--29, 2018}{St. Louis, MO, USA}
\acmYear{2018}
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\fi


%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission
% \setcopyright{none}             %% For review submission
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\copyrightyear{2017}           %% If different from \acmYear


%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations

\input{thesis/macros}

\setlength{\abovecaptionskip}{4pt plus 3pt minus 2pt} % Chosen fairly arbitrarily
\setlength{\belowcaptionskip}{-2pt plus 3pt minus 2pt} % Chosen fairly arbitrarily

\begin{document}

%% Title information
\title[Reasonably Programmable Literal Notation]{Reasonably Programmable Literal Notation}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
% \titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
% \subtitle{Subtitle}                     %% \subtitle is optional
% \subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{Cyrus Omar}
\authornote{The majority of this research was performed while the first author was at Carnegie Mellon University.}          %% \authornote is optional;
                                        %% can be repeated if necessary
\affiliation{
  \institution{University of Chicago}            %% \institution is required
  \city{Chicago}
  \state{IL}
  % \country{USA}
}
\email{comar@cs.uchicago.edu}          %% \email is recommended
% \affiliation{
%   \institution{Carnegie Mellon University}           %% \institution is required
%   \city{Pittsburgh}
%   \state{PA}
%   % \country{USA}
% }

%% Author with two affiliations and emails.
\author{Jonathan Aldrich}
\affiliation{
  \institution{Carnegie Mellon University}           %% \institution is required
  \city{Pittsburgh}
  \state{PA}
  % \country{USA}
}
\email{jonathan.aldrich@cs.cmu.edu}         %% \email is recommended


%% Paper note
%% The \thanks command may be used to create a "paper note" ---
%% similar to a title note or an author note, but not explicitly
%% associated with a particular element.  It will appear immediately
%% above the permission/copyright statement.
% \thanks{with paper note}                %% \thanks is optional
                                        %% can be repeated if necesary
                                        %% contents suppressed with 'anonymous'


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
%Many languages define convenient list literal forms. This paper questions this common practice: why should an otherwise ordinary data structure be privileged in the definition of a general-purpose programming language? 
General-purpose programming languages typically define literal notation for only a small number of common data structures, like lists. This is unsatisfying because there are many other data structures for which literal notation might be useful, e.g. finite maps, regular expressions, HTML elements, SQL queries, syntax trees for various languages and chemical structures. There may also be different implementations of each of these data structures behind a common interface that could all benefit from common literal notation. 
%A less \emph{ad hoc} design would be one that allows library providers to define new literal forms in a decentralized manner. However, the mechanisms available to programmers today, e.g. Camlp4 \cite{ocaml-manual} and Sugar* \cite{erdweg2011sugarj,erdweg2013framework}, are \emph{unreasonable}. In particular, they do not support modular reasoning about syntactic determinism, so separately defined literal forms can and do conflict syntactically with one another. Moreover, clients cannot reason abstractly about types and binding when they encounter an unfamiliar literal form (like they can when they encounter an unfamiliar function being applied.)% Instead, they must reason transparently, i.e. about the underlying expansion. This increases cognitive cost, defeating much of the purpose of convenient literal forms. %This makes it difficult to program ``in the large'', and these mechanisms have not been widely adopted.% Hygienic, typed term-rewriting systems, e.g. the Scala macro system \cite{ScalaMacros2013}, are only somewhat more reasonable, and do not offer direct syntactic control.
This paper introduces \emph{typed literal macros (TLMs)}, which allow library providers to define new literal notation of nearly arbitrary design at any specified type or parameterized family of types. 
% TLMs give library providers programmatic control over the parsing and expansion of expressions and patterns of a flexible \emph{generalized literal form} at {a} specified type or parameteric family of types. % The mechanism is {strictly hygienic}, meaning that 1) the expansion must be context-independent; and 2)  splicing is capture avoiding. 
% Partial parameter application lowers the syntactic cost of this strict style. 
Compared to existing approaches, TLMs are uniquely \emph{reasonable}. TLM clients can reason abstractly, i.e. without examining grammars or generated expansions, about types and binding. The system only needs to convey to clients, via secondary notation, the inferred \emph{segmentation} of each literal body, which gives the locations and types of spliced subterms. TLM providers can reason modularly about syntactic ambiguity and expansion correctness according to clear criteria. This paper incorporates TLMs into Reason, an emerging alternative front-end for OCaml, and demonstrates, through several non-trivial case studies, how TLMs integrate with the advanced features of OCaml, including pattern matching and the module system. We also discuss optional integration with MetaOCaml, which allows TLM providers to be more confident about type correctness. Finally, we establish these abstract reasoning principles formally with a detailed type-theoretic account of expression and pattern TLMs for ``core ML''. 
%This is a hygienic term-rewriting macro system, of any design, for a language with a ML-like type structure and pattern matching. 
%Maintaining information about segmentation is necessary for the novel TLM hygiene mechanism, i.e. the mechanism that ensures that clients can reason abstractly about binding (previously, only unhygienic macro systems supported splicing of terms out of string literal bodies.) 
%The system can determine this {segmentation} automatically and convey it to the client via syntax highlighting, or by presenting the client with a context-free grammar (without revealing the semantic actions associated with each production.) 
% This mechanism captures many common syntactic idioms while avoiding the problem of syntactic conflicts by construction and supplying clients with clear abstract reasoning principles. 
% This calculus is the first such formalization of a typed macro system.

% \emph{Typed syntax matcros (TLMs)}, proposed in a recent short paper by Omar et al. \cite{sac15}, give library providers programmatic control over the parsing and expansion of only terms of {(generalized) literal form}. This appears to occupy a ``sweet spot'' in that it captures many common syntactic idioms while avoiding the problem of conflict. TLMs also maintain a reasonable type and binding discipline.% In particular, clients can use any combination of TLMs in a program without needing to consider conflicts between them, and the language validates each expansion that a TLM generates to maintain 1) a \emph{type discipline} (meaning clients can determine the type of an unexpanded expression without examining its expansion directly); 
% %and 2) a \emph{hygienic binding discipline}.
% %, meaning that the expansion cannot make any assumptions about bindings at the application site, nor  introduce ``hidden bindings'' into subterms. 

% TLMs have only been described minimally -- it is not clear how they should be adapted for integration into languages like ML that support {pattern matching}, {parameterized datatypes}, {modules} and abstract types. Moreover, the prior work makes several simplifying assumptions related to binding that are impractically restrictive.% bno mechanism for binding values for use across TLM definitions has been described and the hygiene mechanism makes giving the expansions that they generate access to ``helper functions'' awkward.

% This paper gives a complete account of TLMs that addresses these deficiencies. In particular, we 1) integrate TLMs with pattern matching; 2) introduce a distinct static phase of evaluation, which gives TLM definitions access to libraries; and 3) introduce type and module parameters, which serve two purposes: they allow for TLMs that operate uniformly at a parameterized family of types (rather than only at a single type), and they give expansions explicit, hygienic access to libraries. Support for partial application  of parameters lowers the syntactic cost of this explicit approach. 

% Put succinctly, we design a programming language in the ML tradition with a \emph{reasonably} programmable syntax.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008.10011009.10011019</concept_id>
<concept_desc>Software and its engineering~Extensible languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011050.10011055</concept_id>
<concept_desc>Software and its engineering~Macro languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Extensible languages}
\ccsdesc[500]{Software and its engineering~Macro languages}


%% Keywords
%% comma separated list
% \keywords{keyword1, keyword2, keyword3}  %% \keywords is optional


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle

\section{Introduction}
\label{sec:intro}
When designing the surface syntax of a general-purpose programming language, it is common practice to define shorthand \emph{literal notation}, i.e. notation that decreases the syntactic cost of constructing and pattern matching over values of some particular data structure or parameterized family of data structures. For example, many languages in the ML family support list literals like \li{[x1, x2, x3]} in both expression and pattern position \cite{harper1997programming,mthm97-for-dart}. 
%\footnote{In SML, list literals work even in contexts where the list expression constructors have been shadowed by other bindings, i.e. the meaning of literal forms is \emph{context independent}. 
% We will return to this concept again later.} 
Lists are common across problem domains, but other literal notation is more specialized. For instance, Ur/Web extends the surface syntax of Ur (an ML-like language \cite{conf/pldi/Chlipala10}) with expression and pattern literals for encodings of XML and HTML data~\cite{conf/popl/Chlipala15}. For example, Fig.~\ref{fig:urweb} shows two Ur/Web HTML literals, one that ``splices in'' a string expression delimited by \li{\{[} and \li{]\}} and the other an HTML expression delimited by \li{\{} and \li{\}}.
%The desugarings of these literal expressions, not shown, are substantially more verbose and less readable to programmers familiar with XHTML. 

\begin{figure}[h]
\vspace{-9px}
\begin{lstlisting}
  fun heading first_name = SURL<xml><h1>Hello, {[EURLfirst_nameSURL]}!</h1></xml>EURL
  val body = SURL<xml><body>{EURLheading "World"SURL} ...</body></xml>EURL
\end{lstlisting}
\vspace{-3px}
\caption{HTML literals with support for splicing at two types are built primitively into Ur/Web \cite{conf/popl/Chlipala15}.}
\label{fig:urweb}
\vspace{-10px}
\end{figure}

This design practice, where the language designer privileges certain library constructs with built-in literal notation, is \emph{ad hoc} in that it is easy to come up with other examples of data structures for which mathematicians, scientists or programmers have invented specialized notation \cite{DBLP:journals/cacm/Iverson80,cajori1928history,TSLs}. For example, (1) clients of a ``collections'' library might want not just list literals, but also literal notation for matrices, finite maps, and so on; (2) clients of a ``web programming'' library might want CSS literals (which Ur/Web lacks); (3) a compiler author might want ``quotation'' literals for the terms of the object language and various intermediate languages of interest; and (4) clients of a ``chemistry'' library might want chemical structure literals based on the SMILES standard \cite{anderson1987smiles}.

Although requests for specialized literal notation are easy to dismiss as superficial, the reality is that literal notation, or the absence thereof, can have a substantial influence on software quality. For example, \citet{Bravenboer:2007:PIA:1289971.1289975} finds that literal notation for structured encodings of queries, like the SQL-based query literals now found in many languages \cite{meijer2006linq}, reduce the temptation to use string encodings of queries and therefore reduce the risk of catastrophic string injection attacks~\cite{owasp2017}. More generally, evidence suggests that programmers frequently resort to ``stringly-typed programming'', i.e. they choose strings instead of composite data structures, largely for reasons of notational convenience. In particular, \citet{TSLs} sampled strings from open source projects and found that at least 15\% of them could be parsed by some readily apparent type-specific grammar, e.g. for URLs, paths, regular expressions and many others. 
Literal notation, with support for splicing, would decrease the syntactic cost of composite encodings, which are more amenable to programmatic manipulation and compositional reasoning than string encodings. %Literal forms would  preferable to string encodings because they support compositional construction and decomposition by pattern matching via splicing.



Of course, it would not scale to ask general-purpose language designers to build in support for all known notations \emph{a priori}. Instead, there has been persistent interest in mechanisms that allow library providers to define new literal notation on their own. For example, direct grammar extension systems like Camlp4 \cite{de2003camlp4} and Sugar* \cite{erdweg2011sugarj,erdweg2013framework}, term rewriting systems like Template Haskell~\cite{SheardPeytonJones:Haskell-02,mainland2007s}, the system of \emph{type-specific languages} (TSLs) described by \citet{TSLs}, and other systems that we will discuss below can all be used to define new literal notation (and, in some cases, other forms of new notation, such as new infix operator forms, control flow operations or type declaration forms, which we leave beyond the scope of this paper). 

\paragraph{Problem} The problem that specifically motivates this paper is that these existing systems  make it difficult or impossible to reason abstractly about such fundamental issues as types and variable binding when presented with a program using user-defined literal forms. Instead, programmers and editor services can only reason transparently, i.e. by inspecting the underlying expansion or the implementation details of the collection of extensions responsible for producing the expansion.% In some existing systems, even determining which syntax extension is responsible for a literal form is difficult.
%, i.e. when developing large programs ``consisting of many small programs o(modules), possibly written by different people''
\begin{figure}[t!]
\vspace{4px}
\begin{subfigure}[t]{0.46\textwidth}
\fcolorbox{gray!20}{gray!20}{\makebox[0.97\textwidth][l]{\textsc{existing grammar extension systems}}}
\begin{lstlisting}[xleftmargin=-2pt, morekeywords={EXTEND}]
  EXTEND /* loaded by, e.g., camlp4 */
    expr:
      |  "`(" q = kquery ")`" -> q
    kquery: 
      | /* ...K query grammar... */
  /* ...more extensions defined... */
  let x = compute_x();
  let y = `((!R)@&{&/x!/:2_!x}'!R)`;
\end{lstlisting}
\vspace{-5px}
\caption{It is difficult to reason abstractly given program text that uses a variety of grammar extensions (see the six reasoning criteria in the text).}
\label{fig:K-dialect}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.51\textwidth}
\fcolorbox{gray!20}{gray!20}{\makebox[\textwidth][l]{\textsc{\textbf{this paper}: typed literal macros}}}
\begin{lstlisting}[xleftmargin=-2pt]
  notation $kq at KQuery.t {
    lexer  KQueryLexer
    parser KQueryParser.start
    in package kquery_parser;
    dependencies = {module KQuery = KQuery}
  }; /* ...more notations defined... */
  let x = compute_x();
  let y = $kq `(SURL(!R)@&{&/EURLxSURL!/:2_!EURLxSURL}'!REURL)`;
\end{lstlisting}
\vspace{-5px}
\caption{TLMs make examples like these more reasonable by leaving the base grammar fixed and strictly enforcing a simple type, binding and segmentation discipline.}
\label{fig:K-tsm-example}
\end{subfigure}
\vspace{2px}
\caption{Two of the possible ways to introduce literal notation for encodings of K queries}
\vspace{-4px}
\end{figure}

Consider, for instance, the perspective of a programmer trying to comprehend the program text in Fig.~\ref{fig:K-dialect}, which is written in an emerging dialect of OCaml's surface syntax called Reason \cite{reason-what} that has, hypothetically, been extended with some number of new literal forms by a grammar extension system --- Lines 1-6 outline the Camlp4 mechanism \cite{de2003camlp4}, but Sugar*/SugarHaskell is similar \cite{erdweg2011sugarj,erdweg2012layout}. 
Line 8 uses one of these active syntax extensions to construct an encoding of a query in the rather obscure database query language K, using its intentionally terse notation~\cite{Whitney:2001:LOR:376284.375783}. The problem is that a programmer examining the program as presented, and unfamiliar with (i.e. holding abstract) the details elided on Lines 1-6, cannot easily answer questions like the following:

\begin{enumerate}[leftmargin=15px]
\item \textbf{Responsibility}: Which syntax extension determined the expansion of the literal on Line~8? Might activating a new extension generate a conflicting expansion for the same literal?
\item \textbf{Expansion Typing}: What type does the expansion, and thus the variable \li{y} on Line 8, have?
\item \textbf{Context Dependence}: Which bindings does the expansion of Line~8 invisibly depend on? If we shadow or remove a module or other binding, could that break or change the meaning of Line~8 because its expansion depends invisibly on the original binding?
\item \textbf{Segmentation}: Are the characters \li{x}, \li{R} and \li{2} on Line~8 parsed as spliced expressions, meaning that they appear directly in the underlying expansion, or are they parsed in some other way peculiar to this literal notation, e.g. as operators in the K query language?
\item \textbf{Segment Typing}: What type is each spliced term expected to have? How can we infer a type for a variable that appears in a spliced term without looking at where it ends up in the expansion?
\item \textbf{Capture}: If \li{x} is in fact a spliced term, does it refer to the binding of \li{x} on Line 7, or might it capture an invisible binding of the same identifier in the expansion of Line 8?
\end{enumerate}

% In short, the problem is that library providers cannot reason modularly about syntactic determinism (because other extensions might define overlapping forms), and if the desugaring of the program text is held abstract, programmers can no longer reason about types, binding and segmentation (i.e. answer questions like those above.) This is burdensome at all scales, but particularly when programming in the large, where it is common to encounter a wide variety of libraries \cite{DBLP:conf/sac/LammelPS11}. 
Forcing the programmer to reason transparently to answer basic questions like these defeats the ultimate purpose of syntactic sugar: decreasing cognitive cost \cite{Green89}. Analagous problems do not arise when programming without syntax extensions in languages like ML --- programmers can reason lexically about where variables and other symbols are bound, and types mediate abstraction over function and module implementations \cite{B304}. Ideally, the programmer would be able to abstract in some analagous manner over the implementation of an unfamiliar notation. % From this perspective, it is unsurprising that the use of Camlp4 has been deprecated in the OCaml system.

Given these issues, we concluded that direct grammar extension systems like \li{camlp4} were not ideally suited for integration into the Reason platform, which seeks to develop a clear and modern surface syntax for the OCaml programming language \cite{reason-what}. We also evaluated various approaches that are based not on direct grammar extension but on term rewriting over a fixed grammar. We give a full account of this evaluation in Sec.~\ref{sec:existing-approaches}, but briefly, we found that:
\begin{itemize}[leftmargin=15px]
\item Unhygienic approaches like OCaml's preprocessor extension point (PPX) rewriters \cite{ocaml-manual} and Template Haskell \cite{SheardPeytonJones:Haskell-02,mainland2007s} allow us to define new literal notation with support for splicing by repurposing existing string literal forms. They also partially or completely solve the problem of reasoning about \textbf{Responsibility} but they do not satisfy the remaining five reasoning criteria.
% \item Hygienic staging macro systems, like MetaOCaml and related systems \cite{DBLP:conf/flops/Kiselyov14,ganz2001macros,Sheard:1999:UMS} are unsuitable for defining literal notation because they do not give the expander access to syntax trees.%, i.e. the rewriting is parametric in the arguments, so they are unsuitable for defining new notation.  
\item Hygienic term rewriting macro systems, like those in various Lisp-family languages, 
% \cite{DBLP:conf/esop/HermanW08,Herman10:Theory,Kohlbecker86a,DBLP:conf/popl/Adams15,DBLP:conf/popl/ClingerR91,DBLP:journals/lisp/DybvigHB92,mccarthy1978history,Flatt:2012:CLR:2063176.2063195}
e.g. Racket \cite{Flatt:2012:CLR:2063176.2063195}, as well as Scala \cite{ScalaMacros2013}, do not allow us to flexibly repurpose string literal forms to define composite literal forms because the hygiene discipline cannot account for base language terms spliced out of string literal bodies via parsing (see Sec.~\ref{sec:existing-approaches} for more details). %(the system cannot identify these terms as sub-terms of the macro arguments \cite{DBLP:conf/esop/HermanW08,Herman10:Theory}).% Only existing composite forms can therefore be repurposed.
\item \emph{Type-specific languages (TSLs)} \cite{TSLs} come closer to our goals in that they explicitly support splicing terms out of literal bodies, but the mechanism falls subtly short with regard to the six reasoning principles just discussed in ways that we detail in Sec. \ref{sec:existing-approaches}. In any case, this approach was designed for simple nominally-typed languages and relies critically on a particular local type inference scheme. It is not immediately suitable for a language with an ML-like semantics, meaning a language with support for structural types (like tuple and function types in ML), parameterized type families, pattern matching and non-local type inference.
\end{itemize}

% Based on this evaluation, we concluded that the existing approaches were not ideally suited for integration into Reason, an increasingly popular open source project that seeks to provide a more clear and practical surface syntax for the OCaml programming language.

\paragraph{Contributions} This paper introduces \emph{typed literal macros} (TLMs): the first system for defining new literal notation that (1) provides the ability to reason abstractly about all six of the topics just outlined; and (2) is semantically expressive enough for integration into Reason/OCaml and other full-scale statically typed functional languages. We evaluate these claims with a number of non-trivial examples appearing throughout the paper that involve the advanced language features mentioned above. In describing these examples, we demonstrate that literal parsing logic can be defined using standard, unmodified parser generators, so the burden on notation providers is comparable to that of existing systems despite these stronger reasoning principles.  Finally, we give a type-theoretic account of TLMs where we formally establish these abstract reasoning principles. % From the client's perspective, which is the focus of our contributions, TLMs are uniquely reasonable. 

%Unique to our approach in this paper is the focus, both formal and informal, on maintaining client-side abstract reasoning principles.

% The empirical study in the prior work on TSLs \cite{TSLs} supports the claim that a macro-based mechanism for defining new literal notation is broadly useful, so This paper is targeted at language designers that seek a deeper understanding of the reasoning principles at play, particularly as they relate to features common to ML and other full-scale statically typed languages (e.g. Haskell, Scala, Rust, etc.)

\paragraph{A Brief Overview} For a brief overview of the proposed mechanism, consider Fig.~\ref{fig:K-tsm-example}. Lines~1-6 define a TLM named \li{$kq} that provides K query literal notation. Line 8 applies this TLM to express the example from Fig. \ref{fig:K-dialect}. We can reason abstractly about this program as follows.
\begin{enumerate}[leftmargin=12pt]
\item \textbf{Responsibility}: The lexer and parser specified by the applied TLM on Lines 2-3 are together exclusively responsible for lexing, parsing and expanding the body of the generalized literal form, i.e. the characters between \li{`(} and \li{)`}. We will give more details on generalized literal forms and on constructing a TLM lexer and parser in the next section. For now, let us simply reiterate that our design goal is to provide a system where the programmer does not normally need to look up the definitions of \li{KQueryLexer} and \li{KQueryParser} to reason about types and binding. % To briefly summarize, our system uses Menhir, which is the most popular parser generator in the OCaml ecosystem and integrated into its standard build tools. %the context-free grammar of the language is fixed, so the TLM provider can modularly establish that the notation that the TLM implements is unambiguous (even in situations where a spliced term itself applies another TLM, which we will show in a later example).
\item \textbf{Expansion Typing}: The type annotation on the definition of \li{$kq} (Line 1) determines the type that the expansion must have, here \li{KQuery.t}.
\item \textbf{Context Dependence}: Lines 4-5 specify that expansions generated by \li{$kq} are allowed to use the module \li{KQuery}, and no others. The system ensures that this dependency is  bound as specified even if the variable \li{KQuery} has been shadowed at the application site. This completely relieves clients from needing to consider expansion-internal dependencies when naming variables. %For example, if the client renames \li{w}, there is not even a remote possibility that this could break or change the meaning of Line 9.
\item \textbf{Segmentation}: The intermediate output that the TLM generates is structured so that the system can infer from it an accurate \emph{segmentation} of the literal body that distinguishes spliced terms, i.e. those that appear in the expansion, from segments  parsed in some other way. The segmentation is all that needs to be communicated to language services downstream of the expander, e.g. editors and pretty printers, which can pass it on to the programmer using secondary notation, e.g. colors in this document. So by examining Line 9, the programmer knows that the two instances of \li{x} are spliced expressions (because they are in black), whereas the \li{R}'s must be parsed in some other way, e.g. as operators of the K language (because they are in lavender). This also implies that errors in spliced terms can always be reported in terms of their original location.
\item \textbf{Segment Typing}: Each spliced segment in the inferred segmentation also has  a type annotation. This, together with the context independence condition, ensures that type inference at the TLM application site can be performed (by editor services or in the programmer's mind) abstractly, i.e. by reference only to the type annotations on the spliced segments, not the full expansion.  %This information is usually not necessary to reason about typing, but it can be conveyed to the programmer upon request by the program editor if desired. %TLM definitions follow the usual scoping rules, so it is easy to ``jump to the definition'' of \li{$kq}.
\item \textbf{Capture}: Splicing is guaranteed to be capture-avoiding, so the spliced expression \li{x} must refer to the binding of \li{x} on Line 2. It cannot have captured a coincidental binding of \li{x} in the expansion. % We will say more about capture later on.
\end{enumerate}

% We will see examples of TLMs that make use of the more advanced features of the OCaml type system -- notably, pattern matching, parameterized types and modules -- later.

%\footnote{In general, spliced expressions might themselves apply TLMs, in which case the convention is to use a distinct color for unspliced segments at each depth. For example, if strings were not primitive, we might write \li{`SURL<body>\{EURLheading ($str "SSTRWorld!ESTR")SURL\} ...</body>EURL`}.}




\paragraph{Paper Outline} 
% \begin{itemize}
% \item 
Sec.~\ref{sec:setlms} details expression TLMs in Reason with several more case studies of varying detail, notably including TLMs for regular expressions, Ur/Web style HTML literals, chemical structure literals and quasiquotation for Reason language terms, which can be used for implementing other TLMs. It also describes experimental integration with MetaOCaml, which can help providers reason about type correctness.
% \item 
Sec.~\ref{sec:sptsms} then briefly introduces {pattern TLMs} and describes the special reasoning conditions in pattern position. %The hygiene condition for pattern TLMs is interesting.
% \item 
Sec.~\ref{sec:ptsms} introduces the more general {parametric TLMs}, which allow us to define literal notation at a type- or module-parameterized family of types. This section also reveals that the \li{dependencies} clause can be understood as parameterization followed immediately by partial parameter application, or, alternatively, that parameterization can be understood in terms of the \li{dependencies} clause together with ML's module functions (functors). Having introduced the basic machinery by example, we proceed in 
%Sec.~\ref{sec:more-examples} with examples that further demonstrate the expressive power of this approach and various parser implementation strategies. Notably, this section gives our take on the example from  Fig.~\ref{fig:urweb} of Ur/Web-style HTML literals as well as a TLM that implements quasiquotation for Reason language terms, which is useful for implementing other TLMs. 
Sec.~\ref{sec:setlms-formally} to define a type-theoretic account of simple expression and pattern TLMs and formally establish the reasoning principles implied above in their essential form. 
% Sec.~\ref{sec:ptlms-formally} adds type functions and an ML-style module system to this calculus, and gives a more general variant of the reasoning principles theorem. 
Sec.~\ref{sec:implementation} provides a brief overview of how we are implementing TLMs for Reason without modifying OCaml's type system. % We also give a brief overview of optional integration with MetaOCaml, which can be used to guarantee that all expansion generated by a TLM are type correct. 
% \item 
Sec.~\ref{sec:existing-approaches} compares TLMs to related work, guided by the rubric of  reasoning principles just discussed. 
% \item 
Sec.~\ref{sec:discussion} concludes with a summary of the contributions of this paper and a discussion of limitations and future work.
Certain technical details, proofs and additional details on the case studies are available in the supplement. % Also available is an additional series of case studies: notation for an encoding of regular expressions as an abstract data type, implemented using TLMs for quasiquotation and grammar-based parser generators. %are also available in the supplement. Sec.~\ref{sec:static-eval} considers the topic of term evaluation during the  expansion phase, i.e. \emph{static evaluation}, in  more detail. Sec \ref{sec:static-eval} also gives examples of TLMs that are useful for defining other TLMs, e.g. TLMs that implement parser generators and quasiquotation. 

%  We say more about LISP macros in Sec. \ref{sec:existing-approaches}.% For the present purposes, we can consider Reason as essentially the calculus defined in the supplement extended with various conveniences that are commonly found in other ML-like languages and, notionally, orthogonal to TLMs. %Reason is, as its name suggests, a conceptual descendent of ML. It diverges from other dialects of ML that have a similar type structure in that it has a bidirectional type system \cite{Pierce:2000:LTI:345099.345100} (like, for example, Scala \cite{OdeZenZen01}) for reasons that have to do with the mechanism of TLM implicits described in Chapters \ref{chap:tsls} and \ref{chap:ptsms}. 
%The reason we will not follow Standard ML \cite{mthm97-for-dart} in giving a complete formal definition of Reason in this work is both to emphasize that the primitives we introduce are ``insensitive'' to the details of the underlying type structure of the language (so TLMs can be considered for inclusion in a variety of languages, not only dialects of ML), and to avoid distracting the reader (and the author) with definitions that are already well-understood in the literature and that are orthogonal to those that are the focus of this work. 
% We will not formally define these features mainly to avoid unnecessarily complicating our presentation with details that are not essential to the ideas presented herein. As such, 
% All examples written in Reason should be understood to be informal motivating material for the subsequent formal material. %We anticipate that future full-scale language specifications will be able to combine the ideas  in the proposed work without trouble. %The purpose of the work being proposed is to serve as a reference for those interested in the new constructs we introduce, not to serve as a language specification. 
%We will give a brief overview of these languages are organized in Sec. \ref{sec:Reason}.




% \subsection{Contributions}\label{sec:contributions}
% This work introduces a system of \textbf{typed literal macros (TLMs)} that gives library providers substantially more syntactic control than existing typed term-rewriting macro systems while maintaining the ability to reason abstractly about types, binding and segmentation.% abstract reasoning principles. % comparable to the level of control they have when defining a syntax dialect.

% Clients apply TLMs to \emph{generalized literal forms}. 

% Because the context-free syntax is never extended, syntactic conflicts are not a concern.

%As such, the semantics can take the type and binding structure of the surrounding program into account when validating the expansion that the TLM programmatically generates to ensure that clients can answer critical questions related to types and binding, like those enumerated in Section \ref{sec:abs-reasoning-intro}. Clients need not have knowledge of the implementation of the TLM or of the generated expansion, i.e. there are useful principles of syntactic abstraction.

% The primary technical challenge has to do with the fact that the applied TLM needs to be able to splice terms out of the literal body for inclusion in the expansion. For example, 

 % We design our mechanism such that these locations can easily be determined from the output of the TLM. This is essential for our hygiene mechanism, and it is also useful in that this information can be presented to the user (e.g. as shown in Figure \ref{fig:first-tsm-example-marked}). %As such, we must develop a mechanism where 1) the positions of spliced subterms can be determined without examining the macro implementation (e.g. so that they can be presented to the user differently by an editor or pretty-printer, ;  and 2) the hygiene mechanism must give only portions of the expansion that correspond to these spliced subterms access to the application site context. 



%In order to reason about types and binding, client programmers need only have knowledge of 1) the segmentation (e.g. by examining a figure like this presented by a code editor or pretty-printer) and 2) a type annotation on the definition of the applied TLM. No other details about the applied TLM's implementation or the expansion that it generates need to be revealed to the client programmer. In other words, 


% \begin{figure}[h]
% \begin{lstlisting}
% PElement Nil Seq(
%   TextNode "Hello, ", 
%   Seq(TextNode (join(" ", Cons(first, Cons(second, Nil)))), 
%   TextNode "!"))
% \end{lstlisting}
% \caption{The desugaring.}
% \end{figure}


% There is also no ambiguity with regard to which TLM has control over each form, and searching for the definition of a TLM is no more difficult than searching for any other binding, i.e. there are well-defined scoping rules.

% In other words, TLMs maintain a useful notion of syntactic abstraction. %More specifically, TLMs maintain a \emph{hygienic binding discipline}, meaning that questions Questions 4 and 5 above were concerned with are disallowed entirely. 
% We will, of course, make this notion more technically precise as we continue.


% \begin{figure}[h]
% \begin{lstlisting}[numbers=none,xleftmargin=0px]
% let syntax $strlist = $list string in 
% $html `SURL<p>Hello, {[EURLjoin ($str ' ') ($strlist [firstSURL,EURL last])SURL]}</p>EURL`
% \end{lstlisting} 
% \caption{The example from Figure \ref{fig:first-tsm-example-marked}, expressed using a parametric TLM.}
% \label{fig:first-ptsm-example-marked}
% \end{figure}

% In Secs. \ref{chap:uetsms} and \ref{chap:uptsms}, we assume for the sake of technical simplicity that each TLM definition is self-contained, needing no access to libraries or to other TLMs. This is an impractical assumption in practice. We relax this assumption in In 

% %\item \textbf{Type-specific languages}, or \textbf{TSLs}. TSLs, described 
% In Chapter \ref{chap:tsls}, we develop a mechanism of \emph{TLM implicits} that allows library clients to contextually designate, for any type, a privileged TLM at that type. The semantics applies this privileged TLM implicitly to unadorned literal forms that appear where a term of the associated type is expected. For example, if we designate 
% %\li{$str} 
% as the privileged TLM at the \li{string} type and 
% %\li{$strlist}
% as the privileged TLM at the \li{list(string)} type, we can express the example from Figure \ref{fig:first-tsm-example-marked} instead as shown in Figure \ref{fig:first-tsm-example-implicit} (assuming \li{join} has type \li{string -> list(string) -> string}.) 
% \begin{figure}[h]
% \begin{lstlisting}[numbers=none]
% $html`SURL<p>Hello, {[EURLjoin ' ' [firstSURL,EURL last]SURL]}</p>EURL`
% \end{lstlisting}
% \caption{The example from Figure \ref{fig:first-tsm-example-marked} drawn to take advantage of TLM implicits.}
% \label{fig:first-tsm-example-implicit}
% \end{figure}

% \noindent This approach is competitive in cost with library-specific syntax dialects (e.g. compare Figure \ref{fig:first-tsm-example-implicit} to Figure \ref{fig:urweb}), while maintaining the abstract reasoning principles characteristic of our approach.
%\item \textbf{Metamodules}, introduced in Sec. \ref{sec:metamodules}, reduce the need to primitively build in the type structure of constructs like records (and variants thereof),  labeled sums and other interesting constructs that we will introduce later by giving library providers programmatic ``hooks'' directly into the semantics, which are specified as a \emph{type-directed translation semantics} targeting a small \emph{typed internal language} (introduced in Sec. \ref{sec:Reason}). %For example, a library provider can implement the type structure of records with a metamodule that:
%\begin{enumerate}
%\item introduces a type constructor, \lstinline{record}, parameterized by finite mappings from labels to types, and defines, programmatically, a translation to unary and binary product types (which are built in to the internal language); and 
%\item introduces operators used to work with records, minimally record introduction and elimination (but perhaps also various functional update operators), and directly implements the logic governing their typechecking and translation to the IL (which builds in only nullary and binary products). 
%\end{enumerate}
%We will see direct analogies between ML-style modules (which our mechanisms also support) and metamodules later.
%\end{enumerate} 


% As vehicles for this work, we will define a small programming language in each of the three parts just mentioned, each building conceptually upon the previous language. All of our formal contributions are relative to these small languages.


%TLMs, like other macro systems, perform \emph{static code generation} (also sometimes called \emph{static} or \emph{compile-time metaprogramming}), meaning that the relevant rules in the static semantics of the language call for the evaluation of \emph{static functions} that generate term encodings. Static functions are functions that are evaluated statically, i.e. during typing. %Library providers write these static functions using the Reason \emph{static language} (SL).  
%Maintaining a separation between the static (or ``compile-time'') phase and the dynamic (or ``run-time'') phase is an important facet of Reason's design. % static code generation. %We will  also introduce a simple variant of each of these primitives that leverages Reason's support for local type inference to further reduce syntactic cost in certain common situations. 

\section{Expression Literals}
\label{sec:setlms}

Consider the recursive datatype \li{Regex.t} defined by Fig.~\ref{fig:Regex-module-def}, which encodes {regular expressions} (regexes) into Reason \cite{Thompson:1968:PTR:363347.363387}. Regexes are  common in, for example, bioinformatics, where they are used to express patterns in DNA sequences. We can construct a regex that matches the strings \li{"A"}, \li{"T"}, \li{"G"} or \li{"C"}, which represent the four bases in DNA, as follows:
\begin{lstlisting}[numbers=none]
  let any_base = Regex.(Or(Str "A", Or(Str "T", Or(Str "G", Or(Str "C")))))
\end{lstlisting}
Note that in Reason, the notation \li{Regex.(}$e$\li{)} locally opens the module \li{Regex} within the expression $e$, so we do not need to qualify each application of \li{Regex.Or} and \li{Regex.Str}. Even with the aid of this shorthand, however, constructing regexes in this way is syntactically costly. Instead, we would like to have the option to use the common POSIX-style notation  \cite{STD95954} when constructing values of type \li{Regex.t}, including values constructed compositionally from other regexes and strings. 
We solve this problem in Fig.~\ref{fig:regex-tlm-def} by defining a TLM named \li{$regex} (pronounced ``lit regex'') that supports POSIX-style regex notation extended with splice forms for regexes and strings. Fig.~\ref{fig:first-tlm-example} shows three examples of \li{$regex} being applied.
% One approach would be to simply define a function \li{parse_rx : string -> Regex.t} that parses a string at run-time to produce a regex. This approach works for simple expressions like the one above -- we can write \li{parse_rx "A|T|G|C"}. However, this approach breaks down when one needs to construct regexes compositionally. For example, the DNA pattern that the BisI restriction enzyme recognizes is \li{AT}$x$\li{GC}, where $x$ is any of the four DNA bases. However, one cannot pass \li{any_base} to the string concatenation operator, because \li{any_base} is already of type \li{Regex.t}. These kinds of situations cause programmers to engage in \emph{stringly-typed programming}, as discussed in Sec.~\ref{sec:intro} -- they simply use strings to encode regexes throughout their application, rather than structured encodings like \li{Regex.t}. This can be catastrophic if the programmer is not extremely careful with string concatenation because strings that happen to contain special characters recognized by the POSIX notation can be misinterpreted because they are not distinguished in any way from strings that intentionally use POSIX notation. When some of these strings come from untrusted sources, e.g. web forms as is common in bioinformatics applications, the result can be a breach in privacy. 





% Moreover, there already exists a broadly adopted shorthand notational standard -- the POSIX standard for regular expressions \cite{STD95954}. For the purposes of this section, we will stick to the subset of the POSIX standard that applies to the encoding in Fig.~\ref{fig:Regex-module-def}, but there is nothing that would fundamentally prevent us from implementing the full POSIX standard using TLMs.
\begin{figure}[t]
\begin{subfigure}[t]{0.45\textwidth}
\vspace{-1px}
\begin{lstlisting}[mathescape=~]
  module Regex = {
    type t = Empty
           | AnyChar 
           | Str(string)
           | Seq(t, t) 
           | Or(t, t) 
           | Star(t);
  }
\end{lstlisting}
\vspace{-5px}
\caption{The \li{Regex} module, which defines the recursive datatype \li{Regex.t}.}
\label{fig:Regex-module-def}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.53\textwidth}
\vspace{-1px}
\begin{lstlisting}[mathescape=|]
  module RegexNotation = {
    notation $regex at Regex.t {
      lexer  RegexLexer;
      parser RegexParser.start;
      dependencies
        { module Regex = Regex; }
    }
  }
\end{lstlisting}
\vspace{-5px}
\caption{The definition of the \li{$regex} TLM. Figure \ref{fig:lexer-and-parser} defines \li{RegexLexer} and \li{RegexParser}.}
\label{fig:regex-tlm-def}
\end{subfigure}

\begin{subfigure}[t]{\textwidth}
\vspace{5px}
\begin{lstlisting}
  notation $regex = RegexNotation.$regex; /* or open RegexNotation */
  module DNA = { let any_base = $regex `(SURLA|T|G|CEURL)`; };
  let bisA = $regex `(SURLGC$(EURLDNA.any_baseSURL)GCEURL)`;
  let restriction_template = (gene) => 
    $regex `(SURL$(EURLbisASURL)$(EURLDNA.any_baseSURL)*$$(EURLgeneSURL)$(EURLDNA.any_baseSURL)*$(EURLbisASURL)EURL)`;
\end{lstlisting}
\vspace{-4px}
\caption{Examples of the \li{$regex} TLM being applied in a bioinformatics application. %In each case, literal body, between backticks, is initially left unparsed according to the language's context-free syntax. %The applied TLM determines a segmentation and expansion during the typed expansion phase, which generalizes the usual typing phase.
}
\label{fig:first-tlm-example}
\end{subfigure}
\vspace{0px}
\caption{Case Study: POSIX-style regex literal notation, with support for string and regex splicing.}
% \vspace{-6px}
\label{fig:regex-case-study}
\end{figure}


\subsection{Client Perspective}\label{sec:client-perspective}
Let us start from the perspective of a client programmer examining Fig.~\ref{fig:regex-case-study} but holding the underlying expansion of Fig.~\ref{fig:first-tlm-example}, as well as the details of the lexer and parser, \li{RegexLexer} and \li{RegexParser}, abstract. We will return to describe the lexer and parser from the provider's perspective in Sec. \ref{sec:provider-perspective}.

% Fig.~\ref{fig:first-tlm-example}, then use these examples to detail the client-side abstract reasoning principles that the mechanism provides, all before finally . % The \li{open notation} directive on Line 2 of Fig.~\ref{fig:first-tlm-example} allow qualifying each TLM application in the file. %reasoning about this program from the perspective of a client who wants to avoid looking at the lexer, parser and the underlying expansion itself.

Line 2 of Fig.~\ref{fig:first-tlm-example} applies \li{$regex} to construct the regex \li{DNA.any_base} that was  described above, this time using the more concise and common POSIX regex notation. Line~3 applies \li{$regex} again, using its regex splice form to compositionally construct a regex matching DNA sequences recognized by the BisA restriction enzyme, where the middle base can be any base. Finally, Lines~4-5 of Fig.~\ref{fig:first-tlm-example} define a function, \li{restriction_template}, that constructs a more complex regex from these first two regexes and a given gene sequence represented as a string.

Let us consider the second of these three TLM applications more closely:
\begin{lstlisting}[numbers=none]
    $regex `(GC$(DNA.any_base)GC)`
\end{lstlisting}
According to the context-free grammar of (this paper's version of) Reason, this form is simply a leaf of the unexpanded parse tree, like a string literal would be. TLM names are prefixed by \li{$} 
 to distinguish them from variables. We call the TLM argument, \li{`(GC$(DNA.any_base)GC)`}, a \emph{generalized literal form}, following the terminology introduced in  prior work on type-specific languages~\cite{TSLs}. The only lexical constraint imposed on the {literal body}, i.e. the characters between \li{`(} and \li{)`}, is that any nested occurrences of \li{`(} must be balanced by \li{)`}, much like nested comments in Reason/OCaml. Generalized literal forms therefore lexically subsume many other literal forms. This nesting constraint is to allow TLM applications to appear inside spliced expressions. An example of nested TLM application is shown in Fig.~\ref{fig:nesting-example}, discussed later in this section. Related work by \citet{TSLs}, discussed in Sec.~\ref{sec:existing-approaches}, specified several other choices of outer delimitation, including layout-sensitive delimitation, but for the purposes of this paper, \li{`(} and \li{)`} suffice. 
% Notice that in the unexpanded expression, we have not yet distinguished the spliced expression \li{any_base} from the rest of the literal. This is to emphasize that it is only during the subsequent \emph{expansion} phase that the body of the generalized literal form is lexed, parsed and expanded to produce an expanded syntax tree where spliced expressions, like \li{any_base}, have been revealed. 

% \vspace{-4px}
% \subsection{Client Perspective: Abstract Reasoning Principles} 
% The question that this paper posits is central when evaluating a notation definition system is: what must be revealed about the lexer, parser and the expansion itself for the client programmer to be able to answer the sorts of questions about types and binding that they would normally ask?

\subsubsection{Responsibility} 
Responsibility for lexing, parsing and expanding each literal body is delegated uniquely to the applied TLM. % Choices about how spliced expressions are recognized inside the literal body are entirely at the discretion of each TLM. In this case, the TLM provider has chosen to use \li{$(}$e$\li{)} for regex splicing and \li{$$(}$e$\li{)} for string splicing, where $e$ is an unexpanded Reason expression of arbitrary form. % We return to these topics below.% reasoning about splicing works when encountering an unfamiliar literal form below.
% and take the perspective of a client that does not want to delve into these details.
TLM definitions follow the same scoping rules as Reason modules, i.e. they can be defined in modules alongside other definitions and accessed through module paths. When a TLM definition appears inside a module with an explicitly specified module type, it must also appear in the module type with the same specification, up to the usual notions of type and module path equivalence in the type annotation and module dependencies, which are discussed below. This is much like the situation with datatype definitions in ML. In Sec. \ref{sec:implementation}, we will describe how we use an encoding of TLM definitions as modules with singleton signatures to avoid having to primitively extend OCaml's module system.

What is fundamental about this design is that there is a well-defined protocol for finding the definition of the TLM uniquely responsible for each generalized literal form in a program, following the usual scoping rules of the language. An editor service or documentation tool can use this protocol to integrate TLM definition lookup into a ``go to definition'' command. Moreover, clients can define scoped abbreviations as shown on Line 1 of Fig.~\ref{fig:first-tlm-example}, again following the usual scoping rules. By convention, we define TLMs in a module suffixed with \li{Notation} so that client programmers can \li{open} just the relevant TLM definition(s) without bringing other definitions into scope. We will demonstrate a simple lexically scoped implicit application mechanism for situations where the same TLM is being repeatedly applied  in Sec. \ref{sec:regexparser}.

% In Reason, TLM definitions like the one in Fig.~\ref{fig:regex-tlm-def} are separately defined in files with the suffix \li{.relit}, which are processed and brought into scope by the Reason build system, \li{rebuild}. This is consistent with the protocol that other syntax extension systems use, e.g. the syntax libraries in Sugar* \cite{erdweg2011sugarj,erdweg2013framework,erdweg2012layout}. In our theoretical treatment in Sec.~\ref{sec:setlms-formally}, however, TLM definitions will be integrated directly into the scoping structure of the language. This is not possible in Reason because this would require extending OCaml's semantics with TLM definitions in module signatures. While we were careful to choose a design where this modification would be feasible in principle---TLM definition equality requires only decidable type and module path equality---modifying the semantics of OCaml has intentionally been placed beyond the scope of the Reason project. In any case, what is fundamentally important is that there is a well-defined protocol for finding the definition of the TLM uniquely responsible for each application site. % via a ``go to definition'' service.

% nitions (Fig.~\ref{fig:regex-tlm-def}) appear in \li{.relit} files, and they are brought into scope by the build system, \li{rebuild}, according to the usual protocols for library discovery. The fully qualified name of the \li{--}% In the theory that we develop in Sections \ref{sec:s-UL}, TLM definitions occur directly within code. However, in a language with a module system, this would require adding TLM definitions to signatures as well. 
% TLM definitions follow well-defined scoping rules, so the client programmer can follow the binding structure of the language in the usual manner (manually or assisted by a ``go to definition'' editor service) to determine which TLM is uniquely responsible for each literal form. In Sec. \ref{sec:ptsms}, we will see an example of TLM synonyms (as a trivial case of partial parameter application).

\subsubsection{Expansion Typing}
Having found the definition of the \li{$regex} TLM, the client can immediately determine the type of the expansion being generated at each application site because it is specified explicitly by the clause \li{at Regex.t} on Line~2 of Fig.~\ref{fig:regex-tlm-def}. The expansion type of a TLM is analagous to the return type of a function. The identity of \li{Regex.t} is determined relative to the TLM definition site, not at each application site, so the module \li{Regex} need not be in scope at the application site, or it can have been shadowed by a different module. % In Reason, TLM definitions can refer to exactly those modules that a Reason module at the top level of the defining package would have access to, i.e. other top-level modules in the package as well as modules exported by other packages that have been imported by the build system.

\subsubsection{Context Dependence}
\label{sec:context-dependence}
The system enforces a strong context independence condition on generated expansions by requiring that the TLM definition explicitly specify all modules that the expansions it generates might internally depend on. In this case, Lines 5-7 of Fig.~\ref{fig:regex-tlm-def} specify that generated expansions might use the module \li{Regex}, again as it is bound at the TLM definition site, using the module variable \li{Regex} internally. In  general, the dependency can be an arbitrary module path, e.g. \li{module List = Core.Std.List}. The \li{Pervasives} module is implicitly opened in each expansion. All other bindings, both at the TLM definition site and the TLM application site, are not internally available to the expansion. 

% For example, consider the following dependency clause:
% \begin{lstlisting}[numbers=none]
%   dependencies { 
%     module Regex = Regex;
%     module List = Core.List;
%   }
% \end{lstlisting}
% The module variable \li{List} will be bound internally to \li{Core.List} in all expansions generated by this TLM, including expansions generated at application sites where \li{List} or \li{Core.List} are not bound, or bound to different modules. 


From the client programmer's perspective, the benefit of an application site context independence discipline is clear---clients do not need to give any thought to which bindings the expansion might invisibly be assuming are in scope. This is a common problem with various unhygienic approaches, e.g., \li{camlp4} \cite{de2003camlp4}, Sugar* \cite{erdweg2011sugarj,erdweg2013framework,erdweg2012layout}, PPX rewriters \cite{ocaml-manual} or Template Haskell \cite{SheardPeytonJones:Haskell-02,mainland2007s}.

The benefits of making the macro definition site dependencies explicit, rather than implicitly allowing expansions to access all bindings at the definition site as in many existing macro systems, are more subtle. One technical benefit is that this exposes the TLM's dependencies in the signature of the module where the TLM is defined. This is useful for build tools that extract dependencies by source code analysis. Moreover, if an ``internal'' module is being used by expansions, then this will be manifest in the signature and can be corrected if this was not intended. 
% (Similar concerns about unintended capture drives recent work on explicit closures for distributed computing \cite{DBLP:conf/ecoop/MillerHO14}.) 
Implicit access to the  definition site would require the implementation to carefully ``smuggle'' internal values to each application site, thereby skirting the abstraction discipline of ML's module system \cite{culpepper2005syntactic}.% (and making the implementation substantially more complex!)\todo{I think there is some work on this...}  %This, in turn, ensures that the expansion does not depend on bindings inaccessible outside the module, i.e. the implementation does not need to ``smuggle'' references to these internal definitions to each application site. Consequently, the abstraction discipline of the ML module system is maintained. 

Another benefit of this explicit approach is that renamings can be propagated without needing to modify AST encodings in parser definitions. For example, if we decided to rename the module \li{Regex} to \li{Regexp}, this renaming would need only to propagate to the module paths in the TLM definition in the usual manner as long as the names of the internal module variables remain the same. (The TLM provider may wish, for the sake of aesthetics, to propagate the change into the parser definition as well, but this is not semantically necessary). This allows us to encode expansions using ordinary OCaml values (see Sec. \ref{sec:provider-perspective}).


Enforcing this strong context independence condition is technically subtle because TLM parsers need to be able to implement splicing, i.e. they need to be able to parse  terms out of the literal body for placement in the expansion. Na\"ively checking that only the explicitly named dependencies are free in the expansion would inappropriately constrain application site spliced expressions, which should certainly not be prevented from referring to variables in scope at the application site. For example, consider the final expansion of our example from Line 2 of Fig.~\ref{fig:first-tlm-example}:
\begin{lstlisting}[numbers=none]
  Regex.Seq(Regex.Str "GC", Regex.Seq(DNA.any_base, Regex.Str "GC"))
\end{lstlisting}
In this term, both \li{Regex} and \li{DNA} are free module variables. There is nothing to distinguish references to \li{DNA} that arose from a spliced sub-expression parsed out of the literal body from those that would indicate that the context independence condition has been violated. 
%\footnote{An exhaustive search for "DNA" in the literal body would not be  because final expansions are abstract syntax trees. There is not a unique mapping back to concrete syntax, e.g. because whitespace can be inserted arbitrarily.} 

Hygienic term-rewriting macro systems, like those available in various Lisp-family languages \cite{mccarthy1978history} and in Scala \cite{ScalaMacros2013}, cannot be used to repurpose string literals for composite literal notation at other types for exactly this reason. These systems would find that the appearance of \li{DNA.any_base}  violates their context independence condition (which is one aspect of {hygiene}), because \li{DNA.any_base} is not, from the perspective of the context-free syntax, an argument to the macro nor even a sub-term of an argument for which a tree path can be assigned \cite{DBLP:conf/esop/HermanW08,Herman10:Theory,gorn1965explicit}. Instead, it arises as the result of performing a complex operation---parsing---on an arbitrary sub-sequence of the string literal passed into the macro for rewriting. %From the macro expander's perspective, it has appeared by magic''.

To address this problem, TLM parsers are not tasked with generating the final expansion directly. Instead, the parser generates a \emph{proto-expansion} that refers to spliced terms indirectly by location relative to the start of the provided literal body. For example, the proto-expansion generated by \li{$regex} for the example above can be pretty printed as follows:
\begin{lstlisting}[numbers=none]
  Regex.Seq(Regex.Str "GC", Regex.Seq(spliced<4..15 : Regex.t>, Regex.Str "GC"))
\end{lstlisting}
Here, \lismall{spliced<4..15 : Regex.t>} is a reference to the spliced expression \li{DNA.any_base} because characters $4$ through $15$, zero-indexed and inclusive, of \lismall{GC$(DNA.any_base)GC} are \lismall{DNA.any_base}. We return to the type annotation on the splice reference, \li{Regex.t}, in Sec. \ref{sec:segment-typing} below. The context independence condition can be enforced straightforwardly on the proto-expansion---the only free variable in the proto-expansion is \li{Regex}, which is an explicitly listed dependency, so all is well. 


% Had a reference to \li{DNA} appeared in the proto-expansion, it would be clear that it did not originate in a spliced expression. Leaving it in the final expansion would mean that the expansion would be valid only in contexts where \li{DNA} is appropriately bound, i.e. \li{DNA} would be an invisible assumption that clients could not reason abstractly about. More subtly, using the binding of \li{DNA} from the TLM definition site would also be problematic because if \li{DNA} is renamed, it would not be obvious that this renaming needs to be propagated into the OCaml values that represent proto-expansions inside the parser (see Sec. \ref{sec:provider-perspective} for further discussion of AST representations). We avoid both of these issues by making dependencies explicit. %  can propagate into the TLM definition without changing the internal module variable being used.% Similar approaches have worked well in other situations where there are subtleties %Recent work on closures in distributed programming took a similar approach of making the variables  explicit, albeit for different reasons \cite{DBLP:conf/ecoop/MillerHO14}\todo{cite heather's paper}\todo{talk somewhere about how you could use implicit dependencies if you defined parser in-line using MetaOCaml, but this would have other trade-offs}
% Requiring that TLMs refer to spliced expressions indirectly in this manner ensures that a TLM cannot ``forge'' spliced terms, i.e. mark some sub-term as being spliced when it does not in fact appear at some specified segment of the literal body.

\subsubsection{Segmentation}\label{sec:segmentation}
The finite set of splice references in the proto-expansion generated for a literal body is called the \emph{segmentation} of that literal body. The segmentation of the example above is the finite set containing one splice reference, \lismall{spliced<4..15 : Regex.t>}. For the more complex example from Line 5 of Fig.~\ref{fig:first-tlm-example}, the segmentation contains five splice references:
\begin{lstlisting}[numbers=none]
  { spliced<2..5 : Regex.t>, spliced<9..20 : Regex.t>, spliced<26..29 : string>, 
    spliced<33..44 : Regex.t>, spliced<49..52 : Regex.t> } 
\end{lstlisting}
The system checks that the segmentation does in fact segment the literal body, i.e. that the segments are in-bounds, of positive extent and non-overlapping. Adjacent spliced segments must also be separated by at least one character. The spliced segment locations can therefore be communicated unambiguously to the programmer by tools downstream of the expander, e.g. program editors and pretty printers, using secondary notation. In this paper, non-spliced segments are shown in color and spliced segments start in black.  

When TLM applications are nested, a distinct color can be used at each depth. For example, Fig.~\ref{fig:nesting-example} shows a program fragment where we transform an encoding of a chemical structure expressed using the standard SMILES notation for chemical structures \cite{anderson1987smiles}, extended with splicing notation, into a vector graphic, then embed this directly into a fragment of a webpage. We detail the TLM \li{$html at Html.t}, which implements HTML notation similar to that found in Ur/Web, in the supplement, and assume a TLM, \li{$smiles at Smiles.t} and a function, \li{Smiles.to_svg : Smiles.t => Html.t}, not shown. In Reason, \li{|>} is reverse function application.
\begin{figure}[t]
\begin{lstlisting}[numbers=none]
  $html `( SURL<div>
             <h3>Chemical Structure of Sucrose</h3>
             <$>EURL$smiles `(SCOLOR{ECOLORmono_glucoseSCOLOR}-O-{ECOLORmono_fructoseSCOLOR}ECOLOR)` |> Smiles.to_svgSURL</$>
           </div>EURL )`
\end{lstlisting}
\caption{A practical demonstration of nested TLM application. The colors communicate the segmentation.}
\vspace{-3px}
\label{fig:nesting-example}
\end{figure}

\subsubsection{Segment Typing}\label{sec:segment-typing} Each splice reference in the segmentation carries not just the location of a spliced expression but also its expected type. The identity of this type is resolved in a context-independent manner, assuming only the dependencies explicitly specified by the TLM.

By associating a type with each spliced segment, type inference can be performed abstractly, meaning that only the segment types, together with the expansion types specified by the applied TLMs, are necessary to infer types for variables appearing in a client-side function. For example, consider the function \li{restriction_template} on Lines 4-5 of Fig.~\ref{fig:first-tlm-example}. The return type of this function can be inferred to be \li{Regex.t} from the expansion type annotation on the \li{$regex} TLM, as previously discussed. The type of the argument, \li{gene}, can be inferred to be \li{string} because the segmentation specifies the type \li{string} for the spliced segment where it appears (cf. the segmentation shown in Sec. \ref{sec:segmentation} above). The context independence condition implies that \li{gene} cannot appear elsewhere in the expansion, and so no further typing constraints could possibly be collected from examining the portions of the (proto-)expansion being held abstract. Another important benefit of explicitly tracking the locations of spliced segments is that errors that originate in spliced terms can be reported in terms of their original source location \cite{DBLP:journals/jsc/DeursenKT93}.

Segment types can be communicated directly to the programmer upon request by an editor service. For Reason, we are equipping the {Merlin} tool \cite{Merlin}, which is used by various Reason editor modes (e.g. for Emacs, Vim, and so on), with a new editor command that reports the expected type of the innermost spliced segment containing the cursor. Note that because the type is explicit, this information can be reported even when there is a parse error or type error in a spliced expression. % An alternative design that simply reported the type assigned to each spliced expression based on the final expansion (which already exists in Merlin) would be less useful in situations where there is a type error in a spliced expression, or the programmer is in the process of editing the program and wants to know what type of expression to construct.

Segment types are somewhat analagous to the argument types of a function. The difference is that the argument signature of a function is the same every time the function is applied, i.e. it is associated with the function itself, whereas the inferred segmentation can differ for each choice of literal body. This, of course, gives TLMs substantially more notational flexibility, even relative to infix or mixfix function notation (where the number of subexpressions is fixed \cite{wieland2009parsing}), as intended.% (even infix or mixfix functions \cite{wieland2009parsing}).

\subsubsection{Capture}
In discussing the question of inferring a type for \li{gene} above, we neglected to consider one critical question: are we sure that the variable \li{gene} in the third spliced segment on Line 5 of Fig.~\ref{fig:first-tlm-example} is, in fact, a reference to the argument \li{gene} of the \li{restriction_template} function? After all, if we hold the expansion abstract then it may well be that the third spliced segment appears under (i.e. captures) a different binding of the identifier \li{gene}. For more common identifiers, e.g. \li{tmp}, inadvertent capture is not difficult to imagine. For example, consider this application site:
\begin{lstlisting}[numbers=none]
  let tmp = /* ... application site temporary ... */;
  $html `(SURL<h1><$>EURLf(tmp)SURL</$></h1>EURL)`;
\end{lstlisting}
Now consider the scenario where the proto-expansion generated by \li{$html} has the following form:
\begin{lstlisting}[numbers=none]
  let tmp = /* ... expansion-internal temporary ... */;
  Html.H1Element(tmp, spliced<7..12 : Html.t>);
\end{lstlisting}
If the final expansion was produced na\"ively, by syntactically replacing the splice reference with the final expansion recursively determined for the corresponding spliced expression, then the  variable \li{tmp} in the spliced expression would capture the expansion-internal binding of \li{tmp}. The result if the types of the two bindings differed would be a type error exposing the internal details of the expansion. If the types of the two bindings of \li{tmp} coincided, then there would be no static indication of the problem but there could be subtle and mysterious changes in run-time behavior. % Capture of invisible bindings is, due to these hazards, clearly unreasonable.

To address this problem, splicing is guaranteed to be capture-avoiding. The final expansion is generated by recursively expanding each spliced expression and then inserting it into the final expansion via capture-avoiding substitution, which automatically alpha-varies the internal bindings of the proto-expansion as necessary. There is no need for TLM providers to manually deploy a mechanism that generates fresh variables (as in, e.g., Racket's reader macros \cite{Flatt:2012:CLR:2063176.2063195}, further discussed in Sec. \ref{sec:existing-approaches}). For example, the final expansion of the example above is alpha-equivalent to the following:
\begin{lstlisting}[numbers=none]
  let tmp = /* ... application site temporary ... */; 
  let tmp_fresh = /* ... expansion-internal temporary ... */;
  Html.H1Element(tmp_fresh, f(tmp));
\end{lstlisting}
%Notice that the expansion-internal binding of \li{tmp} has been alpha-varied to \li{tmp'}. The reference to \li{tmp} in the spliced expression then refers, as intended, to the application site binding. 

% For TLM providers, the benefit of this mechanism is that they can name the variables used internally within expansions freely. TLM clients can, in turn, can reason abstractly about \textbf{Capture}.

Although this strict capture avoidance discipline implies that TLMs cannot intentionally introduce  bindings directly into spliced expressions, this does not imply that values cannot flow from the expansion into a spliced expression. It simply means that when this is intended, the segment type must be a function type, which serves to make this interface explicit. Reason's concise lambda notation, \li{(x) => e}, decreases the syntactic cost of this approach. For example, we cannot define list comprehension notation like the following:
\begin{lstlisting}[numbers=none,deletekeywords={in}]
  $listcomp `(x + 1SURL | x in EURLloSURL .. EURLhi)` /* NO! cannot reason abstractly */
\end{lstlisting}
However, the following is permitted, because \li{x} is bound by the spliced lambda expression and the corresponding segment type makes the type of the interface explicit:
\begin{lstlisting}[numbers=none]
  $listcomp `((x) => x + 1 SURL|EURL lo SURL..EURL hi)`  /* OK! */
\end{lstlisting}

Our contention is that small syntactic costs like these are more than justified by the peace of mind of knowing that unfamiliar literal notation cannot possibly be performing ``magic'' on the type and binding structure of the language. We say more about the future prospect of a  mechanism designed specifically for shorthand binding forms, e.g. Haskell-style \li{do} notation \cite{jones2003haskell}, in Sec. \ref{sec:discussion}.% For now, the existing infix operator support in Reason and OCaml suffices for working with monadic values.

%A polymorphic list comprehension TLM requires type parameters, introduced in Sec. \ref{sec:ptsms}.

% There are various more relaxed capture avoidance conditions that one might consider, e.g. where the segmentation explicitly specifies which spliced identifiers in the literal body are to be captured in each spliced expression. However, in Reason 

% For example, Haskell's notation for values equipped with monadic structure, i.e. \li{do}-notation, cannot be expressed under this strong capture avoidance condition because it introduces bindings into sub-terms . Even without 

% An alternative capture avoidance discipline is one where each spliced segment is annotated with a list of spliced identifiers that will be bound in the corresponding spliced expression, together with their types. Spliced identifiers, like other spliced segments, must appear at an identified location within the literal body, and cannot overlap with other spliced segments. 


% In our initial designs, we were able to express this example because we distinguished spliced identifiers, much as we do spliced types and expressions. These could be explicitly bound inside spliced expressions by extending the \li{SplicedE} and \li{SplicedT} constructors to take finite sets of spliced identifiers. This is conceptually similar to the approach taken by \citet{DBLP:conf/esop/HermanW08} for giving (already parsed) sub-terms access to internal variables. However, we ultimately decided to remove this feature because it would disproportionately increase the reasoning burden on clients when they encounter an unfamiliar literal: \emph{might this obscure literal also be introducing bindings?} It is difficult for a program editor to display a set of ``hidden bindings'' to the programmer. Haskell-style infix notation for monadic values, where bind is \li{e >>= f}, can be expressed using TLMs without support for spliced identifiers. Reason 3.0 is adopting more concise function syntax, which will decrease the cost of this syntax.



% Using this TLM, we can express the Ur/Web example from Figure \ref{fig:urweb} as shown in Figure \ref{fig:first-tsm-example}. On both Lines 1 and 2, we apply 
% \li{$html} 
%  to a \emph{generalized literal form} delimited by \li{[|} and \li{|]}. Generalized literal forms, which first arose in the prior work on TSLs \cite{TSLs}, syntactically subsume other literal forms because the context-free syntax of the language only specifies the outer delimiters. In this paper, we will use  \li{[|} and \li{|]}, but \citet{TSLs} formally specified several other choices, including layout-sensitive delimitation. \emph{Literal bodies} are constrained only in that \li{[|} and \li{|]} must be balanced.  %In particular, there is no pre-specified syntactic to splice out sub-terms.%The initial context-free parser therefore parses generalized literal forms in much the same way as parsers typically parse ``raw'' string literals (i.e. string literals where escape sequences like \li{\\n} have not yet been processed.) 

%  The system delegates responsibility over the parsing and expansion of each literal body to the applied TLM's parse function during a semantic phase called \emph{expansion}, which starts just before and continues into the typing phase. 

%  Because the parse function is applied during this phase, rather than at run-time, we call it a \emph{static function}. Static functions  cannot refer to the surrounding variable bindings because those variables stand for run-time values. Instead, there is are separate \emph{static bindings} marked by the \li{static} keyword that populate a \emph{static environment} that is discarded after expansion finishes. A more detailed account of static evaluation, both informal and informal, is given in the supplement. An alternative design that allows for the explicit lowering of standard-phase modules to the static phase has also been proposed for OCaml \cite{Ocaml/macros}. %\ificfp We are closely following this work in our implementation. \else \fi
%   % return to in Sec. \ref{sec:static-eval}.

%  The input type of the parse function, \li{body}, classifies encodings of literal {bodies}. Literal bodies are sequences of characters, so we define \li{body} as a synonym of \li{string} in \autoref{fig:indexrange-and-parseresult}. The return type is a sum type, defined by applying the parameterized type \li{parse_result} defined in \autoref{fig:indexrange-and-parseresult}, that distinguishes between parse errors and successful parses. Let us consider these two possibilities in turn.

\vspace{-5px}
\subsection{Provider Perspective}
\label{sec:provider-perspective}
\begin{figure}
\begin{subfigure}[t]{0.30\textwidth}
\begin{lstlisting}[morekeywords={rule,parse,eof}]
  { 
  open RegexParser;
  let readsplice = 
    Relit.Segment.read_to(")");
  let unescape = (s) => 
    String.sub(s, 1, 1); 
  }
  let special = 
    ['SCSS\\ECSS' 'SCSS.ECSS' 'SCSS|ECSS' 'SCSS*ECSS' 'SCSS+ECSS' 
     'SCSS?ECSS' 'SCSS(ECSS' 'SCSS)ECSS' 'SCSS$ECSS']
  let not_special = _#special
  let escape = 'SCSS\\ECSS' special
  rule read = 
    parse
    | "." { DOT }
    | "|" { BAR }
    | "*" { STAR }
    | "+" { PLUS }
    | "?" { QMARK }
    | "(" { LPAREN }
    | ")" { RPAREN }
    | not_special+ as s
      { STR(s) }
    | escape as s
      { STR(unescape(s)) }
    | "$(" 
      { SPLICED_REGEX
        (readsplice(lexbuf)) }
    | "$$("  
      { SPLICED_STRING
        (readsplice(lexbuf)) }
    | eof { EOF }
\end{lstlisting}
\vspace{-4px}
\caption{\li{RegexLexer.mll}}
\label{fig:regex-lexer}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.61\textwidth}
\begin{lstlisting}[morekeywords={token,left,start},stringstyle=\ttfamily\color{black}]
  %{ 
  open notation Relit.$proto_expr; 
  %}
  %token DOT BAR STAR PLUS QMARK LPAREN RPAREN EOF
  %token <string> STR
  %token <Relit.Segment.t> SPLICED_REGEX 
  %token <Relit.Segment.t> SPLICED_STRING
  %left BAR
  %start <Relit.ProtoExpr.t> staSOKEOKrt
  %%
  staSURLEURLrt:
    | e = regex; EOF { e }
    | EOF { `(SURL Regex.Empty EURL)` }
  regex:
    | DOT { `(SURL Regex.AnyChar EURL)` }
    | s = STR { `(SURL Regex.Str($s `(EURLsSURL)`) EURL)` }
    | r1 = regex; r2 = regex 
      { `(SURL Regex.Seq(`(EURLr1SURL)`, `(EURLr2SURL)`) EURL)` }
    | r1 = regex; BAR; r2 = regex 
      { `(SURL Regex.Or(`(EURLr1SURL)`, `(EURLr2SURL)`)EURL )` }
    | r = regex; STAR { `(SURL Regex.Star(`(EURLrSURL)`) EURL)` }
    | r = regex; PLUS
      { `(SURL leEURLSURLt r = `(EURLrSURL)`;
           Regex.Seq(r, Regex.Star(r)) EURL)` }
    | r = regex; QMARK 
      { `(SURL Regex.Or(Regex.Empty, `(EURLrSURL)`) EURL)` }
    | LPAREN; r = regex; RPAREN { r }
    | seg = SPLICED_REGEX 
      { `(SURL $spliced `(EURLsegSURL : Regex.t)` EURL)` }
    | seg = SPLICED_STRING 
      { `(SURL Regex.Str
           ($spliced `(EURLsegSURL : strEURLSURLing)`) EURL)` }
\end{lstlisting}
\vspace{-4px}
\caption{\li{RegexParser.mly}}
\label{fig:regex-parser}
\end{subfigure}
\vspace{2px}
\caption{The lexer and parser for the \li{$regex} TLM from Fig.~\ref{fig:regex-tlm-def}. See Fig.~\ref{fig:relit-util} for relevant definitions from \li{Relit}.}
\label{fig:lexer-and-parser}
\vspace{-5px}
\end{figure}

Let us turn now to the perspective of the TLM provider, whose principal task is to define the lexer and parser named in the TLM definition. In our implementation of TLMs for Reason, lexers are generated using \li{ocamllex} \cite{ocaml-manual} and parsers using Menhir \cite{pottier2016menhir}, which is a modern derivative of the parser generator Yacc~\cite{johnson1975yacc,TarditiDR:mly} with support for LR(1) grammars \cite{jourdan2012validating}. Reason itself is implemented using these same tools. 
We will say more about how lexer and parser definitions are resolved by our implementation in Sec. \ref{sec:implementation}. What is fundamentally important is that there is some clear protocol by which the lexer and parser names and the starting non-terminal name specified by the TLM definition, in this case \li{RegexLexer} and \li{RegexParser.start} on Lines 3-4 of Fig.~\ref{fig:regex-tlm-def}, are resolved to lexer and parser implementations that can be evaluated at expansion-time, i.e. during compilation. 

The \li{ocamllex} and Menhir definitions used to implement \li{RegexLexer} and \li{RegexParser} are given in Fig.~\ref{fig:lexer-and-parser}. A fragment of the signature of the \li{Relit} library, which provides various necessary definitions and utilities to TLM providers, is shown in Fig.~\ref{fig:relit-util}. 
After describing this example in detail, we will outline some alternative parsing strategies.




\begin{figure}
\begin{lstlisting}[deletekeywords={spliced}]
  module Segment : { 
    type t = {start_pos: int, end_pos: int};
    let read_to : string => Lexing.lexbuf => Segment.t;
  };
  exception ExpansionError({msg: string, loc: option(Segment.t)});
  module ProtoExpr : { 
    type t = Parsetree.expression; /* from the OCaml compiler library */
    let spliced : Segment.t => Parsetree.core_type => t;
  };
  notation $re_expr at Parsetree.expression { /* ... (see text) ... */ };
  notation $re_type at Parsetree.core_type { /* ... (see text) ... */ };
  notation $proto_expr at ProtoExpr.t = $re_expr;
\end{lstlisting}
\caption{A fragment of the \li{Relit} module signature relevant to expression TLM providers.}
\label{fig:relit-util}
\vspace{-4px}
\end{figure}
    % let match_spliced : t => option(Segment.t, ProtoTyp.t);

\subsubsection{\li{RegexLexer}}
\label{sec:regex-lexer}
In most respects, the definition of the lexer in Fig.~\ref{fig:regex-lexer} is conventional. It reads various token patterns on Lines 6-23, emitting the corresponding tokens specified by the \lstinline[morekeywords={token}]{%token} declarations in Fig.~\ref{fig:regex-parser}, which have been brought into scope by Line 2 of Fig.~\ref{fig:regex-lexer}. Lines 20-21 combine sequences of non-special characters (e.g. alphanumeric characters) into a single string token. Lines 22-23 implement backslash-prefixed escape sequences for characters that have special meaning in this notation for regexes, emitting a string token in each instance.\footnote{A better approach would be to define a second lexing rule that greedily combines sequences consisting of non-special characters and escape characters into a single \li{STR} token, but for the sake of exposition, we stick to this simpler approach.}  

Lines 24-31 of the lexer, which recognize the notation for regex splicing \li{$(}$e$\li{)}, and string splicing, \li{$$(}$e$\li{)}, are more unusual. Recall from Sec.~\ref{sec:context-dependence} that the parser must represent spliced terms abstractly, by their location within the literal body. This implies that the TLM does not need to parse spliced expressions itself, as long as it can independently decide where the spliced expressions start and end. A paired start and end position is a segment,  represented by a value of the type \li{Segment.t} defined on Line 2 of Fig.~\ref{fig:relit-util}. Lines 26-27 and Lines 30-31 of the lexer produce a segment by calling a helper function, \li{Relit.Segment.read_to}, that internally invokes the Reason lexer on the provided lexing buffer until it sees an instance of the provided token outside of a Reason comment, string literal or generalized literal. If the provided token is a right delimiter also used by Reason, like \li{")"} on Line 4 of Fig.~\ref{fig:regex-lexer}, then \li{read_to} looks for the first unmatched instance. For example, if the remaining lexing buffer contains {\small \texttt{f("))"))A|G}} then \li{read_to} will consume up through the final instance of \li{)} and emit the start and end position, excluding the closing delimiter, relative to the original buffer.

This approach has two major benefits. First, it supports splicing even within literal notations that have a very different lexical structure from Reason itself, as in this example. Second, it allows the parser to avoiding needing to link to Reason's expression grammar (though this is also supported by Menhir). Spliced expressions arrive as single, opaque tokens as shown on Lines 6-7 of Fig.~\ref{fig:regex-parser}, and do not need to be parsed by the TLM at all. The system will recursively process spliced expressions after the parser has finished generating the proto-expansion.

Similar helper functions, not shown, are available for reading out single identifiers, simple paths like \li{X1.X2.xy} and a few other useful fragments of the Reason syntax, so that TLM providers can implement lower-cost splicing notation that clients can choose in situations where the spliced expression is of a restricted form that makes the end position of the spliced expression unambiguous without matching delimitation. As a matter of practice, we recommend that TLM providers start with a fully delimited form before adding optional alternative shorthand in order to avoid artificially restricting the client programmer's ability to directly convey the compositional structure of a value. 
Although \li{Segment.t} is not held abstract in order to support certain advanced scenarios, discussed below, we recommend that TLM providers generally treat it as such.

\subsubsection{\li{RegexParser} and Quasiquotation}\label{sec:regexparser}The job of a TLM parser is to generate a proto-expansion given the tokens generated by the lexer, or if this is not possible, to indicate an expansion error by raising either a Menhir parse error or \li{Relit.ExpansionError} (Line 5 of Fig.~\ref{fig:relit-util}) with an appropriate error message and, if possible, an error location. In the case of expression TLMs, the proto-expansion must be a proto-expression (we consider proto-patterns in the next section). The Menhir directive on Line 9 of Fig~\ref{fig:regex-parser}, which specifies the starting non-terminal of the grammar, reveals that proto-expressions are values of type \li{Relit.ProtoExpr.t}. This type is defined on Line 7 of Fig.~\ref{fig:relit-util} as a synonym for \li{Parsetree.expression}, which is the standard representation of expression parse trees exposed by the OCaml compiler \cite{ocaml-manual}.% by many external tools, including the Reason toolchain. 

To be able to repurpose this existing parse tree representation for proto-expansions, there must be some way to unambiguously represent splice references, notated earlier in the paper as \li{spliced<m..n : ty>} where \li{m..n} specifies a segment and \li{ty} is the segment's expected type. The \li{ProtoExpr} module solves this problem by providing a function \li{spSURLEURLliced} that takes a segment, of type \li{Segment.t}, and a type representation of type \li{Parsetree.core_type}, which also comes from OCaml's compiler library, and produces a corresponding value of type \li{ProtoExpr.t} that uniquely represents the corresponding splice references. Internally, we use OCaml's parse tree annotations.

The datatypes in OCaml's \li{Parsetree} module are necessarily intricate, given the sophistication of the OCaml system, so constructing expression encodings manually is generally too unwieldy. Addressing this class of problem is, of course, the motivation for this very paper, so it is only natural that we define TLMs  for working with OCaml parse trees using familiar surface syntax, extended with splice forms. In the literature, literal notation for the host language's own parse trees is called \emph{quasiquotation}, and splicing is referred to as \emph{unquotation} or \emph{antiquotation} \cite{Bawd99a,shabalin2013quasiquotes,mainland2007s}. The TLMs \li{$re_expr} and \li{$re_type} specified on Lines 10-11 of Fig.~\ref{fig:relit-util} implement quasiquotation for OCaml syntax trees using Reason's surface syntax. The supplement describes their implementation. For clarity, the \li{$proto_expr} TLM is defined as a synonym for \li{$re_expr} on Line~12 of Fig.~\ref{fig:relit-util} (the two TLMs may diverge in the future). We apply this TLM many times in the semantic actions in Fig.~\ref{fig:regex-parser}. Notice, however, that the generalized literal forms in Fig.~\ref{fig:regex-parser} are not each prefixed by \li{$proto_expr}. Instead, we use the \li{open notation} directive on Line 2, which implicitly applies \li{$proto_expr} to all unadorned generalized literal forms following the usual lexical scoping rules for \li{open}. 

To support antiquotation, the \li{$re_expr} TLM repurposes TLM application and generalized literal forms, as can be observed throughout Fig.~\ref{fig:regex-parser}. In particular, parse tree splicing is supported by unadorned generalized literal forms, e.g. on Line~23 where we implement the regex notation \li{r+} in terms of sequencing and star, taking care to bind \li{r} to an expansion-internal variable---also \li{r} but distinguished by the segmentation---to avoid double evaluation. String splicing (which converts the spliced string expression into a parse tree of string constant form) is performed by repurposing the TLM name \li{$s} (Line~16). There is also notation for references to spliced expressions that repurposes the TLM name \li{$spliced}, as seen on Lines 28-32, where the regex splicing logic is implemented. 

TLM-related forms are available to be repurposed in this way by \li{$proto_expr} because there is no reasonable way for \li{$re_expr} to support antiquotation across notational boundaries. For example, if \li{$re_expr} had instead used some other antiquotation notation, e.g. \li{^(}$e$\li{)}, then consider this example:
\begin{lstlisting}[numbers=none]
  $proto_expr `( SURLleEURLSURLt y = ^(EURLr1SURL); $m `(...{^(r2)}...)`EURL )`
\end{lstlisting}
The sequence \li{^(r1)} is in expression position, so antiquotation occurs. However, there is no way for \li{$re_expr} to know how the TLM that \li{$m} will eventually refer to will parse the sequence \li{^(r2)}---it may end up in a spliced expression, or it may have some other meaning---so it cannot perform antiquotation. In other words, TLM expansion is fundamentally ``outside-in''.  Notice that the segmentation does communicate the fact that \li{r2} has not been antiquoted (it remains lavender). However, to avoid confusion, we decided not to support the generation of TLM application forms via \li{$re_expr}, and instead repurposed that notation for antiquotation. 

This issue does \textbf{not} come up when using TLMs to define quotation literals for languages other than Reason itself (an example is given in Sec.~\ref{sec:sptsms}), so if this design proves too limiting, it may be reasonable to build ``inside-out'' quasiquotation primitively into Reason as a special case. In fact, this is already possible using MetaOCaml, discussed below. %(Nested TLM application via splicing is, of course, useful and fully supported, e.g. in Fig.~\ref{fig:nesting-example}).
 % Typed quotations with an explicit representation of contexts would need to be built in to the language  necessary to even think about expanding a 


\subsubsection{Correctness Criteria}
In order to maintain the abstract reasoning principles discussed in Sec. \ref{sec:client-perspective}, the system \emph{validates} each proto-expansion generated by the parser as follows:
\begin{enumerate}
  \item First, the segmentation is computed and validated. This involves checking two criteria:
    \begin{enumerate}
      \item The segments must be in-bounds, of positive extent, non-overlapping and separated by at least one character.
      \item The segment types must encode valid types assuming only the TLM dependencies.
    \end{enumerate}
  \item Second, the proto-expansion is typechecked assuming only the TLM dependencies (plus opened \li{Pervasives}), treating each splice reference as a variable of the specified segment type.
\end{enumerate}
If proto-expansion validation fails, the client is notified that the applied TLM is incorrectly implemented. The proto-expansion validation criteria are exactly the correctness criteria that parser writers must consider, quantifying over all possible literal bodies for which a proto-expansion is generated, in order to avoid exposing this class of compile-time error to clients.

\subsubsection{MetaOCaml}\label{sec:metaocaml} In ``vanilla'' OCaml, it is not possible to express a datatype that classifies only valid proto-expansions according to the correctness criteria above. The MetaOCaml type system comes closer by extending the semantics of OCaml with values of type \li{'a code}, which are constructed using a primitive typed quasiquotation operation \cite{DBLP:conf/flops/Kiselyov14,taha2004gentle}. In MetaOCaml-BER, which is an active implementation of (an extension of) MetaOCaml as a fork of the compiler, and \li{ppx_stage}, which implements a subset of MetaOCaml as a compiler plugin \cite{ppxstage}, a value of type \li{'a code} is represented as, and can be coerced safely to, an OCaml parse tree for the corresponding expression of type \li{'a} \cite{DBLP:conf/flops/Kiselyov14}. It is therefore possible to implement a TLM parser using \li{'a code} values internally, doing the coercion only at the end. However, the typing guarantee is relative to the context where the quotation was constructed, i.e. the parser's implementation, so a validation step is still needed to ensure that the assumptions are consistent with those specified by the \li{dependencies} clause. 

The segmentation criteria cannot be statically enforced and, in both mentioned implementations of MetaOCaml, it is impossible to express splice references directly because they do not yet support terms that contain explicit type annotations. There are various workarounds, e.g. by defining for each possible segment type \li{t} a dummy value of type \li{t code} and then post-processing the parse tree after the coercion step. 
Overall, however, this approach can substantially increase a TLM provider's confidence in the type correctness of the expansion logic, with the caveats just mentioned. With \li{ppx_stage}, it is possible to compile the parser separately without requiring that clients use the plugin. 
TLMs can be applied inside quoted code as long as the applied TLM is accessed through a dependency of the TLM being defined (so that internal dependency references correctly resolve). %MetaOCaml-BER does not change the parse tree or bytecode representation of the OCaml system, and values of type \li{'a code} are not part of the public interface of a TLM parser, so it should, in principle, be possible to compile a parser using MetaOCaml-BER for use in  ``vanilla'' OCaml (and therefore Reason) programs. However, MetaOCaml-BER does not suitably package the necessary pieces of its run-time system, so we have not verified this conjecture.

\subsubsection{Other Implementation Strategies}
Although \li{ocamllex} and Menhir are powerful tools, they are not right for every parsing job. For example, we might want to use a different parser generator, a parser combinator library \cite{Hutton1992d}, or post-process the result of calling an existing parser to produce a corresponding proto-expansion. Fortunately, it is easy to bypass \li{ocamllex} and Menhir: 
%The most obvious method is to define a trivial lexer that reads the entire literal body into a single token, and a trivial parser with one semantic action. In fact, this is not actually necessary because the system does not interface with \li{ocamllex} and Menhir directly. Rather, it is the job of the user's build system to make sure that by the time the compiler is invoked, these tools have already been invoked as necessary to produce ordinary OCaml modules satisfying the expected interface. It is just as well to implement this interface manually. 
the \li{Relit} library includes a trivial lexer, \li{Relit.TrivLexer}, and a few functors that take care of the necessary boilerplate in situations where we simply want to define the parser as a function of type \li{string => Relit.ProtoExpr.t}. For example, the supplement briefly describes how \li{$re_expr} can be expressed in terms of the Reason parser, a parse tree serializer and a tree transformation. It also details how the \li{$html} TLM shown previously is implemented using an existing, ``production grade'' HTML parsing library, \li{Markup.ml} \cite{markupml}, together with a simple post-processing step. It is also worth noting that although there is no mechanism for applying TLMs at the module level, this can be emulated by using OCaml's first class modules.



  % These names are resolved as if they were at the top level of the compilation unit, not as module paths at the TLM definition site (because the resulting module must not refer to bound module variables, e.g. as bound by enclosing functors, or else it would be impossible to resolve the lexer and parser implementations at compile-time).

% Lexer and parser names are interpreted relative to the top level of the compilation unit. The lexer must be a module with a signature compatible with 

% ---------------------- {\color{red} stuff below is notes / not yet revised}
% % \vspace{-3px}\paragraph{Parse Errors} 
% If the parse function determines that the literal body is not well-formed according to the syntax that it implements, it must return \li{ParseError \{msg=}$e_\text{msg}$\li{, loc=}$e_\text{loc}$\li{\}} 
% where $e_\text{msg}$ is a custom error message and $e_\text{loc}$ is a value of type \li{segment}, defined in Figure \ref{fig:indexrange-and-parseresult}, that designates a segment of the literal body as the origin of the error \cite{DBLP:journals/jsc/DeursenKT93}.

% % \vspace{-3px}\paragraph{Success} 
% If instead parsing succeeds, the parse function returns \li{Success} ~$\ecand$, 
% where $\ecand$ is called the \emph{encoding of the proto-expansion}. For expression TLMs, the proto-expansion is a \emph{proto-expression} and it is encoded as a value of the recursive datatype \li{proto_expr} that is outlined in Figure \ref{fig:candidate-exp-Reason}. 
% Most of the constructors of \li{proto_expr} are individually uninteresting -- they encode OCaml's various expression forms. 
% Expressions can mention types, so we also need the type \li{proto_typ} also outlined in Figure \ref{fig:candidate-exp-Reason}. It is only the \li{SplicedE} and \li{SplicedT} constructors that are novel. These are discussed next.


% The fact that the context-free grammar of the base language is fixed ensures that notation providers can reason modularly about syntactic ambiguity in their own grammars.
% % \section{Simple Expression TLMs By Example}\label{sec:tsms-by-example}
% % We begin in this section with a ``tutorial-style'' introduction to seTLMs in Reason. %In particular, we will define an seTLM for constructing values of the recursive labeled sum type \li{rx} that was defined in Figure \ref{fig:datatype-rx}. 
% % Sec. \ref{sec:tsms-minimal-formalism} then formally defines a reduced dialect of Reason called $\miniVerseUE$. This will serve as a ``conceptually minimal'' core calculus of TLMs, in the style of the simply typed lambda calculus.   %We conclude in Sec. \ref{sec:uetsms-discussion} 


% % \subsection{TLM Application}\label{sec:uetsms-usage}
% % The following Reason expression, drawn textually, is of \emph{TLM application} form. Here, a TLM named \li{$rx} is applied to the \emph{generalized literal form} \li{/SURLA|T|G|CEURL/}:
% % \begin{lstlisting}[numbers=none,mathescape=|]
% % $rx /SURLA|T|G|CEURL/
% % \end{lstlisting}
% % Generalized literal forms are left unparsed according to the context-free syntax of Reason. Several other outer delimiters are also available, as summarized in Figure \ref{fig:literal-forms}. The client is free to choose any of these for use with any TLM, as long as the \emph{literal body} (shown in lavender above) satisfies the requirements stated in Figure \ref{fig:literal-forms}. For example, we could have equivalently written the example above as \li{$rx `SURLA|T|G|CEURL`}. (In fact, this would have been convenient if we had wanted to express a regex containing forward slashes but not backticks.) 

% % It is only during the subsequent \emph{typed expansion} phase that the applied TLM parses the {body} of the literal form to generate a \emph{proto-expansion}. The language then \emph{validates} this proto-expansion according to criteria that we will describe in Sec. \ref{sec:uetsms-validation}. If proto-expansion validation succeeds, the language generates the \emph{final expansion} (or more concisely, simply the \emph{expansion}) of the TLM application. The behavior of the program is determined by its expansion. 

% % For example, the expansion of the TLM application above is equivalent to the following expression when the regex value constructors \li{Or} and \li{Str} are in scope:
% % \begin{lstlisting}[numbers=none]
% % Or(Str "SSTRAESTR", Or(Str "SSTRTESTR", Or(Str "SSTRGESTR", Str "SSTRCESTR")))
% % \end{lstlisting}
% % To avoid the assumption that the variables \li{Or} and \li{Str} are in scope at the TLM application site, the expansion actually uses the explicit \li{fold} and \li{inj} operators, as described in Sec. \ref{sec:lists}. In fact, the proto-expansion validation process enforces this notion of context independence -- we will return to proto-expansion validation below. (We will show how TLM parameters can reduce the awkwardness of this requirement in Chapter \ref{chap:ptsms}.)
% % %The constructors above are those of the type \li{Rx} that was defined in Figure \ref{fig:datatype-rx}.

% % % A number of literal forms, ,  are available in Reason's concrete syntax. Any literal form can be used with any TLM,  TLMs have access only to the literal bodies. Because TLMs do not extend the concrete syntax of the language directly, there cannot be syntactic conflicts between TLMs.

% %  %The form does not directly determine the expansion. 

% % \begin{figure}
% % \begin{lstlisting}
% % 'SURLbody cannot contain an apostropheEURL'
% % `SURLbody cannot contain a backtickEURL`
% % [SURLbody cannot contain unmatched square bracketsEURL]
% % {|SURLbody cannot contain unmatched barred curly bracesEURL|}
% % /SURLbody cannot contain a forward slashEURL/
% % \SURLbody cannot contain a backslashEURL\
% % \end{lstlisting}
% % %SURL<tag>body includes enclosing tags</tag>EURL
% % \caption[Available Generalized Literal Forms]{Generalized literal forms available for use in Reason's textual syntax. The characters in lavender indicate the literal bodies and describe how the literal body is constrained by the form shown on that line. The Wyvern language defines additional forms, including whitespace-delimited forms \cite{TSLs} and multipart forms \cite{sac15}, but for simplicity we leave these out of Reason.}
% % \label{fig:literal-forms}
% % \end{figure}

% \subsection{Splicing}\label{sec:splicing-and-hygiene}
% When the parse function determines that some segment of the literal body is a spliced expression, according to whatever syntactic criteria it deems suitable, it can indirectly refer to it in the encoding it produces using the \li{SplicedE} constructor of \li{proto_expr}, which takes a value of type \li{segment} that indicates the zero-indexed location of the spliced expression relative to the start of the provided literal body. The \li{SplicedE} constructor also requires a value of type \li{proto_typ}, which indicates the type that the spliced expression is expected to have. Types can be spliced out by using the \li{SplicedT} constructor of \li{proto_typ} analagously.

% \begin{figure}
% \begin{lstlisting}[numbers=none,xleftmargin=0pt]
% type body = string;
% type segment = {startIdx: int, endIdx: int};
% type parse_result('a) 
%   = ParseError {msg: string, loc: segment}
%   | Success('a);
% type proto_typ = Arrow(proto_typ, proto_typ)
%                | StringTy
%                | /* ... */ 
%                | SplicedT(segment);
% type proto_expr = Tuple(list(proto_expr))
%                 | /* ... */
%                 | SplicedE(segment, proto_typ);
% \end{lstlisting}
% \caption[Definitions of various types used by TLM definitions.]{Definitions of various types available ambiently to TLM definitions.}
% \label{fig:indexrange-and-parseresult}
% \label{fig:candidate-exp-Reason}
% \end{figure}

% For example, consider again the two TLM applications in Figure \ref{fig:first-tsm-example}. In each case, the parse function of the \li{$html} TLM (Figure \ref{fig:html-tlm-def}, Lines 2-4) first sends the literal body through an off-the-shelf HTML parser, \li{parse_html}. It then passes the result to a function \li{html_to_ast}, not shown, which produces the corresponding expression encoding of type \li{proto_expr}. When this function encounters an HTML text node containing matched \li{\{[} and \li{]\}}, then that segment is inserted as a spliced expression of type \li{string}, and similarly text nodes containing matched curly braces produce a spliced expression of type \li{html}. For instance,  the proto-expansion generated for first TLM application in \ref{fig:first-tsm-example}, pretty-printed, is:
% \begin{lstlisting}[numbers=none]
%     H1Element(Nil, Cons(TextNode "Hello, ", Cons(
%       TextNode spliced<13..22; string>, Nil)))
% \end{lstlisting}
% Here, \li{spliced<13; 22; string>} is a reference to the spliced string expression \li{first_name} by its location relative to the start of the literal body being expanded (the off-the-shelf HTML parser provides the necessary baseline location information for use by \li{html_to_ast}). It corresponds to the encoding \li{SplicedE(\{startIdx=13, endIdx=22\}, StringTy)}. Requiring that TLMs refer to spliced expressions indirectly in this manner ensures that a TLM cannot ``forge'' spliced terms, i.e. claim that some sub-term of the expansion should be given the privileges of a spliced term, discussed in Sec. \ref{sec:uetsms-validation}, when it does not in fact appear in the literal body.% Our semantics distinguishes spliced expressions when validating the proto-expansion, which is important for reasons of hygiene that we will detail shortly. 


% % The parse function can similarly extract \emph{spliced types} from a literal body using the \li{SplicedT} variant of \li{proto_typ}. %In particular, the parse function must provide the index range of spliced subexpressions to the \li{Spliced} constructor of the type \li{MarkedExp}. %Only subexpressions that actually appear in the body of the literal form can be marked as spliced subexpressions.

% %For example, had the  would not be a valid expansion, because the  that are not inside spliced subexpressions:
% %\begin{lstlisting}[numbers=none]
% %Q.Seq(Q.Str(name), Q.Seq(Q.Str ": ", ssn))
% %\end{lstlisting}


% % \subsection{Splice Summaries and Segmentations}
% The \emph{segmentation} inferred from a proto-expansion is the finite set of references to spliced terms contained within. For example, the segmentation inferred from the proto-expression above contains only \li{spliced<13; 22; string>}. % Notice that no information about  the spliced terms appear is communicated by the splice summary.
% The system checks that all of the locations in the segmentation are 1) in bounds relative to the literal body; and 2) non-overlapping. 
% This resolves the problem of \textbf{Segmentation} described in Sec. \ref{sec:intro}, i.e. every literal body in a well-formed program has a well-defined segmentation. The TSL mechanism  did not maintain this reasoning principle \cite{TSLs}. A program editor or pretty-printer can communicate this segmentation information to the programmer, e.g. by coloring non-spliced segments lavender as is our convention in this document. In general, spliced expressions might themselves apply TLMs, in which case the convention is to use a distinct color for unspliced segments at each depth. For example, consider a TLM \li{$smiles} for chemical structures \cite{anderson1987smiles} with support for splicing using curly braces:
% \begin{lstlisting}[numbers=none]
%     $html [|SURLChemical structure of sucrose: {EURL
%         $smiles [|SCSS{ECSSm_glucoseSCSS}-O-{ECSSm_fructoseSCSS}ECSS|] 
%         |> SMILES.to_svg SURL}EURL|]
% \end{lstlisting}
% %For example, if strings were not primitive but rather defined as sequences of characters, we might define a TLM \li{$str} to recover string literal notation and write \li{$html [|SURL<body>\{EURLheading $str [|SSTRWorld!ESTR|]SURL\} ...</body>EURL|]}.

% A program editor or pretty-printer can communicate the type of each spliced expression, also specified abstractly by the segmentation, upon request (for Reason, via Merlin \cite{Merlin}.) %When there is a type mismatch, the type annotation on the spliced segment also allows the error message to report the problem without revealing the full expansion.

% \subsection{Proto-Expansion Validation}\label{sec:uetsms-validation}
% Three important concerns described in Sec. \ref{sec:intro} remain: those related to reasoning abstractly about the \emph{hygiene properties}, i.e. \textbf{Capture} and \textbf{Context Dependence}, and \textbf{Typing}. Addressing these concerns is the purpose of the \emph{proto-expansion validation} process, which occurs during the typing phase. 
% Proto-expansion validation results in the \emph{final expansion}, which is simply the proto-expansion with the references to spliced segments replaced with their own final expansions. 


% \subsubsection{Capture}\label{sec:capture}

% Proto-expansion validation ensures that spliced terms have access \emph{only} to the bindings at the application site---spliced terms cannot capture bindings internal to the proto-expansion. 
% %We will show an alternative formulation of Haskell's syntax for monadic commands that uses Reason's anonymous function syntax to bind variables in Sec. \ref{sec:application-monadic-commands}. 

% \subsubsection{Context Dependence}\label{sec:context-dependence}
% %The prohibition on shadowing ensures only that variables that appear in spliced terms do not refer to bindings that appear in the surrounding expansion. 
% The proto-expansion validation process also ensures that variables that appear in the proto-expansion do not refer to bindings that appear either at the TLM definition or the application site. In other words, expansions must be completely \emph{context independent} -- they can make no assumptions about the surrounding context whatsoever. 

% A minimal example of a ``broken'' TLM that does not generate context-independent proto-expansions is below:
% \begin{lstlisting}[numbers=none]
%     syntax $broken at t by static { 
%       fun(_) => Success (Var "SSTRxESTR") };
% \end{lstlisting}
% The proto-expansion that this TLM generates (for any literal body) refers to a variable \li{x} that it does not itself bind. If proto-expansion validation permitted such a proto-expansion, it would be well-typed only under those application site typing contexts where \li{x} is bound. This ``hidden assumption'' makes reasoning about binding and renaming difficult.

% Of course, this prohibition does not extend into the spliced terms in a proto-expansion -- spliced terms appear at the application site, so they can justifiably refer to application site bindings. (like \li{first_name} in Fig.~\ref{fig:first-tsm-example}.) Because proto-expansions refer to spliced terms indirectly, enforcing context independence is straightforward -- we need only that the proto-expansion itself be closed.% In the next section, we will formalize this intuition. % The TLM provider can only refer to them opaquely.

% Na\"ively, this restriction, also present in the prior work on TSLs \cite{TSLs}, is quite restrictive -- expansions cannot access any library functions. At best, they can require the client to ``pass in'' required library functions via splicing at every application. In Sec. \ref{sec:ptsms}, we will introduce module parameters and partial parameter application to neatly resolve this problem. 


% % This prohibition on context dependence explains why the expansion generated by the TLM application in Sec. \ref{sec:uetsms-usage} cannot make use of the regex value constructors, e.g. \li{Str} and \li{Or}, directly. (In Chapter \ref{chap:ptsms}, we will relax this restriction to allow proto-expansions to access explicit parameters.)

% % Collectively, we refer to the prohibition on capture and the prohibition on context dependence as \emph{hygiene properties}, by conceptual analogy to corresponding properties in term-rewriting macro systems (see Sec. \ref{sec:macro-systems}.) The novelty here comes from the fact that spliced terms are being extracted from an initially unparsed sequence of characters.
% % In the examples in Sec. \ref{sec:uetsms-usage} and Sec. \ref{sec:splicing-and-hygiene}, the expansion used constructors associated with the \li{Rx} type, e.g. \li{Seq} and \li{Str}. This might appear to violate our prohibition on context-dependent expansions. This is not the case only because in Reason, constructor labels are not variables or scoped symbols. Syntactically, they must begin with a capital letter (like Haskell's datatype constructors). Different labeled sum types can use common constructor labels without conflict because the type the term is being checked against -- e.g. \li{Rx}, due to the type ascription on \li{$rx} -- determines which type of value will be constructed. For dialects of ML where datatype definitions do introduce new variables or scoped symbols, we need parameterized TLMs. We will return to this topic in Chapter \ref{chap:ptsms}. % Indeed, we used the label \li{Spliced} for two different recursive labeled sum types in Figure \ref{fig:candidate-exp-Reason}.

% \subsubsection{Typing}\label{sec:typing-e}
% Finally, proto-expansion validation maintains a reasonable {typing discipline} by (1) checking that the expansion is of the type specified by the TLM's type annotation; (2) checking that each spliced type is valid; (3) checking that the type annotation on each spliced expression is valid; and (4) checking each spliced expression against the specified type annotation. Context independence implies that ML-style type inference can be performed using only the segmentation (because the remainder of an expansion cannot mention the very variables whose types are being inferred). In the prior work on TSLs, spliced terms did not have type annotations 
% %The OCaml type system is not strong enough to allow us to express only contextually well-typed syntax trees. In The details are discussed as future work in Sec. \ref{sec:discussion}.
%  % This addresses the problem of reasoning abstractly about \textbf{Typing} described in Sec. \ref{sec:intro}, i.e.:
%  % \begin{enumerate}
%  %   \item determining the type of an expansion requires examining only the type annotation on the TLM definition (much as determining the type of a function application requires examining only the function's type); and 
%  % \item determining the type that a spliced expression must have requires only the information in the splice summary (rather than complete knowledge of the proto-expansion).
%  % \end{enumerate}

% % The language \emph{validates} proto-expansions before a final expansion is generated. One aspect of proto-expansion validation is checking  the proto-expansion against the type annotation specified by the TLM, e.g. the type \li{Rx} in the example above. This maintains a \emph{type discipline}: if a programmer sees a TLM being applied when examining a well-typed program, they need only look up the TLM's type annotation to determine the type of the generated expansion. Determining the type does not require examining the expansion directly.


% % \subsection{Hygiene}
% % The spliced subexpressions that the proto-expansion refers to (by their position within the literal body, cf. above) must be parsed, typed and expanded during the proto-expansion validation process (otherwise, the language would not be able to check the type of the proto-expansion). To maintain a useful \emph{binding discipline}, i.e. to allow programmers to reason also about variable binding without examining expansions directly, the validation process maintains two additional properties related to spliced subexpressions: \textbf{context independent expansion} and \textbf{expansion independent splicing}. These are collectively referred to as the \emph{hygiene properties} (because they are conceptually related to the concept of hygiene in term rewriting macro systems, cf. Sec. \ref{sec:term-rewriting}.) 

% % \paragraph{Context Independent Expansion} 

% % \paragraph{Expansion Independent Splicing} 
% % %These properties suffice to ensure that programmers and tools can freely rename a variable without changing the meaning of the program. The only information that is necessary to perform such a \emph{rename refactoring} is the locations of spliced subexpressions within all the literal forms for which the variable being renamed is in scope; the expansions need not otherwise be examined. It would be straightforward to develop a tool and/or editor plugin to indicate the locations of spliced subexpressions to the user, like we do in this document (by coloring spliced subexpressions black). We discuss tool support as future work in Sec. \ref{sec:interaction-with-tools}.

% % \subsubsection{Final Expansion}

% % For example, the final expansion of the body of \li{lookup_rx} is equivalent to the following, under an environment where the regex value constructors are available:
% % \begin{lstlisting}[numbers=none]
% % Seq(Str(name), Seq(Str "SSTR: ESTR", ssn))
% % \end{lstlisting}
% % (Again, due to the prohibition on context dependent expansions, the final expansion actually involves explicit \li{fold} and \li{inj} operators.)


\section{Pattern Literals}
\label{sec:sptsms}

\begin{figure}[t]
\begin{subfigure}[t]{0.55\textwidth}
\begin{lstlisting}[morekeywords={expression}]
  module Lambda = {/* a typical encoding */};
  module LambdaNotation {
    notation $term at Lambda.term { 
      lexer SimpleLangLexer;
      expression parser LambdaParser.term_e;
      pattern parser LambdaParser.term_p;
      dependencies 
        { module Lambda = Lambda } };
    notation $v at Lambda.v {/* analagous */}
  };
\end{lstlisting}
\vspace{-6px}
\caption{Expression and pattern TLMs for \lismall{Lambda} terms and values}
\label{fig:lam}
\vspace{2px}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.42\textwidth}
\begin{lstlisting}
  open LambdaNotation;
  exception Unbound(Lambda.var);
  let rec eval = $term.(fun 
  | `(x)` => raise Unbound(x)
  | `(SURLlamEURL xSURL.EURLe)` => $v `(SURLlamEURL xSURL.EURLe)`
  | `(e1SURL(EURLe2SURL)EURL)` => {
    let $v `(SURLlamEURL xSURL.EURLe)` = eval(e1);
    let v2 = eval(e2);
    eval(`(SURL[EURLv2SURL/EURLxSURL]EURLe)`) 
  });
\end{lstlisting}
\vspace{-6px}
\caption{A reasonably elegant \lismall{Lambda} evaluator}
\label{fig:lam-eval}
\end{subfigure}
\vspace{2px}
\caption{Expression and pattern literal notation for lambda terms and values}
\vspace{-8px}
\label{fig:lam-example}
\end{figure}

The previous section introduced expression TLMs, which support value construction. This section introduces pattern TLMs, which support value deconstruction via the structural pattern matching facilities common to ML-like languages. 

For example, the definitions outlined in Fig. \ref{fig:lam} allow us to elegantly express an evaluator for the lambda calculus as shown in Fig. \ref{fig:lam-eval}. We assume that the terms of \li{Lambda} are represented differently from the values to avoid needing additional ``impossible'' cases, but we can use the same notation for both without conflict by defining separate TLMs, \li{$term} and \li{$v}, respectively. Each of these TLMs defines both expression and pattern literal notation, distinguished by qualifiers in the TLM definition as shown on Lines 5-6 of Fig. \ref{fig:lam}, and we see examples of all four of these notations in use in Fig. \ref{fig:lam-eval}. These TLMs were designed to be used for operating on \li{Lambda} terms, so we chose a splicing convention inspired by the typical convention ``on paper'', where identifiers appearing in the literal are parsed as spliced (OCaml) variables, as indicated by the colors in Fig. \ref{fig:lam-eval}. For example, on Line 5, \li{x} and \li{e} are bound by the literal pattern for \li{Lambda} terms on the left, then spliced into the expression literal on the right to construct the corresponding \li{Lambda} value. % A different pair of TLMs at the same types could make the opposite choice, parsing identifiers as \li{Lambda} language variables and defining a separate notation for splicing. %These would be less elegant for writing the evaluator but more useful when, for example, writing its tests. This sort of flexibility is difficult to achieve in settings where the grammar of the host language is itself extended (e.g. with the more limited mixfix systems of Coq \cite{Coq:manual} or Agda \cite{norell2007towards}) because syntactic conflicts ensue.

From a client programmer's perspective, reasoning principles analagous to those described in Sec. \ref{sec:client-perspective} for expression TLMs are available. \textbf{Responsibility} is assigned by the same protocol, and the type annotation on the responsible TLM governs the type of the generated pattern, satisfying the \textbf{Expansion Typing} condition. Patterns can contain module paths and type annotations, so pattern TLMs are governed by the same \textbf{Context Independence} condition as expression TLMs. The same protocol around \textbf{Segmentation} is also enforced. 

The main novelty has to do with \textbf{Segment Typing} and \textbf{Capture}. Variables in patterns do not refer to existing bindings, as in expressions, but rather introduce bindings into other expressions, e.g. the corresponding branch of a case analysis or the body of a function, so the critical question is this: given only the segmentation of a pattern literal, can we determine exactly which variables the expansion binds, and what the types of these variables are? To answer in the affirmative, we need to preclude the possibility of ``invisible bindings'', so the system ensures that pattern literals bind only those variables that appear inside spliced patterns. This implies that \emph{proto-patterns simply cannot contain pattern variables}. The segmentation assigns a type to each spliced segment, so the question just asked can be answered given only the segmentation.

From the provider's perspective, the parser specified by the \li{pattern parser} clause must generate a proto-pattern, rather than a proto-expression. In Reason, this is a value of type \li{Relit.ProtoPat.t}. Much as with \li{Relit.ProtoExpr.t} from Fig. \ref{fig:relit-util}, this type is defined as a synonym for \li{Parsetree.patteSURLEURLrn} from OCaml's compiler library equipped with an additional function, \li{Relit.ProtoPat.spSURLEURLliced}, that constructs splice references. In principle, it might be better to define \li{Relit.ProtoPat.t} to exclude the representation of pattern variables, and we do just that in the theoretical development in Sec. \ref{sec:setlms-formally}, but \li{Relit} checks this straightforward condition during proto-expansion validation. Note that in OCaml, boolean guards can be associated with rules of a \li{match} (in Reason, \li{switch}) expression, but the guard is not part of the syntax of patterns. (If it were, we would need to validate it as in Sec.~\ref{sec:setlms}). %In order to ensure that a pattern TLM can be applied anywhere a pattern can appear, e.g. in a \li{let} expression as on Line 7 of Fig. \ref{fig:lam-eval}, a pattern TLM cannot generate a guard. % In a language where boolean guards are included in the syntax of patterns, we would need to enforce context independence, capture avoidance and segmentation-related conditions on guard expressions as well.%  In languages where boolean guards can appear at any depth in a pattern, the same 


% \begin{figure}
% \begin{subfigure}[t]{0.51\textwidth}
% \begin{lstlisting}
%   module Lambda = {
%     type var = int;
%     type tm = Var(var) | Lam(var, tm) | Ap(tm, tm);
%     type v  = VLam(var, tm);
%     let subst : v -> var -> tm -> tm = (* ... *)
%   };
% \end{lstlisting}
% \caption{Lambda calculus terms}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.49\textwidth}
% \begin{lstlisting}
%   module Lam = { /* a typical encoding */ };
%   module LamNotation {
%     notation $term at Lam.term { 
%       lexer SimpleLangLexer;
%       expression parser LamParser.tm_expr;
%       pattern parser Lam.tm_pat;
%       dependencies { module Lam }
%     }
%     notation $v at Lam.v { /* analagous */ } 
%   };
% \end{lstlisting}
% \vspace{-4px}
% \caption{Lambda TLM definitions}
% \vspace{2px}
% \end{subfigure}
% \caption{Lambda calculus example}
% \end{figure}


% From the client's perspective, pattern TLMs work much like expression TLMs. Let us briefly review the six reasoning principles from Sec. \ref{sec:setlms}.

% \subsubsection{Responsibility} With respect to responsibility, client programmers can reason as in Sec.~\ref{sec:setlms}, following the usual scoping structure of the language. Notice that a single TLM definition, in this case \li{HtmlNotation.$html}, can define both expression and pattern literal notation, distinguishing the two parsers by qualifiers as shown on Lines 4-5 of Fig. \ref{fig:html-tlm-def}.


% \subsubsection{Expansion Typing} When applying a pattern TLM, the expansion type specified by the TLM definition, again identified relative to the definition site, also classifies the generated pattern.

% \subsubsection{Context Dependence} Patterns can contain module paths and type annotations, so pattern TLMs must be governed by the same context independence condition as expression TLMs.



% \subsubsection{Segmentation} A pattern TLM can splice other patterns out from the provided literal body. The same guarantees with respect to segmentation apply. So, from the colors in Fig. \ref{fig:pattern-tlm-example}, we know where the spliced patterns are. Spliced patterns can be of any valid Reason pattern form. For example, we see Reason's built in list pattern notation in the outermost splice of Fig. \ref{fig:pattern-tlm-example}.

% \subsubsection{Segment Typing} 

% \subsubsection{Capture}

% capture --- here's the rub!!!


%  In languages where guards can be part of patterns and refer to the variables that the pattern binds, the capture avoidance considerations from before apply. 

% \subsection{Provider Perspective}
% Pattern TLM definitions differ from expression TLM definitions in two ways: (1) a \emph{sort qualifier}, \li{for patterns}, distinguishes them from expression TLM definitions; (2) the return type of the parse function is \li{parse_result(proto_pat)}, rather than \li{parse_result(proto_expr)}. The type \li{proto_pat}, outlined below, classifies encodings of \emph{proto-patterns}.
% \begin{lstlisting}[numbers=none]
%    type proto_pat = /* no variable pattern form! */
%                   | Wild
%                   | /* ... */
%                   | SplicedP(segment, proto_typ);
% \end{lstlisting}

% % \subsection{Splicing}
% The constructor \li{SplicedP} operates much like \li{SplicedE} to allow a proto-pattern to refer indirectly to spliced patterns.% by their location within the literal body.


% %  For example, the proto-pattern generated for the example at the top of this section would be written concretely as follows:
% % \begin{lstlisting}[numbers=none]
% %   H1Element (_, spliced<6; 7; list(html)>)
% % \end{lstlisting}


% % \subsection{Proto-Pattern Validation}
% % Proto-pattern validation serves, like proto-expression validation, to maintain the ability to reason abstractly about binding and typing. 

% To maintain the abstract binding discipline, variable patterns can appear only within spliced patterns. Enforcing this restriction is straightforward: we simply have not defined a variant of the \li{proto_pat} type that encodes variable patterns (wildcards are allowed.)  This restriction ensures that only variables visible to the client in a spliced pattern are bound in the corresponding branch expression. This is analagous to the capture avoidance principle for expression TLMs.

% Type annotations on references to spliced patterns could refer to type variables, so we also need to enforce context independence in the manner discussed in the previous section. %Pattern guards are part of pattern matching rules, not patterns themselves, in Reason/OCaml, but were they part of patterns, context independence would also need to be enforced for guard expressions.% In languages like OCaml, which support arbitrary boolean guard expressions, we would also need to enforce both context independence and capture avoidance as discussed in the previous section for spliced expressions that end up in a guard expression. In our formalism, we do not support guard expressions.

% % \subsubsection{Typing} 
% To maintain an abstract typing discipline, proto-pattern validation checks type annotations much as in Sec. \ref{sec:typing-e}.% (1) that the final expansion is a pattern that matches values of the type specified by the TLM's type annotation; (2) that each spliced pattern matches values of the type indicated in the segmentation; and (3) that each of these types are themselves well-formed types.


\section{Parametric TLMs}
\label{sec:ptsms}

All of the examples that we have discussed so far operate at a single specified type. This section introduces \emph{parametric TLMs}, which can take type and module parameters and operate over a type- and module-parameterized family of types. %Parametric TLMs also allow an alternative interpretation of TLM expansion dependencies as partially applied module parameters.
%Moreover, as discussed at the end of Sec. \ref{sec:context-dependence}, the expansions that they generate have no access to libraries. This section introduces \emph{parametric TLMs} to neatly resolve both of these limitations (which also limited the prior work on TSL \cite{TSLs}.) Parametric TLMs can be defined over a type- and module-parameterized family of types, and the proto-expansions they generate can refer to supplied type and module parameters. Partial parameter application decreases the syntactic cost of this explicit parameter passing style. 
% \subsection{Parametric TLMs By Example}\label{sec:ptsms-by-example}

\begin{figure}
\begin{subfigure}[t]{0.4\textwidth}
\begin{lstlisting}
  module Map = { 
    module type S = {
      type key;
      type t('a);
      let empty : t('a);
      let add : 
        key -> 'a -> t('a) -> t('a);
    /* ... */ }
  }
\end{lstlisting}
\vspace{-7px}
\caption{The \li{Map.S} signature}
\label{fig:map-sig}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.55\textwidth}
\begin{subfigure}[t]{\textwidth}
\begin{lstlisting}
  notation $map(M : Map.S, type a) at M.t(a) {
    lexer  MapLexer; parser MapParser; 
  }
\end{lstlisting}
\vspace{-6px}
\caption{The parametric TLM \li{$map}}
\vspace{5px}
\label{fig:map-tlm}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
\begin{lstlisting}
  notation $stringmap = $map(Misc.StringMap);
  let make_request id req v = $stringmap
    `( "session_id" SURL->EURL string_of_int(id)SURL,EURL
                req SURL->EURL v )`;
\end{lstlisting}
\vspace{-4px}
\caption{Applying \li{$map}}
\label{fig:map-ap}
\end{subfigure}
\end{subfigure}
\vspace{4px}
\caption{Literal notation for all finite map implementations using parametric TLMs.}
\vspace{-5px}
\label{fig:map-fig}
\end{figure}

Fig. \ref{fig:map-sig} shows a portion of the signature \li{Map.S} from the OCaml standard library, which specifies a polymorphic abstract data type \cite{liskov1974programming,harper1997programming} \li{t('a)} of finite maps with keys of type \li{key}. There are many ways to implement this signature. For example, the \li{Map.Make} functor in the standard library implements this signature using balanced binary trees given a module that specifies a \li{key} type equipped with comparison functions. The \li{Misc.StringMap} module in the standard library also provides a specialized implementation of this signature with type \li{key = string}.

We can define a TLM, \li{$map}, that is parametric over implementations of this signature, \li{M : Map.S}, and over choices of the co-domain type, \li{a}, as shown in Fig. \ref{fig:map-tlm}. We can then partially apply \li{$map} to \li{Misc.StringMap} as shown on Line 1 of Fig.~\ref{fig:map-ap}, producing a TLM, \li{$stringmap},  parameterized only over the type \li{a}, with expansion type \li{Misc.StringMap.t(a)}. We can apply \li{$stringmap} to a type specialize it further, e.g. \li{$stringmap string} has expansion type \li{Misc.StringMap.t(string)}.  For TLMs where all remaining parameters are types mentioned in the expansion type, we can also immediately apply the TLM to a generalized literal form as shown on Lines~2-3 of Fig.~\ref{fig:map-ap}. The type parameters are determined by unification using the types inferred for the spliced expressions together with the segment types specified in the segmentation, and any surrounding type constraints. In this example, we can infer \li{a} to be \li{string} because \li{string_of_int(id) : string} and the corresponding segment type is \li{a}. TLM abbreviations can themselves be parameterized to support partial application of parameters other than the last.

% For example, given some module that implements this signature, \li{HashDict : DICT}, we can apply \li{$dict} as follows:
% \begin{lstlisting}[numbers=none]
%     $dict HashDict int [|"key1"SURL=>EURL10SURL; EURL"key2"SURL=>EURL15|]
% \end{lstlisting}
% Notice that the segmentation immediately reveals which punctuation is particular to this TLM and where the spliced key and value expressions appear. Because the context-free syntax of unexpanded terms is never modified, it is possible to reason modularly about syntactic determinism, i.e. we can reason above that \li{=>} does not appear in the follow set of unexpanded expressions \cite{conf/pldi/SchwerdfegerW09}, so there can never be an ambiguity about where a key expression ends.

The proto-expansion generated by a parametric TLM can refer to its parameters. Validation checks that the proto-expansion is truly parametric---in this case, the proto-expansion must be valid for all modules \li{M : Map.S} and types \li{a}. It is only after validation that we substitute the actual parameters, here \li{Misc.StringMap} for \li{M} and \li{string} for \li{a}, into the final expansion. 

%Only the type annotations on references to spliced terms are subject to early parameter substitution (because they classify application site terms.)

% If we will use the \li{HashDict} implementation ubiquitously, we can abbreviate the partial application of \li{$dict} to \li{HashDict}, resulting in a TLM that is parametric over only the type \li{'a}:
% \begin{lstlisting}[numbers=none]
%     syntax $dict' = $dict HashDict; 
% \end{lstlisting}

The \li{dependencies} clause shown in previous examples can be understood in terms of parameterization over the dependencies, using the given module variables, followed immediately by partial application of the corresponding module paths. Note, however, that module parameters need an explicit signature, like functor (module function) arguments in ML.

An alternative point-of-view in ML is to treat the \li{dependencies} clause as fundamental and achieve parameterization using module functions (functors), e.g. we could express \li{$map} as follows:
\begin{lstlisting}[numbers=none]
  module MapNotation = (M : Map.S, A : { type a; }) => {
    notation $map at M.t(A.a) { 
      lexer MapLexer; parser MapParser; 
      dependencies { module M = M; type a = A.a; } } }
\end{lstlisting}
% The conveniences related to type parameter inference are not available when parameterization is encoded in this way.



% In Sec. \ref{sec:context-dependence} we discussed the problem of the strict context independence discipline being too restrictive, in that it would seem to restrict expansions from referring to useful helpers bound at the TLM definition site. Module parameters address this problem -- the helper values, types and modules can be packaged into a module, passed in and partially applied to hide this detail from clients. Because this will be common in practice, we provide the following shorthand:
% \begin{lstlisting}[numbers=none]
%     syntax $a at t using X~${}_1$~=M~${}_1$~,...,X~${}_n$~=M~${}_n$~ by ...
% \end{lstlisting} 
% % This is the same as a definition that specifies $n$ type, expression or module parameters and immediately applies the given arguments, binding the result to the same TSM name. 

% This explicit parameter passing discipline is reminiscent of work on explicit capture for distributed functions \cite{DBLP:conf/ecoop/MillerHO14}. By not implicitly giving expansions access to all definition-site bindings, we need not examine parse functions to reason about, e.g., renaming. Consequently, encodings of proto-terms can be values of standard datatypes (e.g. \li{proto_expr}) with variables represented simply as, e.g., strings, and quasiquotation notation can be expressing using TLMs (see supplement). A central design goal of Reason is to leave the OCaml semantics unchanged, so building quasiquotation in primitively, to integrate the free variables in term encodings into the overall binding discipline \cite{Bawd99a}, was in any case infeasible.


% \newcommand{\moreExamplesSec}{Additional Examples}
% \section{\protect\moreExamplesSec}
% \label{sec:more-examples}

\begin{comment}
\newcommand{\staticEvalSec}{Static Evaluation}
\section{\protect\staticEvalSec}
\label{sec:static-eval}

% It is important to note that module parameters are accessible by expansions, but not by parse functions directly, because the applied module parameters will not have been dynamically instantiated when typed expansion occurs. We will discuss a distinct mechanism for providing helper functions to parse functions in Sec. \ref{sec:static-eval}. 

There is one more major impracticality from the prior work on TSLs \cite{TSLs} that we will now briefly address: in the prior work, parse functions had no access to any libraries, nor any ability to themselves apply TLMs. To address this problem, we introduce a \emph{static environment}. 

\subsection{The Static Environment}
Figure \ref{fig:static-module-example} shows an example of a module, \li{ParserCombos}, that defines a number of parser combinators following \citet{Hutton1992d}. The \li{static} qualifier indicates that this module is bound for use only within similarly qualified values, including in particular parse functions of subsequent TLM definitions.
\begin{figure}[h]
\begin{lstlisting}[numbers=none]
  static module ParserCombos = { 
    type p('c, 't) = list('c) -> list('t, list('c));
    let alt : p('c, 't) -> p('c, 't) -> p('c, 't);
    /* ... */
  };
  syntax $a at t by static { fun(b) => 
    ParserCombos.alt /* ... OK */ };
  static let y = ParserCombos.alt /* ... OK */;
  let z = /* ... ParserCombos nor y bound here */;
\end{lstlisting}
\caption{Binding static modules and values for use within parse functions.}
\label{fig:static-module-example}
\end{figure}

The values that arise when the static phase runs do not persist from ``compile-time'' to ``run-time'', so we do not need a full staged computation system, e.g. as described by \citet{Taha99multi-stageprogramming:}. Instead, a sequence of static bindings operates like a read-evaluate-print loop (REPL) scoped according to the program structure, in that each static expression is evaluated immediately and the evaluated values are tracked by a \emph{static environment}, which is discarded after expansion finishes. We do not restrict the language features available to the static phase, though this is a worth considering in languages where that is more straightforward (e.g. to ensure deterministic builds.)

% A language designer might choose to restrict the external effects available to static terms in some way, e.g. to ensure deterministic builds. It might also be helpful to restrict mutable state shared between TLMs to prevent undesirable TLM application order dependencies. On the other hand, these features, if used for the purposes of caching, might speed up typed expansion. These are orthogonal design decisions.

\subsection{Applying TLMs Within TLM Definitions}\label{sec:tsms-for-tsms}
TLMs and TLM abbreviations can also be qualified with the \li{static} keyword, which marks them for use within subsequent static expressions and patterns. Let us consider some examples of relevance to TLM providers.

\subsubsection{Quasiquotation}\label{sec:quasiquotation}
TLMs must construct values of type \li{proto_expr} or \li{proto_pat}. Constructing values of these types explicitly can have high syntactic cost. To decrease this cost, we can define TLMs that provide support for \emph{quasiquotation syntax} similar to that built in to languages like Lisp \cite{Bawd99a} and Scala \cite{shabalin2013quasiquotes}. The following TLM defines quasi-quotation for encodings of proto-expressions:
\begin{lstlisting}[numbers=none]
  static syntax $proto_expr at proto_expr by static { /* ... */ };
\end{lstlisting}
For example, \li{$proto_expr `SQTg x ^EQTx`} might have expansion \li{App(App(Var "g", Var "x"), x)}. Notice that prefixing a variable (or parenthesized expression) with \li{^} serves to splice in its value, which here must be of type \li{proto_expr} (though in other syntactic positions, it might be \li{proto_typ}.) This is also known as \emph{anti-quotation}.

% A similar approach can be taken for working with encodings of terms of other languages (e.g. when writing an interpretter or compiler in Reason.)

\subsubsection{Parser Generators}
Abstractly, a grammar-based parser generator is a module matching the signature \li{PARSEGEN} defined below:
\begin{lstlisting}
  module type PARSEGEN = { 
    type grammar('a);
    /* ... operations on grammars ... */
    val generate : grammar('a) -> (body -> parse_result('a));
  };
\end{lstlisting}

Rather than constructing a grammar equipped with semantic actions using the associated operations (whose specifications are elided in \li{PARSEGEN}), we wish to use a syntax for context-free grammars that follows standard conventions. We can do so by defining a static parametric TLM:
\begin{lstlisting}[numbers=none]
  static syntax $grammar(P : PARSEGEN)(type 'a) at P.grammar('a) /*...*/
\end{lstlisting}
To support splicing, we need non-terminals that recognize unexpanded terms and produce the corresponding splice references, rather than the AST itself. This requires that the parser generator keep track of location information (as most production-grade parser generators already do for error reporting.) A more detailed example of this mechanism being used to define a TLM for a modular encoding of regular expressions is given in the supplement.% For spliced expressions, this non-terminal would need to be a family of non-terminals indexed by a value of type \li{proto_typ}. 

A grammar containing such non-terminals can serve as a \emph{summary specification} -- a human can simply be take this grammar as a specification of what the segmentation will be for every recognized string, rather than relying on an editor to communicate this information. The associated semantic actions can be held abstract as long as the system performs a simple check to ensure that the proto-expansion does mention each spliced expression from the corresponding production.

% \begin{figure}[h!]
% \begin{lstlisting}[deletekeywords={as}]
% syntax $rx(R : RX) at R.t by static 
%   P.generate ($grammar P proto_expr {|SHTML #\label{line:rx_parse_fn_start}#
%     start <- ""
%       EHTMLfn () => $proto_expr `SCSSR.EmptyECSS`SHTML
%     start <- "(" start ")"
%       EHTMLfn e => eSHTML
%     token str_tok #\label{line:str_tok_start}#
%       EHTMLRU.parse "SSTR[^(@$]+ESTR" /* cannot use $rx within its own def */SHTML #\label{line:str_tok_end}#
%     start <- str_tok
%       EHTMLfn s => $proto_expr `SCSSR.Str %(ECSSstr_to_proto_lit sSCSS)ECSS`SHTML
%     start <- start start
%       EHTMLfn e1 e2 => $proto_expr `SCSSR.Seq (%ECSSe1SCSS, %ECSSe2SCSS)ECSS`SHTML
%     start <- start "|" start 
%       EHTMLfn e1 e2 => $proto_expr `SCSSR.Or (%ECSSe1SCSS, %ECSSe2SCSS)ECSS`SHTML
%     start <- start "*"
%       EHTMLfn e => $proto_expr `SCSSR.Star %ECSSe`SHTML

%     using EHTMLspliced_uexp ($proto_typ `SCSSR.tECSS`) SHTML as spliced_rx #\label{line:splicede_using}#
%     start <- "${" spliced_rx "}" #\label{line:splicing-start}#
%       EHTMLfn e => eSHTML

%     using EHTMLspliced_uexp ($proto_typ `SCSSstringECSS`) SHTML as spliced_str
%     start <- "@{" spliced_str "}"
%       EHTMLfn e => $proto_expr `SCSSR.Str %(ECSSeSCSS)ECSS`SHTML #\label{line:splicing-end}#
%   EHTML|})
% end #\label{line:rx_parse_fn_end}#
% \end{lstlisting}
% \caption{A grammar-based definition of \texttt{\$rx}.}
% \label{fig:rx-grammar-based}
% \end{figure}

\subsection{Static Evaluation, Formally}
It is not difficult to extend $\miniVerseParam$ to account for static evaluation. Static environments, $\Sigma$, take the form $\staticenv{\omega}{\uOmega}{\uPsi}{\uPhi}$, where $\omega$ is a substitution. Each binding form is annotated with a \emph{phase}, $\phi$, either $\staticphase$ or $\standardphase$. The rules for binding forms annotated with $\standardphase$ are essentially unchanged, differing only in that $\Sigma$ passes through opaquely. The rules for binding forms annotated with $\staticphase$ are based on the corresponding $\standardphase$ phase rules, differing only in that 1) they operate on $\Sigma$ and 2) evaluation occurs immediately. Finally, the forms for TLM definition are modified so that the parse function is now an unexpanded, rather than an expanded expression. The substitution $\omega$ is applied to the parse function after it is expanded. The full details are defined as a small patch of $\miniVerseParam$ called $\miniVersePH$ in the supplement.

\subsection{Library Management}
In the examples above, and in our formal treatment, we explicitly qualified various definitions with the \li{static} keyword to make them available within static values. In practice, we would like to be able to use libraries within both static values and standard values as needed without duplicating code. This can be accomplished either by the package manager (e.g. SML/NJ's CM \cite{blume:smlnj-cm}, extended with phase annotations) or by allowing one to explicitly lower an instance of a module define in the standard phase for use also in  the static phase, as in a recent proposal for modular staging macros in OCaml \cite{Ocaml/macros}.

TLMs definitions can be exported from the top level of packages, but they cannot be exported from within ML-style modules because that would require that they also appear in signatures, and that, in turn, would complicate reasoning about signature equivalence, since TLM definitions contain arbitrary parse functions. 
%It would also bring in confusion about whether the generated expansions can use private knowledge about type identity. 
That said, it should be possible to export TLM \emph{abbreviations} from modules, since they refer to TLM definitions only through symbolic names. We have not yet formalized this intuition, but the work of \citet{culpepper2005syntactic,culpepper2007advanced} considered a closely related question: how should Typed Scheme's macros interact with its unit (i.e. package) system.



% For example, a language-external library manager for Reason similar to SML/NJ's CM \cite{blume:smlnj-cm} could support a \li{static} qualifier on imported libraries, which would place the definitions exported by the imported library into the static phase of the library being defined. In particular, a library definition in such a compilation manager might look like this:
% \begin{lstlisting}[numbers=none,morekeywords={Library,is}]
% Library 
%   /* ... exported module, signature and TLM names ... */
% is 
%   /* ... files defining those exports ... */

%   /* imports: */
%   static parsegen.cm 
% \end{lstlisting}

% A similar approach could be taken for languages the incorporate library management directly into the syntax of programs, e.g. Scala \cite{odersky2008programming}:
% \begin{lstlisting}[numbers=none]
% static import edu.cmu.comar.parsegen
% \end{lstlisting}


% \begin{equation}\label{rule:mExpandsPH-syntaxpe-standard}
% \inferrule{
%   \tsmtyExpands{\uOmega}{\urho}{\rho}\\
%   \Sigma = \staticenv{\omega}{\uOmega_S}{\uPsi_S}{\uPhi_S}\\\\
%   \expandsP{\uOmega_S}{\uPsi_S}{\uPhi_S}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultPCEExp}}\\\\
%   \evalU{[\omega]\eparse}{\eparse'}\\
%   \mExpandsPH{\uOmega}{\uAS{\uA \uplus \mapitem{\tsmv}{\adefref{a}}}{\Psi, \petsmdefn{a}{\rho}{\eparse'}}}{\uPhi}{\uM}{M}{\sigma}{\Sigma}
% }{
%   \mExpandsPH{\uOmega}{\uAS{\uA}{\Psi}}{\uPhi}{\defpetsmH{\standardphase}{\tsmv}{\urho}{\ueparse}{\uM}}{M}{\sigma}{\Sigma}
% }
% \end{equation}
% \begin{equation}\label{rule:mExpandsPH-syntaxpe-static}
% \inferrule{
%   \tsmtyExpands{\uOmega}{\urho}{\rho}\\
%   \Sigma = \staticenv{\omega}{\uOmega_S}{\uPsi_S}{\uPhi_S}\\
%   \uPsi_S = \uAS{\uA_S}{\Psi_S}\\\\
%   \expandsP{\uOmega_S}{\uPsi_S}{\uPhi_S}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultPCEExp}}\\\\
%   \evalU{[\omega]\eparse}{\eparse'}\\
%   \mExpandsPH{\uOmega}{\uPsi}{\uPhi}{\uM}{M}{\sigma}{\staticenv{\omega}{\uOmega_S}{\uAS{\uA_S \uplus \mapitem{\tsmv}{\adefref{a}}}{\Psi_S, \petsmdefn{a}{\rho}{\eparse'}}}{\uPhi_S}}
% }{
%   \mExpandsPH{\uOmega}{\uPsi}{\uPhi}{\defpetsmH{\staticphase}{\tsmv}{\urho}{\ueparse}{\uM}}{M}{\sigma}{\Sigma}
% }
% \end{equation}
\end{comment}

\section{Typed Literal Macros, Formally}
\label{sec:setlms-formally}

This section will present a calculus of simple expression and pattern TLMs based on ``core ML'' called $\miniVersePat$. By the end of this section, we will have a theorem that formalizes the six reasoning principles that were outlined informally in the previous sections. $\miniVersePat$ consists of an \emph{unexpanded language}, or \emph{UL}, defined by typed expansion to the core language, named for the purposes of this paper $\miniVerseBase$. 

% Because our focus is on these reasoning principles, and for reasons of space, we leave our full calculus of parametric TLMs to the supplement. The full calculus extends the simple calculus of this section with an ML module calculus based closely on the system defined by \citet{pfple1}, which in turn is based on early work by \citet{MacQueen:1984:MSM:800055.802036,DBLP:conf/popl/MacQueen86}, subsequent work on the phase splitting interpretation of modules \cite{harper1989higher} and on using dependent singleton kinds to track type identity \cite{stone2006extensional,DBLP:conf/lfmtp/Crary09}, and finally on formal developments by \citet{dreyer2005understanding} and \citet{conf/popl/LeeCH07}. These additional mechanisms are necessary only to formalize the advanced features of Sec. \ref{sec:ptsms}. Proofs are in the supplement for both the simple and full calculus.


% Programs are written as unexpanded expressions but evaluate as well-typed expanded expressions. We will start with a brief overview of our XL before turning in the remainder of the section on the UL.

\subsection{Core Language}\label{sec:s-XL}
Fig. \ref{fig:U-expanded-terms} gives the syntax of the core language, $\miniVerseBase$. This language forms a standard pure functional language with partial function types, quantification over types, recursive types, labeled product types, labeled sum types and support for pattern matching.\footnote{The supplement shows how to remove pattern matching  (and pattern TLMs) from the calculus by adding the usual elimination forms, e.g. $\unfold{e}$ rather than the $\aefoldp{p}$ pattern for values of recursive type.} Formally, core language terms are {abstract binding trees} (ABTs) identified up to alpha-equivalence, so we follow the syntactic conventions of \citet{pfple1}. This notational convention also clearly distinguishes core terms from unexpanded terms, which formally behave quite differently as we will describe below. 


 The reader is directed to \emph{PFPL} \cite{pfple1} for a detailed introductory account of all of these language constructs. We will only tersely summarize the static and dynamic semantics of the core language because the particularities are not critical to our ideas. The static semantics is organized around the type formation judgement, $\istypeU{\Delta}{\tau}$, the expression typing judgement, $\hastypeU{\Delta}{\Gamma}{e}{\tau}$, and the pattern typing judgement, $\patType{\pctx}{p}{\tau}$. Type formation contexts, $\Delta$, track hypothesis of the form $\Dhyp{t}$, and typing contexts, $\Gamma$, track hypotheses of the form $x : \tau$. In the pattern typing judgement, $\pctx$ collects the typing hypotheses generated by $p$. These judgements are inductively defined in the supplemental material along with necessary auxiliary structures and  standard lemmas. The dynamic semantics of $\miniVerseBase$ is organized around the judgements $\isvalU{e}$, which says that $e$ is a value, and $\evalU{e}{e'}$, which says that $e$ evaluates to the value $e'$. As in ML, evaluation can diverge (general recursion is possible via recursive types \cite{pfple1}). It can also result in match failure.%\emph{Type formation contexts}, $\Delta$, and \emph{typing contexts}, $\Gamma$ are defined in the supplement in the standard way \cite{pfpl}, 
% \[\begin{array}{ll}
% % \textbf{Judgement Form} & \textbf{Description}\\
% \istypeU{\Delta}{\tau} & \text{$\tau$ is a well-formed type}\\
% %\isctxU{\Delta}{\Gamma} & \text{$\Gamma$ is a well-formed typing context assuming $\Delta$}\\
% \hastypeU{\Delta}{\Gamma}{e}{\tau} & \text{$e$ is assigned type $\tau$}\\
% \ruleType{\Delta}{\Gamma}{r}{\tau}{\tau'} & \text{$r$ takes values of type $\tau$ to $\tau'$}\\
% \patType{\pctx}{p}{\tau} & \text{$p$ matches values of type $\tau$}\\
% & \text{and generates hypotheses $\pctx$} 
% \end{array}\]
% \emph{Type formation contexts}, $\Delta$, are finite sets of hypotheses of the form $\Dhyp{t}$. %Empty finite sets are written $\emptyset$, or omitted entirely within judgements, and non-empty finite sets are written as comma-separated finite sequences identified up to exchange and contraction. We write $\Delta, \Dhyp{t}$ when $\Dhyp{t} \notin \Delta$ for $\Delta$ extended with the hypothesis $\Dhyp{t}$. %Finite sets are written as finite sequences identified up to exchange.% We write $\Dcons{\Delta}{\Delta'}$ for the union of $\Delta$ and $\Delta'$.
% \emph{Typing contexts}, $\Gamma$, are finite functions that map each variable $x \in \domof{\Gamma}$, where $\domof{\Gamma}$ is a finite set of variables, to the hypothesis $\Ghyp{x}{\tau}$, for some $\tau$. The judgements above are inductively defined in the supplemental material and validate standard lemmas, also given in the supplement.
%Empty typing contexts are written $\emptyset$, or omitted entirely within judgements, and non-empty typing contexts are written as finite sequences of hypotheses identified up to exchange and contraction. We write $\Gamma, \Ghyp{x}{\tau}$, when $x \notin \domof{\Gamma}$, for the extension of $\Gamma$ with a mapping from $x$ to $\Ghyp{x}{\tau}$, and $\Gcons{\Gamma}{\Gamma'}$ when $\domof{\Gamma} \cap \domof{\Gamma'} = \emptyset$ for the typing context mapping each $x \in \domof{\Gamma} \cup \domof{\Gamma'}$ to $x : \tau$ if $x : \tau \in \Gamma$ or $x : \tau \in \Gamma'$. % We write $\isctxU{\Delta}{\Gamma}$ if every type in $\Gamma$ is well-formed relative to $\Delta$.
% \begin{definition}[Typing Context Formation] \label{def:isctxU}
% $\isctxU{\Delta}{\Gamma}$ iff for each hypothesis $x : \tau \in \Gamma$, we have $\istypeU{\Delta}{\tau}$.
% \end{definition}

% \begin{subequations}\label{rules:istypeU}
% \begin{equation*}\label{rule:istypeU-var}
% \inferrule{ }{\istypeU{\Delta, \Dhyp{t}}{t}}
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-parr}
% \inferrule{
%   \istypeU{\Delta}{\tau_1}\\
%   \istypeU{\Delta}{\tau_2}
% }{\istypeU{\Delta}{\aparr{\tau_1}{\tau_2}}}
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-all}
%   \inferrule{
%     \istypeU{\Delta, \Dhyp{t}}{\tau}
%   }{
%     \istypeU{\Delta}{\aall{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-rec}
%   \inferrule{
%     \istypeU{\Delta, \Dhyp{t}}{\tau}
%   }{
%     \istypeU{\Delta}{\arec{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-prod}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}
%   }{
%     \istypeU{\Delta}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-sum}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}
%   }{
%     \istypeU{\Delta}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \end{subequations}
% Premises of the form $\{{J}_i\}_{i \in \labelset}$ mean that for each $i \in \labelset$, the judgement ${J}_i$ must hold. 


% \begin{subequations}\label{rules:hastypeU}
% \begin{equation*}\label{rule:hastypeU-var}
%   \inferrule{ }{
%     \hastypeU{\Delta}{\Gamma, \Ghyp{x}{\tau}}{x}{\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-lam}
%   \inferrule{
%     \istypeU{\Delta}{\tau}\\
%     \hastypeU{\Delta}{\Gamma, \Ghyp{x}{\tau}}{e}{\tau'}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-ap}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e_1}{\aparr{\tau}{\tau'}}\\
%     \hastypeU{\Delta}{\Gamma}{e_2}{\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aeap{e_1}{e_2}}{\tau'}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-tlam}
%   \inferrule{
%     \hastypeU{\Delta, \Dhyp{t}}{\Gamma}{e}{\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aetlam{t}{e}}{\aall{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-tap}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\aall{t}{\tau}}\\
%     \istypeU{\Delta}{\tau'}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aetap{e}{\tau'}}{[\tau'/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-fold}
%   \inferrule{\
%     \istypeU{\Delta, \Dhyp{t}}{\tau}\\
%     \hastypeU{\Delta}{\Gamma}{e}{[\arec{t}{\tau}/t]\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aefold{e}}{\arec{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-unfold}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\arec{t}{\tau}}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aeunfold{e}}{[\arec{t}{\tau}/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-tpl}
%   \inferrule{
%     \{\hastypeU{\Delta}{\Gamma}{e_i}{\tau_i}\}_{i \in \labelset}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aetpl{\labelset}{\mapschema{e}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-pr}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\aprod{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \ell \hookrightarrow \tau}}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aepr{\ell}{e}}{\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-in}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}\\
%     \istypeU{\Delta}{\tau}\\
%     \hastypeU{\Delta}{\Gamma}{e}{\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aein{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \ell \hookrightarrow \tau}{e}}{\asum{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \ell \hookrightarrow \tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-case}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}\\
%     \istypeU{\Delta}{\tau}\\
%     \{\hastypeU{\Delta}{\Gamma, x_i : \tau_i}{e_i}{\tau}\}_{i \in \labelset}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aecase{\labelset}{e}{\mapschemab{x}{e}{i}{\labelset}}}{\tau}
%   }
% \end{equation*}
% \end{subequations}

%Rules (\ref{rules:istypeU}) and (\ref{rules:hastypeU}) are syntax-directed, so we assume an inversion lemma for each rule as needed without stating it separately. 
% The following standard lemmas also hold. 

\begin{figure}
%\hspace{-5px}
\begin{minipage}{\textwidth}
\small
$\arraycolsep=2pt\begin{array}{llcl}
\mathsf{Typ} & \tau & ::= & t ~\vert~ \aparr{\tau}{\tau} ~\vert~ \aall{t}{\tau} ~\vert~ \arec{t}{\tau} ~\vert~ \aprod{\labelset}{\mapschema{\tau}{i}{\labelset}} ~\vert~ \asum{\labelset}{\mapschema{\tau}{i}{\labelset}}\\
\mathsf{Exp} & e & ::= & x ~\vert~ \aelam{\tau}{x}{e} ~\vert~ \aeap{e}{e} ~\vert~ \aetlam{t}{e} ~\vert~ \aetap{e}{\tau} ~\vert~ \aefold{e} ~\vert~ \aetpl{\labelset}{\mapschema{e}{i}{\labelset}} ~\vert~  \aein{\ell}{e} \\
& & \vert & \aematchwith{n}{e}{\seqschemaX{r}}\\
\mathsf{Rule} & r & ::= & \aematchrule{p}{e}\\
\mathsf{Pat} & p & ::= & x  ~\vert~ \aewildp ~\vert~ \aefoldp{p} ~\vert~ \aetplp{\labelset}{\mapschema{p}{i}{\labelset}} ~\vert~ \aeinjp{\ell}{p}
\end{array}$
\end{minipage}
\caption[Syntax of the core language of $\miniVersePat$]{Syntax of the core language, $\miniVerseBase$, which is an entirely standard typed lambda calculus.  Metavariable $x$ ranges over variables, $t$ over type variables, $\ell$ over labels and $L$ over finite sets of labels. We write $\mapschema{\tau}{i}{\labelset}$ for a finite mapping of each label $i$ in $\labelset$ to some type $\tau_i$, and similarly for other sorts of terms. We write $\seqschemaX{r}$ for a finite sequence of $n > 0$  rules. %When using stylized forms, the label set is omitted when it can be inferred, e.g. the labeled product type $\prodt{\finmap{\mapitem{\ell_1}{e_1}, \mapitem{\ell_2}{e_2}}}$ leaves the label set $\{\ell_1, \ell_2\}$ implicit. 
% When we use the stylized forms, we assume that the reader can infer suppressed indices and arguments from the surrounding context.
}
\label{fig:U-expanded-terms}
\end{figure}

\begin{figure}[t!]
\begin{minipage}{\textwidth}
\small
$\arraycolsep=2pt\begin{array}{llcl}
\mathsf{UTyp} & \utau & ::= & 
\ut ~\vert~ 
\parr{\utau}{\utau} ~\vert~
\forallt{\ut}{\utau} ~\vert~
\rect{\ut}{\utau} ~\vert~
\prodt{\mapschema{\utau}{i}{\labelset}} ~\vert~
\sumt{\mapschema{\utau}{i}{\labelset}}\\
\mathsf{UExp} & \ue & ::= & 
\ux ~\vert~
% \asc{\ue}{\utau} ~\vert~
% \letsyn{\ux}{\ue}{\ue} & \text{value binding}\\
\lam{\ux}{\utau}{\ue} ~\vert~
\ap{\ue}{\ue} ~\vert~
\Lam{\ut}{\ue} ~\vert~
\App{\ue}{\utau} ~\vert~
\fold{\ue} ~\vert~
\tpl{\mapschema{\ue}{i}{\labelset}} ~\vert~
\inj{\ell}{\ue} \\
& & \vert & 
\matchwith{\ue}{\seqschemaX{\urv}}
 \\
& & \vert & \uesyntaxq{\tsmv}{\utau}{e}{\ue} \\   
& & \vert & \usyntaxup{\tsmv}{\utau}{e}{\ue} \\
&&\vert&  \utsmap{\tsmv}{b} \\
\mathsf{URule} & \urv & ::= & 
%& \aumatchrule{\upv}{\ue} 
\matchrule{\upv}{\ue} \\
\mathsf{UPat} & \upv & ::= & 
%& \ux 
\ux ~\vert~
\wildp ~\vert~ 
\foldp{\upv} ~\vert~
\tplp{\mapschema{\upv}{i}{\labelset}} ~\vert~
\injp{\ell}{\upv} ~\vert~
\utsmap{\tsmv}{b}

% \LCC  &  & 
% %& \lightgray 
% & \color{Yellow} & \color{Yellow} \\
% &&
% %& \audefuetsm{\utau}{e}{\tsmv}{\ue} 
% & \uesyntax{\tsmv}{\utau}{e}{\ue} & \text{seTLM definition}\\ 
% &&
% %& \autsmap{b}{\tsmv} 
% & \utsmap{\tsmv}{b} & \text{seTLM application}\ECC
\end{array}$
\end{minipage}
\caption[Syntax of the $\miniVersePat$ unexpanded language (UL)]{Syntax of the $\miniVersePat$ unexpanded language (UL). Metavariable $\ut$ ranges over type identifiers, $\ux$ over expression identifiers, $\tsmv$ over TLM identifiers and $b$ over literal bodies. 
}
\label{fig:U-unexpanded-terms}
\end{figure}
% \subsubsection{Evaluation Semantics}\label{sec:dynamics-U}
%The \emph{evaluation semantics} of $\miniVersePat$ is organized around the judgements $\isvalU{e}$, which says that $e$ is a value, and $\evalU{e}{e'}$, which says that $e$ evaluates to the value $e'$.% Additional  judgements, not shown, are needed to define the dynamics of pattern matching, but they do not appear directly in our subsequent developments, so we omit them. % We assume an eager dynamics. 

\subsection{Syntax of the Unexpanded Language}\label{sec:syntax-U}\label{sec:s-UL}
Fig. \ref{fig:U-unexpanded-terms} defines the syntax of the unexpanded language. Unlike core language types and expressions, unexpanded expressiona and types are \textbf{not} abstract binding trees -- we do \textbf{not} assume the standard notions of renaming, alpha-equivalence or substitution. Instead, they are simple inductive  structures. This is because unexpanded expressions remain ``partially parsed'' due to the presence of literal bodies, $b$, from which spliced terms might be extracted during expansion. In fact, unexpanded types and expressions do not involve variables at all, but rather \emph{identifiers}, $\ut$ and $\ux$. % This distinction between identifiers and variables is technically crucial to our developments. %We \textbf{cannot} adopt the usual definitions of $\alpha$-renaming of identifiers, because unexpanded types and expressions are still in a ``partially parsed'' state -- the literal bodies, $b$, within an unexpanded expression might contain spliced subterms that are ``surfaced'' by a TLM only during typed expansion, as we will detail below. %identifiers are given meaning by expansion to variables. %In other words, unexpanded expressions are not abstract binding trees, nor sequences of characters, but a ``transitional'' structure with some characteristics of each of these. 
%For this reason, we will need to handle generating fresh variables explicitly at binding sites in our semantics. %To do so, we distinguish \emph{type identifiers}, $\ut$, and \emph{expression identifiers}, $\ux$, from type variables, $t$, and expression variables, $x$. identifiers will be given meaning by expansion to variables (which, in turn, are given meaning by substitution, as described above). 

% There are only two unexpanded expression forms, highlighted in gray in Figure \ref{fig:U-unexpanded-terms}, that do not correspond to expanded expression forms -- the seTLM definition form and the seTLM application form. %These are the ``interesting'' forms. % These are the ``interesting'' forms. % Let us define this correspondence by the metafunction $\Uof{e}$:
%\[
%\begin{split}
%\Uof{x} & = x\\
%\Uof{\aelam{\tau}{x}{e}} & = \aulam{\tau}{x}{\Uof{e}}\\
%\Uof{\aeap{e_1}{e_2}} & = \auap{\Uof{e_1}}{\Uof{e_2}}
%\end{split}
%\] and so on for the remaining expanded expression forms.


There is also a corresponding context-free textual syntax for the UL. 
Giving a complete definition of the context-free textual syntax with, for example, a context-free grammar is not critical to our purpose. 
%Our paper on Wyvern defines a textual syntax for a similar system \cite{TSLs}. 
Instead, we only posit partial metafunctions $\parseUTypF{b}$, $\parseUExpF{b}$ and $\parseUPatF{b}$  that go from character sequences, $b$, to unexpanded types, expressions and patterns, respectively.
% \begingroup
% \def\thetheorem{\ref{condition:textual-representability-SES}}
% \begin{condition}[Textual Representability] ~
% \begin{enumerate}[nolistsep]
% \item For each $\utau$, there exists $b$ such that $\parseUTyp{b}{\utau}$. 
% \item For each $\ue$, there exists $b$ such that $\parseUExp{b}{\ue}$.
% \item For each $\upv$, there exists $b$ such that $\parseUPat{b}{\upv}$.
% \end{enumerate}
% \end{condition}
% \endgroup



\vspace{-5px}
\subsection{Typed Expansion}\label{sec:typed-expansion-U}\label{sec:s-TE}
The \emph{typed expansion judgements} below specify the expansion process, which in the setting of $\miniVersePat$ occurs simultaneously with typing (see Sec. \ref{sec:implementation} for certain nuances in our implementation).
\[\begin{array}{ll}
% \textbf{Judgement Form} & \textbf{Description}\\
\expandsTU{\uDelta}{\utau}{\tau} & \text{$\utau$ has well-formed expansion $\tau$}\\
\expandsUPX{\ue}{e}{\tau} & \text{$\ue$ has expansion $e$ of type $\tau$}\\
% \ruleExpands{\uDelta}{\uGamma}{\uPsi}{\uPhi}{\urv}{r}{\tau}{\tau'} & \text{$\urv$ has expansion $r$ taking values of type $\tau$ to values of type $\tau'$}\\
\patExpands{\upctx}{\uPhi}{\upv}{p}{\tau} & \text{$\upv$ has expansion $p$ matching $\tau$}
\end{array}\]
%\newcommand{\gray}[1]{{\color{gray} #1}}



% These judgements are inductively defined in the supplement. 
% \begingroup 
% \def\thetheorem{\ref{thm:typed-expansion-short-U}}

%These rules validate the following theorem, which establishes that typed expansion produces an expansion of the assigned type. 
%\begin{theorem}[Typed Expression Expansion] If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uPsi}{\ue}{e}{\tau}$ and $\uetsmenv{\Delta}{\uPsi}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.\end{theorem}
%\begin{proof} This is the first part of Theorem \ref{thm:typed-expansion-U}, defined and proven below.\end{proof}


Most of the unexpanded forms in Figure \ref{fig:U-unexpanded-terms}  mirror the expanded forms. We refer to these as the \emph{common forms}. % The mapping from expanded forms to common unexpanded forms is defined explicitly in the supplement. 
The typed expansion rules that handle common forms mirror the corresponding typing rules. The \emph{expression TLM context}, $\uPsi$, and the \emph{pattern TLM context}, $\uPhi$, detailed below pass through these rules opaquely. For example, the rules for variables and lambdas are shown being applied in the example derivation in Fig.~\ref{fig:expansion-exmpl}, discussed below. The full set of rules is in the supplement.
%Each of these rules is based on the corresponding typing rule, i.e. Rules (\ref{rule:hastypeU-var}) through (\ref{rule:hastypeU-case}), respectively. For example, the following typed expansion rules are based on the typing rules (\ref{rule:hastypeU-var}), (\ref{rule:hastypeU-lam}) and (\ref{rule:hastypeU-ap}), respectively:% for unexpanded expressions of variable, function and application form, respectively: 
% {\small\begin{mathpar}
%   \inferrule[ee-id]{ }{\expandsUP{\uDelta}{\uGamma, \uGhyp{\ux}{x}{\tau}}{\uPsi}{\uPhi}{\ux}{x}{\tau}}

%   \inferrule[ee-lam]{
%     \expandsTU{\uDelta}{\utau}{\tau}\\
%     \expandsUP{\uDelta}{\uGamma, \uGhyp{\ux}{x}{\tau}}{\uPsi}{\uPhi}{\ue}{e}{\tau'}
%   }{\expandsUPX{\lam{\ux}{\utau}{\ue}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}}
% \end{mathpar}}





\begin{figure*}
\vspace{-4px}
{\small\begin{mathpar}
\inferrule{
  \inferrule{ }{\expandsTU{\uDelta}{\utau}{\tau}}\\
  \inferrule{
    \inferrule{ }{\expandsTU{\uDelta}{\utau}{\tau}}\\
    \inferrule{ }{
      \expandsUP{\uDelta}{\uGG{\vExpands{\ux}{x_2}}{x_1 : \tau, x_2 : \tau}}{\uPsi}{\uPhi}{\ux}{x_2}{\tau}
    }~\textsc{ee-id}
  }{
    \expandsUP{\uDelta}{\uGG{\vExpands{\ux}{x_1}}{x_1 : \tau}}{\uPsi}{\uPhi}{\lam{\ux}{\utau}{\ux}}{\aelam{\tau}{x_2}{x_2}}{\aparr{\tau}{\tau}}
    % \expandsU{\uDelta}{\uGG}
  }~\textsc{ee-lam}
}{
  \expandsUP{\uDelta}{\uGG{\emptyset}{\emptyset}}{\uPsi}{\uPhi}{\lam{\ux}{\utau}{\lam{\ux}{\utau}{\ux}}}{\aelam{\tau}{x_1}{\aelam{\tau}{x_2}{x_2}}}{\aparr{\tau}{\aparr{\tau}{\tau}}}
}~\textsc{ee-lam}
\end{mathpar}}
\vspace{-6px}
\caption{An example expansion derivation demonstrating how identifiers and variables are separately tracked.}
\label{fig:expansion-exmpl}
\vspace{-8px}
\end{figure*}
The only technical subtlety related to common forms has to do with the relationship between identifiers, $\ux$, in the UL and variables, $x$, in the core language. We might hope to identify identifiers in the UL with variables in the core language and track the bindings in unexpanded terms using typing contexts as defined in  the core language, but we cannot because the only operation for producing a new typing context from an existing typing context is context extension, written $\Gamma, x : \tau$, which is defined only when $x \notin \domof{\Gamma}$. When working with abstract binding trees, i.e. terms identified up to variable renaming, we typically need to give no thought to this condition because it is always possible to implicitly rename the term under consideration when shadowing occurs to discharge this requirement. However, we cannot implicitly rename unexpanded terms. Changing the definition of typing contexts would have significant implications throughout the metatheory of the core language, which we seek to avoid touching. 

Instead, we define \emph{unexpanded typing contexts}, $\uGamma$, as pairs of the form $\uGG{\uG}{\Gamma}$, where $\uG$ maps each expression identifier $\ux \in \domof{\uG}$ to a variable, $x$, written $\vExpands{\ux}{x}$. The typing context, $\Gamma$, only tracks the type of these variables. We define the identifier update operation $\ctxUpdate{\uG}{\ux}{x}$ as producing the expression identifier expansion context that maps $\ux$ to $x$, written $\vExpands{\ux}{x}$, and defers to $\uG$ for all other expression identifiers, with no requirement that $\ux$ be apart from $\domof{\uG}$. %Note the distinction between update and extension (which requires that the new identifier is not already in the domain.) %We write $\uGammaOK{\uGamma}$ when $\uGamma=\uGG{\uG}{\Gamma}$ and each expression variable in $\uG$ is assigned a type by $\Gamma$.
%\begin{definition} $\uGammaOK{\uGG{\uG}{\Gamma}}$ iff for each $\vExpands{\ux}{x} \in \uG$, we have $\Ghyp{x}{\tau} \in \Gamma$ for some $\tau$.\end{definition}
%\noindent 
We define $\uGamma, \uGhyp{\ux}{x}{\tau}$ when $\uGamma = \uGG{\uG}{\Gamma}$ as an abbreviation of $\uGG{\ctxUpdate{\uG}{\ux}{x}}{\Gamma, \Ghyp{x}{\tau}}$. Unexpanded type formation contexts, $\uDelta$, are analagous. 

To develop an intuition for how this formulation solves the problem, it is instructive to inspect the derivation in Fig.~\ref{fig:expansion-exmpl} of the expansion of the unexpanded expression $\lam{\ux}{\utau}{\lam{\ux}{\utau}{\ux}}$ assuming $\expandsTU{\uDelta}{\utau}{\tau}$. Notice that each time Rule \textsc{ee-lam} is applied, the type identifier expansion context is updated but the typing context is extended with a fresh variable, first $x_1$ then $x_2$, which is possible by alpha varying only the expansion, leaving the unexpanded term unchanged. % Without this mechanism, expansions for unexpanded terms with shadowing would not be derivable.\todo{add let notation to show how this is actually relevant?}

% \emph{Unexpanded type formation contexts}, $\uDelta$, consist of a \emph{type identifier expansion context}, $\uD$, paired with a standard type formation context, $\Delta$, and operate analagously (see supplement.)
% of the form $\uDD{\uD}{\Delta}$, i.e. they . We similarly define $\uDelta, \uDhyp{\ut}{t}$ when $\uDelta=\uDD{\uD}{\Delta}$ as an abbreviation of $\uDD{\ctxUpdate{\uD}{\ut}{t}}{\Delta, \Dhyp{t}}$.%type identifier expansion context is always extended/updated together with 


% % Before we continue, let us state an important invariant: that Typed expansion produces a well-typed expression. 
% \begin{theorem}[Typed Expression Expansion]\label{thm:typed-expansion-short-U} ~\\
% If $\expandsUP{\uDD{\uD}{\Delta}\hspace{-3px}}{\uGG{\uG}{\Gamma}\hspace{-3px}}{\uPsi}{\uPhi}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
% \end{theorem}
% For the typed expansion rules governing common forms, like the two example rules applied in Fig.~\ref{fig:expansion-exmpl}, it is easy to see that this invariant holds. The  rules of particular interest are the rules governing TLM definitions and TLM application, which are the topic of the remainder of this section. 
% \endgroup
% \subsubsection{Type Expansion}
% \emph{unexpanded type formation contexts}, $\udelta$, are of the form $\udd{\ud}{\delta}$, i.e. they consist of a \emph{type identifier expansion context}, $\ud$, paired with a standard type formation context, $\delta$. 

% A \emph{type identifier expansion context}, $\uD$, is a finite function that maps each type identifier $\ut \in \domof{\uD}$ to the hypothesis $\vExpands{\ut}{t}$, for some type variable $t$. We write $\ctxUpdate{\uD}{\ut}{t}$ for the type identifier expansion context that maps $\ut$ to $\vExpands{\ut}{t}$ and defers to $\uD$ for all other type identifiers (i.e. the previous mapping is \emph{updated}.) We define $\uDelta, \uDhyp{\ut}{t}$ when $\uDelta=\uDD{\uD}{\Delta}$ as an abbreviation of $\uDD{\ctxUpdate{\uD}{\ut}{t}}{\Delta, \Dhyp{t}}$.%type identifier expansion context is always extended/updated together with 

% The \emph{type expansion judgement}, $\expandsTU{\uDelta}{\utau}{\tau}$, is inductively defined by the rules given in the supplement. The first three of these rules are reproduced below:
% % \begin{subequations}%\label{rules:expandsTU}
% \begin{mathpar}
% \inferrule[te-id]{ }{\expandsTU{\uDelta, \uDhyp{\ut}{t}}{\ut}{t}}

% \inferrule[te-parr]{
%   \expandsTU{\uDelta}{\utau_1}{\tau_1}\\
%   \expandsTU{\uDelta}{\utau_2}{\tau_2}
% }{\expandsTU{\uDelta}{\parr{\utau_1}{\utau_2}}{\aparr{\tau_1}{\tau_2}}}

% \inferrule[te-all]{
%     \expandsTU{\uDelta, \uDhyp{\ut}{t}}{\utau}{\tau}
%   }{
%     \expandsTU{\uDelta}{\forallt{\ut}{\utau}}{\aall{t}{\tau}}
% }
% \end{mathpar}
% % \begin{equation*}\label{rule:expandsTU-rec}
% %   \inferrule{
% %     \expandsTU{\uDelta, \uDhyp{\ut}{t}}{\utau}{\tau}
% %   }{
% %     \expandsTU{\uDelta}{\aurec{\ut}{\utau}}{\arec{t}{\tau}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:expandsTU-prod}
% %   \inferrule{
% %     \{\expandsTU{\uDelta}{\utau_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \expandsTU{\uDelta}{\auprod{\labelset}{\mapschema{\utau}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:expandsTU-sum}
% %   \inferrule{
% %     \{\expandsTU{\uDelta}{\utau_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \expandsTU{\uDelta}{\ausum{\labelset}{\mapschema{\utau}{i}{\labelset}}}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}
% % \end{subequations}
% %We write $\uDeltaOK{\uDelta}$ when $\uDelta=\uDD{\uD}{\Delta}$ and each type variable in $\uD$ also appears in $\Delta$.
% %\begin{definition}\label{def:uDeltaOK} $\uDeltaOK{\uDD{\uD}{\Delta}}$ iff for each $\vExpands{\ut}{t} \in \uD$, we have $\Dhyp{t} \in \Delta$.\end{definition}



% The Type Expansion Lemma establishes that the expansion of an unexpanded type is a well-formed type.

% % \begingroup
% % \def\thetheorem{\ref{lemma:type-expansion-U}}
% \begin{lemma}[Type Expansion] If $\expandsTU{\uDD{\uD}{\Delta}}{\utau}{\tau}$ then $\istypeU{\Delta}{\tau}$.\end{lemma}
% % \begin{proof} By rule induction over Rules (\ref{rules:expandsTU}). In each case, we apply the IH to or over each premise, then apply the corresponding type formation rule in Rules (\ref{rules:istypeU}). \end{proof}
% % \endgroup
% \begin{subequations}\label{rules:expandsU}




% The rules for the remaining expressions of common form are entirely straightforward, mirroring the corresponding typing rules, i.e. Rules (\ref{rules:hastypeU}). %In particular, observe that, in each of these rules, the unexpanded and expanded expression forms in the conclusion correspond, and each premise corresponds to a premise of the corresponding typing rule. %Type formation premises in the typing rule give rise to  type expansion premises in the corresponding typed expansion rule, and each typed expression expansion premise in each rule above corresponds to a typing premise in the corresponding typing rule. 
% The type assigned in the conclusion of each rule above is identical to the type assigned in the conclusion of the corresponding typing rule. The seTLM context, $\uPsi$, passes opaquely through these rules (we will define seTLM contexts below.) As such, the corresponding cases in the proof of Theorem \ref{thm:typed-expansion-short-U} are by application of the induction hypothesis and the  corresponding typing rule. %Rules (\ref{rules:expandsTU}) could similarly have been generated by mechanically transforming Rules (\ref{rules:istypeU}).

% We can express this scheme more precisely with the rule transformation given in Appendix \ref{appendix:SES-uexps}. For each rule in Rules (\ref{rules:istypeU}) and Rules (\ref{rules:hastypeU}),
% \begin{mathpar}
% \refstepcounter{equation}
% % \label{rule:expandsU-tlam}
% % \refstepcounter{equation}
% % \label{rule:expandsU-tap}
% % \refstepcounter{equation}
% \label{rule:expandsU-fold}
% \refstepcounter{equation}
% \label{rule:expandsU-unfold}
% \refstepcounter{equation}
% \label{rule:expandsU-tpl}
% \refstepcounter{equation}
% \label{rule:expandsU-pr}
% \refstepcounter{equation}
% \label{rule:expandsU-in}
% \refstepcounter{equation}
% \label{rule:expandsU-case}
% \inferrule{J_1\\ \cdots \\ J_k}{J}
% \end{mathpar}
% the corresponding typed expansion rule is 
% \begin{mathpar}
% \inferrule{
%   \Uof{J_1} \\
%   \cdots\\
%   \Uof{J_k}
% }{
%   \Uof{J}
% }
% \end{mathpar}
% where
% \[\begin{split}
% \Uof{\istypeU{\Delta}{\tau}} & = \expandsTU{\Uof{\Delta}}{\Uof{\tau}}{\tau} \\
% \Uof{\hastypeU{\Gamma}{\Delta}{e}{\tau}} & = \expandsU{\Uof{\Gamma}}{\Uof{\Delta}}{\uPsi}{\Uof{e}}{e}{\tau}\\
% \Uof{\{J_i\}_{i \in \labelset}} & = \{\Uof{J_i}\}_{i \in \labelset}
% \end{split}\]
% and where:
% \begin{itemize}
% \item $\Uof{\tau}$ is defined as follows:
%   \begin{itemize}
%   \item When $\tau$ is of definite form, $\Uof{\tau}$ is defined as in Sec. \ref{sec:syntax-U}.
%   \item When $\tau$ is of indefinite form, $\Uof{\tau}$ is a uniquely corresponding metavariable of sort $\mathsf{UTyp}$ also of indefinite form. For example, in Rule (\ref{rule:istypeU-parr}), $\tau_1$ and $\tau_2$ are of indefinite form, i.e. they match arbitrary types. The rule transformation simply ``hats'' them, i.e. $\Uof{\tau_1}=\utau_1$ and $\Uof{\tau_2}=\utau_2$.
%   \end{itemize}
% \item $\Uof{e}$ is defined as follows
% \begin{itemize}
% \item When $e$ is of definite form, $\Uof{e}$ is defined as in Sec. \ref{sec:syntax-U}. 
% \item When $e$ is of indefinite form, $\Uof{e}$ is a uniquely corresponding metavariable of sort $\mathsf{UExp}$ also of indefinite form. For example, $\Uof{e_1}=\ue_1$ and $\Uof{e_2}=\ue_2$.
% \end{itemize}
% \item $\Uof{\Delta}$ is defined as follows:
%   \begin{itemize} 
%   \item When $\Delta$ is of definite form, $\Uof{\Delta}$ is defined as above.
%   \item When $\Delta$ is of indefinite form, $\Uof{\Delta}$ is a uniquely corresponding metavariable ranging over unexpanded type formation contexts. For example, $\Uof{\Delta} = \uDelta$.
%   \end{itemize}
% \item $\Uof{\Gamma}$ is defined as follows:
%   \begin{itemize}
%   \item When $\Gamma$ is of definite form, $\Uof{\Gamma}$ produces the corresponding unexpanded typing context as follows:
% \begin{align*}
% \Uof{\emptyset} & = \uGG{\emptyset}{\emptyset}\\
% \Uof{\Gamma, \Ghyp{x}{\tau}} & = \Uof{\Gamma}, \uGhyp{\identifierof{x}}{x}{\tau}
% \end{align*}
%   \item When $\Gamma$ is of indefinite form, $\Uof{\Gamma}$ is a uniquely corresponding metavariable ranging over unexpanded typing contexts. For example, $\Uof{\Gamma} = \uGamma$.
% \end{itemize}
% \end{itemize}

% It is instructive to use this rule transformation to generate Rules (\ref{rules:expandsTU}) and Rules (\ref{rule:expandsU-var}) through (\ref{rule:expandsU-tap}) above. We omit the remaining rules, i.e. Rules (\ref*{rule:expandsU-fold}) through (\ref*{rule:expandsU-case}). By instead defining these rules solely by the rule transformation just described, we avoid having to write down a number of rules that are of limited marginal interest. Moreover, this demonstrates the general technique for generating typed expansion rules for unexpanded types and expressions of common form, so our exposition is somewhat ``robust'' to changes to the inner core. 
%o that when the inner core changes,  typed expansion rules  our exposition somewhat robust to changes to the inner core (though not to changes to the judgement forms in the statics of the inner core).% Even if changes to the judgement forms in the statics of the inner core are needed (e.g. the addition of a symbol context), it is easy to see would correspond to changes in the generic specification above.
% \begin{subequations}\label{rules:expandsU}
% \begin{equation*}\label{rule:expandsU-var}
%   \inferrule{ }{\expandsU{\Delta}{\Gamma, x : \tau}{\uPsi}{x}{x}{\tau}}
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-lam}
%   \inferrule{
%     \istypeU{\Delta}{\tau}\\
%     \expandsU{\Delta}{\Gamma, x : \tau}{\uPsi}{\ue}{e}{\tau'}
%   }{\expandsUX{\aulam{\tau}{x}{\ue}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}}
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-ap}
%   \inferrule{
%     \expandsUX{\ue_1}{e_1}{\aparr{\tau}{\tau'}}\\
%     \expandsUX{\ue_2}{e_2}{\tau}
%   }{
%     \expandsUX{\auap{\ue_1}{\ue_2}}{\aeap{e_1}{e_2}}{\tau'}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tlam}
%   \inferrule{
%     \expandsU{\Delta, \Dhyp{t}}{\Gamma}{\uPsi}{\ue}{e}{\tau}
%   }{
%     \expandsUX{\autlam{t}{\ue}}{\aetlam{t}{e}}{\aall{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tap}
%   \inferrule{
%     \expandsUX{\ue}{e}{\aall{t}{\tau}}\\
%     \istypeU{\Delta}{\tau'}
%   }{
%     \expandsUX{\autap{\ue}{\tau'}}{\aetap{e}{\tau'}}{[\tau'/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-fold}
%   \inferrule{
%     \istypeU{\Delta, \Dhyp{t}}{\tau}\\
%     \expandsUX{\ue}{e}{[\arec{t}{\tau}/t]\tau}
%   }{
%     \expandsUX{\aufold{t}{\tau}{\ue}}{\aefold{e}}{\arec{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-unfold}
%   \inferrule{
%     \expandsUX{\ue}{e}{\arec{t}{\tau}}
%   }{
%     \expandsUX{\auunfold{\ue}}{\aeunfold{e}}{[\arec{t}{\tau}/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tpl}
%   \inferrule{
%     \{\expandsUX{\ue_i}{e_i}{\tau_i}\}_{i \in \labelset}
%   }{
%     \expandsUX{\autpl{\labelset}{\mapschema{\ue}{i}{\labelset}}}{\aetpl{\labelset}{\mapschema{e}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-pr}
%   \inferrule{
%     \expandsUX{\ue}{e}{\aprod{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}}
%   }{
%     \expandsUX{\aupr{\ell}{\ue}}{\aepr{\ell}{e}}{\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-in}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}\\
%     \istypeU{\Delta}{\tau}\\
%     \expandsUX{\ue}{e}{\tau}
%   }{
%     \left\{\shortstack{$\Delta~\Gamma \vdash_\uPsi \auin{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}{\ue}$\\$\leadsto$\\$\aein{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}{e} : \asum{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}$\vspace{-1.2em}}\right\}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-case}
%   \inferrule{
%     \expandsUX{\ue}{e}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}\\
%     \{\expandsU{\Delta}{\Gamma, \Ghyp{x_i}{\tau_i}}{\uPsi}{\ue_i}{e_i}{\tau}\}_{i \in \labelset}
%   }{
%     \expandsUX{\aucase{\labelset}{\ue}{\mapschemab{x}{\ue}{i}{\labelset}}}{\aecase{\labelset}{e}{\mapschemab{x}{e}{i}{\labelset}}}{\tau}
%   }
% \end{equation*}
% \end{subequations}


% \begin{equation*}\label{rule:expandsU-syntax}
% \inferrule{
%   \istypeU{\Delta}{\tau}\\
%   \expandsU{\emptyset}{\emptyset}{\emptyset}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultExp}}\\\\
%   a \notin \domof{\uPsi}\\
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\ue}{e}{\tau'}
% }{
%   \expandsUX{\audefuetsm{\tau}{\ueparse}{\tsmv}{\ue}}{e}{\tau'}
% }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tsmap}
% \inferrule{
%   \encodeBody{b}{\ebody}\\
%   \evalU{\ap{\eparse}{\ebody}}{\inj{\lbltxt{SuccessE}}{\ecand}}\\
%   \decodeCondE{\ecand}{\ce}\\\\
%   \cvalidE{\emptyset}{\emptyset}{\esceneU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{b}}{\ce}{e}{\tau}
% }{
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\autsmap{b}{\tsmv}}{e}{\tau}
% }
% \end{equation*}
%\end{subequations}

%Notice that each form of expanded expression (Figure \ref{fig:U-expanded-terms}) corresponds to a form of unexpanded expression (Figure \ref{fig:U-unexpanded-terms}). For each typing rule in Rules (\ref{rules:hastypeU}), there is a corresponding typed expansion rule -- Rules (\ref{rule:expandsU-var}) through (\ref{rule:expandsU-case}) -- where the unexpanded and expanded forms correspond. The premises also correspond -- if a typing judgement appears as a premise of a typing rule, then the corresponding premise in the corresponding typed expansion rule is the corresponding typed expansion judgement. The seTLM context is not extended or inspected by these rules (it is only ``threaded through'' them opaquely).

%There are two unexpanded expression forms that do not correspond to an expanded expression form: the seTLM definition form, and the seTLM application form. The rules governing these two forms interact with the seTLM context, and are the topics of the next two subsections, respectively.

\subsubsection{TLM Definitions}\label{sec:U-uetsm-definition}\label{sec:s-TLM-def}
% An unexpanded expression of seTLM definition form, $\uesyntaxq{\tsmv}{\utau}{\eparse}{\ue}$, 
%The operational form corresponding to this stylized form is \[\audefuetsm{\utau}{\eparse}{\tsmv}{\ue}\]
 % defines an {seTLM} identified as $\tsmv$ with \emph{unexpanded type annotation} $\utau$ and \emph{parse function} $\eparse$ for use within $\ue$. 
 There are four unexpanded forms that are not common forms -- the two TLM definition forms and the two TLM application forms. Let us start with TLM definitions. Rule \textsc{ee-def-setlm} in Fig.~\ref{fig:ee-def-setlm} governs simple expression TLM (seTLM) definitions. % Each expression TLM definition specifies a TLM identifier, $\tsmv$, an unexpanded expansion type, $\utau$, a parse function, $\eparse$ and an unexpanded dependency, $\uedep$. TLM definitions are scoped to the specified unexpanded expression, $\ue$. The premises of Rule~\textsc{ee-def-setlm} operate as follows.

 The first premise expands the unexpanded expansion type, $\utau$, producing the expansion type, $\tau$. 

 The second premise checks that the parse function, $\eparse$, is a closed expanded function with input type $\tBody$ and return type $\tParseResultExp$.\footnote{Think of $\eparse$ as the result of parser name resolution, which produces a ``compiled''---i.e. closed and expanded---term.}
 %An alternative design would be to index the typed expansion judgements by a mapping of parser names to these parse functions, but we chose this design to avoid passing this index through every judgement.}  
 The type abbreviated $\tBody$ classifies encodings of literal bodies, $b$. Rather than defining $\tBody$ explicitly it suffices to take as a condition that there is an isomorphism between literal bodies and values of type $\tBody$ mediated in one direction by a judgement $\encodeBody{b}{\ebody}$ that is used in the rule for TLM application discussed below. The return type, $\tParseResultExp$, abbreviates a labeled sum type, 
{$
% L_\mathtt{SE} & \defeq \lbltxt{ParseError}, \lbltxt{SuccessE}\\ \asumNL{
  \asum{~}{\mapitem{\lbltxt{Error}}{\prodt{}}, 
  \mapitem{\lbltxt{SuccessE}}{\tCEExp}}
$}, that allows the TLM's parser to distinguish parse errors from successful parses.\footnote{In \lifootnote{Relit}, we used an exception, \lifootnote{Relit.ExpansionError}, rather than a sum type for the same purpose.}


% \begin{subequations}[resume]
% \begin{equation*}\label{rule:expandsU-syntax}
% \inferrule{
%   \istypeU{\Delta}{\tau}\\
%   \expandsU{\emptyset}{\emptyset}{\emptyset}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultExp}}\\\\
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\ue}{e}{\tau'}
% }{
%   \expandsUX{\audefuetsm{\tau}{\ueparse}{\tsmv}{\ue}}{e}{\tau'}
% }
% \end{equation*}
\begin{figure*}
{\small\begin{mathpar}
\inferrule[ee-def-setlm]{
  \expandsTU{\uDelta}{\utau}{\tau}\\\\
  \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultExp}}\\
  \expandsUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{\uedep}{\edep}{\taudep}\\\\
  \uGamma = \uGG{\uG}{\Gamma}\\
  \expandsUP{\uDelta}{\uGG{\uG}{\Gamma, x : \taudep}}{\uPsi, \uShyp{\tsmv}{x}{\tau}{\eparse}}{\uPhi}{\ue}{e}{\tau'}\\\\
  e_\text{defn} = \aeap{\aelam{\taudep}{x}{e}}{\edep}
}{
  \expandsUPX{\uesyntaxqr{\tsmv}{\utau}{\eparse}{\ue}}{e_\text{defn}}{\tau'}
}
\end{mathpar}}
{\small\begin{mathpar}
\inferrule[ee-def-sptlm]{
  \expandsTU{\uDelta}{\utau}{\tau}\\
  \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultPat}}\\\\
  \expandsUP{\uDelta}{\uGamma}{\uPsi}{\uPhi, \uPhyp{\tsmv}{\_}{\tau}{\eparse}}{\ue}{e}{\tau'}
}{
  \expandsUPX{\usyntaxup{\tsmv}{\utau}{\eparse}{\ue}}{e}{\tau'}
}
\end{mathpar}}
\caption{The typed expansion rules for expression and pattern TLM definitions.}
\label{fig:ee-def-setlm}
\vspace{-5px}
\end{figure*}
% \end{subequations}





% \end{align*}} %[\mapitem{\lbltxt{ParseError}}{\prodt{}}, \mapitem{\lbltxt{SuccessE}}{\tCEExp}]
% \] 

The type abbreviated $\tCEExp$ classifies encodings of \emph{proto-expressions}, $\ce$ (pronounced ``grave $e$''). The syntax of proto-expressions, defined in Fig.~\ref{fig:U-candidate-terms}, will be described when we describe proto-expansion validation in Sec. \ref{sec:ce-syntax-U}. 
As with $\tBody$, we need only take as a condition that there is an isomorphism between values of type $\tCEExp$ and closed proto-expressions, which is mediated in one direction by the \emph{proto-expression decoding judgement}, $\decodeCondE{e}{\ce}$ that we will return to when we describe TLM application below (see supplement for the full condition).

In $\miniVersePat$, we model the \lismall{dependencies} clause as specifying a single value dependency.
%\footnote{In \lifootnote{Relit}, this is further restricted to a module expression with a singleton signature, for reasons discussed in Sec.~\ref{sec:implementation}.} 
The third premise of Rule~\textsc{ee-def-setlm} generates the expanded dependency, $\edep$ of type $\taudep$, from the unexpanded dependency, $\uedep$. Note that we do not need to explicitly allow for type dependencies (which would be formally cumbersome because we cannot ``tuple together'' types like we can values in this setting). Instead, type dependencies can be expressed by dependency on a value of existential type to be unpacked by the expansion. This existential type can in turn be expressed in terms of universal types by the well-known encoding \cite{B304,pfple1}. This workaround is perhaps unsurprising given that existentials relate closely to modules, which package both types and values \cite{pfple1,mitchell1988abstract}. 

Having processed the TLM definition, we are ready to continue into the unexpanded expression $\ue$ where the TLM is bound. To activate the TLM definition for use by $\ue$, the third row of premises in Rule~\textsc{ee-def-setlm} first generates a fresh variable, \li{x}, to stand for the value dependency. We will use this variable to instantiate the dependency in each generated expansion when we discuss TLM application below. Note that there is no corresponding expression identifier in $\uG$. Second, the rule extends the expression TLM context, $\uPsi$, to associate the TLM identifier $\tsmv$ with $x$, as well as with the given expansion type and parse function. If $\tsmv$ was already defined, the previous definition is shadowed (the supplement gives some additional details on how TLM contexts are structured).

The final premise of Rule~\textsc{ee-def-setlm}, together with the conclusion of the rule, specifies the expansion of the TLM definition as being of function application form---it wraps $e$, where the variable $x$ stands free for the dependency, with a lambda binding $x$, and then immediately applies it to pass down the actual value dependency, $\edep$. In other words, it \li{let}-binds the dependency. This defers to the core language with regard to whether $\edep$ is evaluated eagerly or lazily.

% The final premise of Rule \textsc{ee-def-setlm} extends the expression TLM context, $\uPsi$, with the newly determined {seTLM definition}, and proceeds to assign a type, $\tau'$, and expansion, $e$, to $\ue$. The conclusion of the rule then assigns this type and expansion to the seTLM definition as a whole. % i.e. TLMs define behavior that is relevant during typed expansion, but not during evaluation. 

% {Expression TLM contexts}, $\uPsi$, are of the form $\uAS{\uA}{\Psi}$, where $\uA$ is a \emph{TLM identifier expansion context} and $\Psi$ is an \emph{expression TLM definition context}. We distinguish TLM identifiers, $\tsmv$, from TLM names, $a$, for much the same reason that we distinguish type and expression identifiers from type and expression variables: in order to allow a TLM definition to shadow a previously defined TLM definition without relying on an implicit identification convention.

% A {TLM identifier expansion context}, $\uA$, maps each TLM identifier $\tsmv \in \domof{\uA}$ to the \emph{TLM identifier expansion}, $\vExpands{\tsmv}{a}$, for some \emph{TLM name}, $a$. 



% An {expression TLM definition context}, $\Psi$, is a finite function mapping each TLM name $a \in \domof{\Psi}$ to an \emph{expanded seTLM definition}, $\xuetsmbnd{a}{\tau}{\eparse}$, where $\tau$ is the seTLM's type annotation, and $\eparse$ is its parse function. 
% We define $\uPsi, \uShyp{\tsmv}{a}{\tau}{\eparse}$, when $\uPsi=\uAS{\uA}{\Psi}$, as an abbreviation of $\uAS{\ctxUpdate{\uA}{\tsmv}{a}}{\Psi, \xuetsmbnd{a}{\tau}{\eparse}}$.

%Moreover, this distinction will be crucial in the semantics of TLM abbreviations in Chapter \ref{chap:ptsms}. 

% \end{enumerate}

Rule \textsc{ee-ap-sptlm} for pattern TLM definitions shown in Fig. \ref{fig:ee-def-setlm} is analagous but simpler, because in $\miniVersePat$ patterns are entirely structural (there are no module paths that they might depend on). %As such, we do not need the machinery for dependency passing.
% {\vspace{-3px}\small\begin{mathpar}
% \inferrule[ee-def-sptsm]{
%   \expandsTU{\uDelta}{\utau}{\tau}\\
%   \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultPat}}\\\\
%   \evalU{\eparse}{\eparse'}\\
%   \expandsUP{\uDelta}{\uGamma}{\uPsi}{\uPhi, \uPhyp{\tsmv}{a}{\tau}{\eparse'}}{\ue}{e}{\tau'}
% }{
%   \expandsUPX{\usyntaxup{\tsmv}{\utau}{\eparse}{\ue}}{e}{\tau'}
% }
% \end{mathpar}}
% \vspace{-3px}

% \[\begin{array}{ll}
% \textbf{Judgement Form} & \textbf{Description}\\
% \uetsmenv{\Delta}{\uPsi} & \text{$\uPsi$ is well-formed assuming $\Delta$}\end{array}\]
% This judgement is inductively defined by the following rules:
% \begin{subequations}[intermezzo]\label{rules:uetsmenv-U}
% \begin{equation*}\label{rule:uetsmenv-empty}
% \inferrule{ }{\uetsmenv{\Delta}{\emptyset}}
% \end{equation*}
% \begin{equation*}\label{rule:uetsmenv-ext}
% \inferrule{
%   \uetsmenv{\Delta}{\uPsi}\\
%   \istypeU{\Delta}{\tau}\\
%   \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultExp}}
% }{
%   \uetsmenv{\Delta}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}
% }
% \end{equation*}
% \end{subequations}

\subsubsection{TLM Application}\label{sec:U-uetsm-application}\label{sec:s-TLM-ap}
The unexpanded expression form for applying an seTLM identified as $\tsmv$ to a literal form with literal body $b$ is $\utsmap{\tsmv}{b}$. 
% This form uses forward slashes\todo{use backticks} to delimit the literal body, but other generalized literal forms could also be included as derived forms in the textual syntax. % (we omit them for simplicity).
%The corresponding operational form is $\autsmap{b}{\tsmv}$. %i.e. for each literal body $b$, the operator $\texttt{uapuetsm}[b]$ is indexed by the TLM name $\tsmv$ and takes no arguments. %\footnote{This is in following the conventions in \emph{PFPL} \cite{pfpl}, where operators parameters allow for the use of metatheoretic objects that are not syntax trees or binding trees, e.g. $\mathsf{str}[s]$ and $\mathsf{num}[n]$.} This operator is indexed by the TLM name $\tsmv$ and takes no arguments. 
Rule \textsc{ee-ap-setlm} governing this form is in Fig.~\ref{fig:ee-ap-setlm}. 

% \begin{subequations}[resume]
% \begin{equation*}\label{rule:expandsU-tsmap}
% \inferrule{
%   \encodeBody{b}{\ebody}\\
%   \evalU{\ap{\eparse}{\ebody}}{\inj{\lbltxt{SuccessE}}{\ecand}}\\
%   \decodeCondE{\ecand}{\ce}\\\\
%   \cvalidE{\emptyset}{\emptyset}{\esceneU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{b}}{\ce}{e}{\tau}
% }{
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\autsmap{b}{\tsmv}}{e}{\tau}
% }
% \end{equation*}
% {\small\begin{mathpar}
% \inferrule[ee-ap-setlm]{
%   \encodeBody{b}{\ebody}\\
%   \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{\ecand}}\\
%   \decodeCondE{\ecand}{\ce}\\\\
%   \segOK{\segof{\ce}}{b}\\
%   \cvalidE{\emptyset}{\emptyset}{\esceneUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{b}}{\ce}{e}{\tau}
% }{
%   \expandsUP{\uDelta}{\uGamma}{\uPsi', \uShyp{\tsmv}{a}{\tau}{\eparse}}{\uPhi}{\utsmap{\tsmv}{b}}{e}{\tau}
% }
% \end{mathpar}}
The first two premises serve simply to ``look up'' $\tsmv$ in the expression TLM context, $\uPsi$, and to look up the correspondency dependency variable, $x$, in $\uGamma$.

The third premise encodes the literal body, $\ebody$, producing a value $\ebody$ of type $\tBody$ according to the body encoding judgement $\encodeBody{b}{\ebody}$ described in Sec.~\ref{sec:s-TLM-def} above.

The fourth premise applies the parse function $\eparse$ to the encoding of the literal body. If parsing succeeds, i.e. a value of the form $\aein{\mathtt{SuccessE}}{\ecand}$ results from evaluation, then $\ecand$ will be a value of type $\tCEExp$ (by type safety and the canonical forms lemma, assuming a well-formed expression TLM context). We call $\ecand$ the \emph{encoding of the proto-expansion}. If the parse function produces a value labeled $\lbltxt{Error}$, then typed expansion fails and formally, no rule is necessary.



\begin{figure*}
{\small\begin{mathpar}
\inferrule[ee-ap-setlm]{
  \uPsi = \uPsi', \uShyp{\tsmv}{x}{\tau}{\eparse}\\
  \uGamma = \uGG{\uG}{\Gamma, x : \taudep}\\\\
  \encodeBody{b}{\ebody}\\
  \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{\ecand}}\\
  \decodeCondE{\ecand}{\ce}\\\\
  \segOK{\segof{\ce}}{b}\\
  \cvalidE{\emptyset}{\emptyset}{\esceneUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{b}}{\ce}{e}{\aparr{\taudep}{\tau}}
}{
  \expandsUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{\utsmap{\tsmv}{b}}{\aeap{e}{x}}{\tau}
}
\end{mathpar}}
{\small\begin{mathpar}
\inferrule[pe-ap-sptlm]{
  \uPhi = \uPhi', \uPhyp{\tsmv}{\_}{\tau}{\eparse}\\\\
  \encodeBody{b}{\ebody}\\
  \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessP}}{\ecand}}\\
  \decodeCEPat{\ecand}{\cpv}\\\\
    \segOK{\segof{\cpv}}{b}\\
  \cvalidP{\upctx}{\pscene{\uDelta}{\uPhi}{b}}{\cpv}{p}{\tau}
}{
  \patExpands{\upctx}{\uPhi}{\utsmap{\tsmv}{b}}{p}{\tau}
}\end{mathpar}}
\caption{The typed expansion rules for expression and pattern TLM application.}
\label{fig:ee-ap-setlm}
\end{figure*}

The fifth premise decodes the encoding of the proto-expansion using the judgement described in Sec. \ref{sec:U-uetsm-definition}, producing the proto-expansion itself, $\ce$. 

The final two premises validate the proto-expansion. Proto-expansion validation is described in Sec.~\ref{sec:s-PEV} below. In $\miniVersePat$, the proto-expansion does not expand directly to the final expansion but to a function, here $e$, from the dependency type, $\taudep$, to the expansion type, $\tau$, as suggested by the right-hand side of the final premise. In the conclusion of the rule, we apply the dependency variable, $x$, to produce the final expansion.\footnote{In \lifootnote{Relit}, the dependency is always on the singleton module induced by the \lifootnote{dependencies} clause, which is \lifootnote{open}ed immediately, so the necessary boilerplate is inserted automatically.% If this were changed to require that the proto-expansion be a curried functor over dependencies listed in lexical order, the upside would be that the module identifiers used internally to a proto-expansion would not be exposed in the TLM definition, but the downside is that this would require the signature of each dependency be reproduced in each proto-expansion. We chose the more concise approach.
}

The typed pattern expansion rule governing pattern TLM application, Rule~\textsc{pe-ap-sptlm} in Fig.~\ref{fig:ee-ap-setlm}, is analagous but again simpler because there is no need to pass dependencies into patterns in $\miniVersePat$.\footnote{If there were a need to pass dependencies into patterns, we would need either pattern functions or to use an encoding trick, e.g. by requiring the result be a function containing a ``dummy'' match with one pattern.} We describe proto-pattern validation in Sec.~\ref{sec:s-PEV} below.
% {\small\begin{mathpar}
% \inferrule[pe-ap-sptsm]{
%   \encodeBody{b}{\ebody}\\
%   \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessP}}{\ecand}}\\
%   \decodeCEPat{\ecand}{\cpv}\\\\
%   \segOK{\segof{\cpv}}{b}\\
%   \cvalidP{\upctx}{\pscene{\uDelta}{\uPhi}{b}}{\cpv}{p}{\tau}
% }{
%   \patExpands{\upctx}{\uPhi', \uPhyp{\tsmv}{a}{\tau}{\eparse}}{\utsmap{\tsmv}{b}}{p}{\tau}
% }
% \end{mathpar}}

% \subsection{Syntax of Proto-Expansions}\label{sec:ce-syntax-U}

\subsection{Proto-Expansion Validation}\label{sec:ce-validation-U}\label{sec:ce-syntax-U}\label{sec:s-PEV}
Proto-expansion validation occurs in two steps, corresponding to the final two premises of the TLM application rules in Fig.~\ref{fig:ee-ap-setlm}. 

The first of these two premises determines the segmentation of the proto-expansion by pulling out the splice references and ensures that it is valid via the predicate $\segOK{\psi}{b}$, where $\psi$ is the segmentation. This checks that each segment  has positive length and is within bounds of $b$, and that the segments do not overlap and are separated by at least one character (see supplement).

The second of these two premises {typechecks} the proto-expression or proto-pattern, and simultaneously generates a corresponding core language expression or pattern, from which the final expansion is constructed as described above. This step is specified by the following judgements:% are types and expanded expressions, respectively.
\[\begin{array}{ll}
\cvalidT{\Delta}{\tscenev}{\ctau}{\tau} & \text{$\ctau$ has well-formed expansion $\tau$}\\
\cvalidE{\Delta}{\Gamma}{\escenev}{\ce}{e}{\tau} & \text{$\ce$ has expansion $e$ of type $\tau$}\\
% \cvalidR{\Delta}{\Gamma}{\escenev}{\crv}{r}{\tau}{\tau'} & \text{$\crv$ has expansion $r$ taking values of type $\tau$ to values of type $\tau'$}\\
\cvalidP{\upctx}{\pscenev}{\cpv}{p}{\tau} & \text{$\cpv$ has expansion $p$ matching $\tau$}
\end{array}\]



The purpose of the \emph{splicing scenes} $\tscenev$, $\escenev$ and $\pscenev$ is to ``remember'' the contexts and literal body from the TLM application site (cf. the final premise of Rule \textsc{ee-ap-setlm} in Fig.~\ref{fig:ee-def-setlm}) for when validation encounters spliced terms. For example, \emph{expression splicing scenes}, $\escenev$, are of the form $\esceneUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{b}$.

\paragraph{Common Forms} Most of the proto-expansion forms, including all of those elided in Fig.~\ref{fig:U-candidate-terms} mirror corresponding expanded forms. The rules governing proto-expansion validation for these common forms  in the supplement correspondingly mirror the typing rules. Splicing scenes---$\escenev$, $\tscenev$ and $\pscenev$--pass opaquely through these rules, i.e. none of these rules can access the application site contexts. Notice that the initial expansion-interal typing contexts, $\Delta$ and $\Gamma$, start out empty in the rules in Fig.~\ref{fig:ee-ap-setlm}. Together, this maintains context independence (defined formally below).

Notice that proto-rules, $\crv$, involve expanded patterns, $p$, not proto-patterns, $\cpv$, because proto-rules appear in proto-expressions, which are generated by expression TLMs. Proto-patterns arise only from pattern TLMs. There is no variable proto-pattern form, for the reasons described in Sec. \ref{sec:sptsms}.

\paragraph{Splice References} 
\begin{figure}
\begin{minipage}{\textwidth}
\small
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{PrTyp} & \ctau & ::= & t ~\vert~
\aceparr{\ctau}{\ctau} ~\vert~ \cdots ~\vert~
% \aceall{t}{\ctau} ~\vert~
% \acerec{t}{\ctau} ~\vert~
% \aceprod{\labelset}{\mapschema{\ctau}{i}{\labelset}} \\
% & & \vert & 
% \acesum{\labelset}{\mapschema{\ctau}{i}{\labelset}} ~\vert~ 
\acesplicedt{m}{n}\\
\mathsf{PrExp} & \ce & ::= & x ~\vert~
% \aceasc{\ctau}{\ce} ~\vert~
% \aceletsyn{x}{\ce}{\ce} ~\vert~
\acelam{\ctau}{x}{\ce} ~\vert~
\aceap{\ce}{\ce} ~\vert~
\cdots ~\vert~ 
% \acetlam{t}{\ce} ~\vert~
% \acetap{\ce}{\ctau} ~\vert~
% \acefold{\ce} \\
% & & \vert & \acetpl{\labelset}{\mapschema{\ce}{i}{\labelset}} ~\vert~
% \acein{\ell}{\ce} ~\vert~
\acematchwith{n}{\ce}{\seqschemaX{\crv}} ~\vert~
\acesplicede{m}{n}{\ctau}\\
\mathsf{PrRule} & \crv & ::= & \acematchrule{p}{\ce}\\
\mathsf{PrPat} & \cpv & ::= & \acewildp ~\vert~
\cdots ~\vert~
% \acefoldp{p} ~\vert~
% \acetplp{\labelset}{\mapschema{\cpv}{i}{\labelset}} ~\vert~
% \aceinjp{\ell}{\cpv} ~\vert~
\acesplicedp{m}{n}{\ctau} 
\end{array}$
\end{minipage}
\caption[Syntax of proto-types and proto-expressions]{Syntax of proto-expansions. Proto-expansion terms are ABTs identified up to alpha-equivalence.}
\label{fig:U-candidate-terms}
\end{figure}
The only interesting forms in Fig.~\ref{fig:U-candidate-terms} are the references to spliced types, expressions and patterns. Let us consider the rule for references to spliced expressions:

{\small\begin{mathpar}
\inferrule[pev-spliced]{
  \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\
  \cvalidT{\emptyset}{\tsceneUP{\uDD{\uD}{\Delta_\text{app}}}{b}}{\ctau}{\tau}\\
  \expandsUP{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{\uPhi}{\ue}{e}{\tau}\\\\
  \Delta \cap \Delta_\text{app} = \emptyset\\
  \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset
}{
  \cvalidE{\Delta}{\Gamma}{\esceneUP{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{\uPhi}{b}}{\acesplicede{m}{n}{\ctau}}{e}{\tau}
}
\end{mathpar}}

% {\small\begin{mathpar}
%   \inferrule[ptv-spliced]{
%     \parseUTyp{\bsubseq{b}{m}{n}}{\utau}\\
%     \expandsTU{\uDD{\uD}{\Delta_\text{app}}}{\utau}{\tau}\\
%     \Delta \cap \Delta_\text{app} = \emptyset
%   }{
%     \cvalidT{\Delta}{\tsceneU{\uDD{\uD}{\Delta_\text{app}}}{b}}{\acesplicedt{m}{n}}{\tau}
%   }
% \end{mathpar}}

\noindent
This first premise of this rule parses out the requested segment of the literal body, $b$, to produce an unexpanded expression, $\ue$. The second premise performs proto-type expansion on the given type annotation, $\ctau$, producing a type, $\tau$. The expansion-internal type variables in $\Delta$ are not available to $\tau$. The third premise then invokes type expansion on $\ue$ \emph{under the application site contexts}, $\uDD{\uD}{\Delta_\text{app}}$ and $\uGG{\uG}{\Gamma_\text{app}}$, but \emph{not} the expansion-internal contexts, $\Delta$ and $\Gamma$.  The final premise requires that the application site contexts are disjoint from the expansion-local type formation context. Because proto-expansions are ABTs identified up to alpha-equivalence, we can always discharge the final premise by alpha-varying the proto-expansion. This serves to enforce capture avoidance. Note that the purpose of $\miniVersePat$ is to specify the necessary conditions, not to specify a particular implementation strategy. There are various ways to implement this capture avoidance condition. We will formally state the capture avoidance property in terms of capture avoiding substitution below, which is one strategy. Another is to pro-actively generate fresh variables for all internal bindings \emph{a priori}.

The rule for references to spliced unexpanded types and patterns are analagous (see supplement). Note that we did not give examples of type splicing in the previous sections, but it is occasionally useful. For example, a TLM that implements a parser generator in the style of Menhir could need type splicing for reading in the non-terminal types, e.g. as on Line 9 of Fig.~\ref{fig:regex-parser}.
% \noindent
% We first splice out the requested segment. The second premise expands the type annotation under an empty context, because the type annotation must be meaningful at the application site (so, independent of $\Delta$ and $\Gamma$) and not itself make any assumptions about the application site context. Spliced types can appear in the annotation. The third premise performs typed expansion of the spliced unexpanded expression under the application site contexts, but not the expansion-local contexts. The final two premises ensure that these contexts are disjoint, again to force capture avoidance.



% The rule for references to spliced unexpanded patterns is entirely analagous (see supplement).
% {\small\begin{mathpar}
% \inferrule[ppv-spliced]{
%   \parseUPat{\bsubseq{b}{m}{n}}{\upv}\\
%   \cvalidT{\emptyset}{\tsceneUP{\uDelta}{b}}{\ctau}{\tau}\\
%   \patExpands{\upctx}{\uPhi}{\upv}{p}{\tau}
% }{
%   \cvalidP{\upctx}{\pscene{\uDelta}{\uPhi}{b}}{\acesplicedp{m}{n}{\ctau}}{p}{\tau}
% }
% \end{mathpar}}


\subsection{Metatheory}\label{sec:s-metatheory}
Let us now sketch the main metatheoretic properties of $\miniVersePat$.

\subsubsection{Typed Expansion} The first property that we are interested in is simple: that typed expansion produces a well-typed expansion. As it turns out, in order to prove this theorem, we must  prove the following stronger theorem, because the proto-expression validation judgement is defined mutually inductively with the typed expansion judgement (due to splicing).

% \begingroup
% \def\thetheorem{\ref{thm:typed-expansion-full-U}}
\begin{theorem}[Typed Expression Expansion (Strong)] ~
\begin{enumerate}[nolistsep]
\item If $\expandsUP{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uPsi}{\uPhi}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
\item If $\cvalidE{\Delta}{\Gamma}{\esceneUP{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{\uPhi}{b}}{\ce}{e}{\tau}$ and $\Delta \cap \Delta_\text{app} = \emptyset$ and $\domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset$ then $\hastypeU{(\Dcons{\Delta}{\Delta_\text{app}})}{(\Gcons{\Gamma}{\Gamma_\text{app}})}{e}{\tau}$.
\end{enumerate}
\end{theorem}
\vspace{-4px}

% \endgroup
The additional second clause simply states that the final expansion produced by proto-expression validation is well-typed under the combined application site and expansion-internal context (because spliced terms are distinguished only in the proto-expansion, but not in the final expansion). % The combined context can only be formed if these are disjoint.
The proof proceeds by mutual rule induction and appeal to the analagous typed pattern expansion theorem and simple lemmas about type expansion   and proto-type validation. The only issue is that it is not immediately clear that the mutual induction is well-founded, because the case in the proof of part 2 for Rule \textsc{pev-spliced} invokes part 1 of the induction hypothesis on a term that is not a sub-term of the conclusion, but rather parsed out of the literal body, $b$. To establish that the mutual induction is well-founded, then, we need to explicitly establish a decreasing metric. The intuition is that parsing a term of out a literal body cannot produce a bigger term than the term that contained that very literal body. 
% More specifically, {the sum of the lengths of the literal bodies that appear in the term strictly decreases each time you perform a nested TLM application} because some portion of the term has to be consumed by the TLM name and the delimiters. 
The details are given in the supplement.

%  A similar argument is needed to prove Typed Pattern Expansion:
% \begin{theorem}[Typed Pattern Expansion (Strong)] ~
% \begin{enumerate}[nolistsep]
%   \item If $\pExpandsSP{\uDD{\uD}{\Delta}}{\uAS{\uA}{\Phi}}{\upv}{p}{\tau}{\uGG{\uG}{\pctx}}$ then $\patType{\pctx}{p}{\tau}$.
%   \item If $\cvalidP{\uGG{\uG}{\pctx}}{\pscene{\uDD{\uD}{\Delta}}{\uAP{\uA}{\Phi}}{b}}{\cpv}{p}{\tau}$ then $\patType{\pctx}{p}{\tau}$.
% \end{enumerate}
% \end{theorem}


\subsubsection{TLM Reasoning Principles} 

The following theorem summarizes the abstract reasoning principles that client programmers can rely on when applying a simple expression TLM. Informal descriptions of the labeled clauses are given inline, in gray boxes.
% \begingroup
% \def\thetheorem{\ref{thm:tsc-SES}}
\noindent
\begin{theorem}[seTLM Reasoning Principles]\label{thm:setlm-reasoning} If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uPsi}{\utsmap{\tsmv}{b}}{e}{\tau}$ then
\begin{enumerate}[nolistsep,leftmargin=10pt,label={\arabic*.}]
\item (\textbf{Expansion Typing}) $\uPsi = \uPsi', \uShyp{\tsmv}{\_}{\tau}{\eparse}$ and $\hastypeU{\Delta}{\Gamma}{e}{\tau}$ 
  \begin{grayparbox}
     The type of the expansion is consistent with the type annotation on the applied seTLM definition.
  \end{grayparbox}
\item (\textbf{Responsibility}) $\encodeBody{b}{\ebody}$ and $\evalU{\ap{\eparse}{\ebody}}{\aein{\lbltxt{SuccessE}}{\ecand}}$ and $\decodeCondE{\ecand}{\ce}$
  \begin{grayparbox}
  The parse function of the applied TLM is responsible for generating the proto-expansion.
  \end{grayparbox}
\item (\textbf{Segmentation}) $\segOK{\segof{\ce}}{b}$ 
          \begin{grayparbox}
        The segmentation determined by the proto-expansion segments the literal body.
          \end{grayparbox}
\item (\textbf{Segment Typing})  $\segof{\ce} = \sseq{\acesplicedt{m'_i}{n'_i}}{\nty} \cup \sseq{\acesplicede{m_i}{n_i}{\ctau_i}}{\nexp}$ and
  \begin{enumerate}
  \item $\sseq{
        \expandsTU{\uDD{\uD}{\Delta}}
        {
          \parseUTypF{\bsubseq{b}{m'_i}{n'_i}}
        }{\tau'_i}
      }{\nty}$ and $\sseq{\istypeU{\Delta}{\tau'_i}}{\nty}$
          \begin{grayparbox}

          Each spliced type has a well-formed expansion.
          \end{grayparbox}
  \item $\sseq{
    \cvalidT{\emptyset}{
      \tsceneUP
        {\uDD
          {\uD}{\Delta}
        }{b}
    }{
      \ctau_i
    }{\tau_i}
  }{\nexp}$ and $\sseq{\istypeU{\Delta}{\tau_i}}{\nexp}$
        \begin{grayparbox}
        Each segment type has a well-formed expansion.
        \end{grayparbox}
  \item $\sseq{
    \expandsU
      {\uDD{\uD}{\Delta}}
      {\uGG{\uG}{\Gamma}}
      {\uPsi}
      {\parseUExpF{\bsubseq{b}{m_i}{n_i}}}
      {e_i}
      {\tau_i}
  }{\nexp}$ and $\sseq{\hastypeU{\Delta}{\Gamma}{e_i}{\tau_i}}{\nexp}$\vspace{-5px}
        \begin{grayparbox}
        Each spliced expression has a well-typed expansion consistent with the segment type.
        \end{grayparbox}
\end{enumerate}
\item (\textbf{Capture Avoidance}) $e = [\sseq{\tau'_i/t_i}{\nty}, \sseq{e_i/x_i}{\nexp}]e'$ for fresh variables $\sseq{x_i}{\nexp}$ and $\sseq{t_i}{\nty}$ and some $e'$
    \begin{grayparbox}
      We can decompose the final expansion, $e$, into an ``internal expression'', $e'$, with fresh variables in place of each spliced type or expression. The expansion can be produced by substituting in the expansions of these spliced types and expressions in the standard capture avoiding manner.
    \end{grayparbox}
\item (\textbf{Context Independence}) $\mathsf{fv}(e') \subset \sseq{t_i}{\nty} \cup \sseq{x_i}{\nexp}$
      \begin{grayparbox}
      The internal expression makes no mention of bindings in the application site context, i.e. the only free variables remaining are those standing for spliced terms.
      \end{grayparbox}
  % $\hastypeU
  % {\sseq{\Dhyp{t_i}}{\nty}}
  % {\sseq{x_i : \tau_i}{\nexp}}
  % {e'}{\tau}$
\end{enumerate}
\end{theorem}


% The proof, which involves auxiliary lemmas about the decomposition of proto-types and proto-expressions, is given in the supplement.

Notice that we were able to state the hygiene properties (\textbf{Capture Avoidance} and \textbf{Context Independence}) without needing a notion of alpha-equivalence of source terms, as in typical formal accounts of hygiene \cite{Kohlbecker86a,DBLP:conf/popl/Adams15,DBLP:conf/popl/ClingerR91,DBLP:journals/lisp/DybvigHB92,DBLP:conf/esop/HermanW08,Herman10:Theory}. Instead, we used standard notions of capture avoiding substitution and free variables combined with the context disjointness conditions in the rules above. This is possible only because we keep track of spliced terms explicitly in the proto-expansion. %In fact, doing so is critical for TLMs -- there is no notion of alpha-conversion for partially parsed terms, so any notion of hygiene that relies on this notion would be inapplicable.

The reasoning principles theorem for pattern TLMs is below. The \textbf{Visibility} clause establishes that the hypotheses generated by a pattern of TLM application form are exactly the union of the hypothesis generated by the spliced patterns---there are no invisible bindings (see Sec.~\ref{sec:sptsms}). % No additional hypotheses can be produced by the TLM.

\begin{theorem}[spTLM Abstract Reasoning Principles]
\label{thm:spTLM-Typing-Segmentation}
If $\patExpands{\upctx}{\uPhi}{\utsmap{\tsmv}{b}}{p}{\tau}$ where $\uDelta=\uDD{\uD}{\Delta}$ and $\uGamma=\uGG{\uG}{\Gamma}$ then all of the following hold:
\begin{enumerate}[nolistsep,leftmargin=10pt,label={\arabic*.}]
        \item (\textbf{Expansion Typing}) $\uPhi=\uPhi', \uPhyp{\tsmv}{\_}{\tau}{\eparse}$ and $\patType{\pctx}{p}{\tau}$
        \item (\textbf{Responsibility}) $\encodeBody{b}{\ebody}$ and $\evalU{\eparse(\ebody)}{\aein{\mathtt{SuccessP}}{\ecand}}$ and $\decodeCEPat{\ecand}{\cpv}$
        \item (\textbf{Segmentation}) $\segOK{\segof{\cpv}}{b}$
        \item (\textbf{Segment Typing}) $\segof{\cpv} = \sseq{\acesplicedt{n'_i}{m'_i}}{\nty} \cup \sseq{\acesplicedp{m_i}{n_i}{\ctau_i}}{\npat}$ and
          \begin{enumerate}
          \item $\sseq{
                \expandsTU{\uDelta}
                {
                  \parseUTypF{\bsubseq{b}{m'_i}{n'_i}}
                }{\tau'_i}
              }{\nty}$ and $\sseq{\istypeU{\Delta}{\tau'_i}}{\nty}$
          \item $\sseq{
            \cvalidT{\emptyset}{
              \tsceneUP
                {\uDelta}{b}
            }{
              \ctau_i
            }{\tau_i}
          }{\npat}$ and $\sseq{\istypeU{\Delta}{\tau_i}}{\npat}$
          \item $\sseq{
            \patExpands
              {\uGG{\uG_i}{\pctx_i}}
              {\uPhi}
              {\parseUPatF{\bsubseq{b}{m_i}{n_i}}}
              {p_i}
              {\tau_i}
          }{\npat}$  and $\sseq{\patType{\pctx_i}{p_i}{\tau_i}}{\npat}$
          \end{enumerate}
      \item (\textbf{Visibility}) $\uG = \biguplus_{0 \leq i < \npat} \uG_i$ and $\Gamma = \bigcup_{0 \leq i < \npat} \pctx_i$
\end{enumerate}
\end{theorem}


% \newcommand{\pTLMsFormallySec}{Parametric TLMs, Formally}
% \section{\protect\pTLMsFormallySec}
% \label{sec:ptlms-formally}

% % \begin{figure}[p]
% % \[\begin{array}{llcl}
% % \mathsf{UMType} & \urho & ::= 
% % %& \autype{\utau} 
% % & \utau & \text{type annotation}\\
% % % &&
% % %& \aualltypes{\ut}{\urho} 
% % % & \alltypes{\ut}{\urho} & \text{type parameterization}\\
% % &&
% % %& \auallmods{\usigma}{\uX}{\urho} 
% % & \allmods{\uX}{\usigma}{\urho} & \text{module parameterization}\\
% % \mathsf{UMExp} & \uepsilon & ::= 
% % %& \abindref{\tsmv} 
% % & \tsmv & \text{TLM identifier reference}\\
% % % &&
% % %& \auabstype{\ut}{\uepsilon} 
% % % & \abstype{\ut}{\uepsilon} & \text{type abstraction}\\
% % &&
% % %& \auabsmod{\usigma}{\uX}{\uepsilon} 
% % & \absmod{\uX}{\usigma}{\uepsilon} & \text{module abstraction}\\
% % % &&
% % %& \auaptype{\utau}{\uepsilon} 
% % % & \aptype{\uepsilon}{\utau} & \text{type application}\\
% % &&
% % %& \auapmod{\uM}{\uepsilon} 
% % & \apmod{\uepsilon}{\uX} & \text{module application}\ECC
% % \end{array}
% % \]
% % \caption{Syntax of unexpanded TLM types and expressions.}
% % \label{fig:P-macro-expressions-types-u}
% % \end{figure}

% We will now outline $\miniVerseParam$, a calculus that extends $\miniVersePat$ with parametric TLMs. This calculus is organized, like $\miniVersePat$, as an unexpanded language (UL) defined by typed expansion to an expanded language (XL). There is not enough space to describe $\miniVerseParam$ with the same level of detail as in Sec. \ref{sec:setlms-formally}, so we highlight only the most important concepts below. The details are in the supplement.

% The XL consists of 1) module expressions, $M$, classified by signatures, $\sigma$; 2) constructions, $c$, classified by kinds, $\kappa$; and 3) expressions classified by types, which are constructions of kind $\akty$ (we use metavariables $\tau$ instead of $c$ for types by convention.) Metavariables $X$ ranges over module variables and $u$ or $t$ over construction variables. The module and construction languages are based closely on those defined by \citet{pfple1}, which in turn are based on early work by \citet{MacQueen:1984:MSM:800055.802036,DBLP:conf/popl/MacQueen86}, subsequent work on the phase splitting interpretation of modules \cite{harper1989higher} and on using dependent singleton kinds to track type identity \cite{stone2006extensional,DBLP:conf/lfmtp/Crary09}, and finally on formal developments by \citet{dreyer2005understanding} and \citet{conf/popl/LeeCH07}. A complete account of these developments is unfortunately beyond the scope of this paper. The expression language extends the language of $\miniVersePat$ only to allow projection out of modules.

% The main conceptual difference between $\miniVersePat$ and $\miniVerseParam$ is that $\miniVerseParam$ introduces the notion of unexpanded and expanded TLM expressions and types, as shown in Fig.~\ref{fig:P-macro-expressions-types}. 

% \begin{figure}[h]
% \small
% \begin{minipage}{0.38\textwidth}
% $\arraycolsep=3pt\begin{array}{llcl}
% \mathsf{UMType} & \urho & ::= & \utau ~\vert~ \allmods{\uX}{\usigma}{\urho}\\
% \mathsf{UMExp} & \uepsilon & ::= & \tsmv ~\vert~ \absmod{\uX}{\usigma}{\uepsilon}~\vert~ \apmod{\uepsilon}{\uX}
% \end{array}$
% \end{minipage}
% \begin{minipage}{0.6\textwidth}
% $\arraycolsep=3pt\begin{array}{llcl}
% \mathsf{MType} & \rho & ::= & \aetype{\tau} ~\vert~ \aeallmods{\sigma}{X}{\rho} \\
% \mathsf{MExp} & \epsilon & ::= & \adefref{a} ~\vert~ \aeabsmod{\sigma}{X}{\epsilon} ~\vert~\aeapmod{X}{\epsilon} 
% \end{array}$
% \end{minipage}
% \caption{Syntax of unexpanded and expanded TLM types and expressions in $\miniVerseParam$}
% \label{fig:P-macro-expressions-types}
% \end{figure}
% The TLM type $\aeallmods{\sigma}{X}{\rho}$ classifies TLM expressions that have one module parameter matching $\sigma$. For simplicity, we formalize only module parameters. Type parameters can be expressed as module parameters having exactly one abstract type member.

% The rule governing expression TLM application, reproduced below, touches all of the main ideas in $\miniVerseParam$, so we will refer to it throughout the remainder of this section.%It might be useful to compare this rule to Rule \textsc{ee-ap-sptsm}.
% {\small\begin{mathpar}
% \inferrule[ee-ap-petsm]{
%   \uOmega = \uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}\\
%   \uPsi=\uAS{\uA}{\Psi}\\\\
%   \tsmexpExpandsExp{\uOmega}{\uPsi}{\uepsilon}{\epsilon}{\aetype{\tau_\text{final}}}\\
%   \tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}\\\\
%   \tsmdefof{\epsilon_\text{normal}}=a\\
%   \Psi = \Psi', \petsmdefn{a}{\rho}{\eparse}\\\\
%   \encodeBody{b}{\ebody}\\
%   \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{e_\text{pproto}}}\\
%   \decodePCEExp{e_\text{pproto}}{\pce}\\\\
%   \prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon_\text{normal}}{\aetype{\tau_\text{proto}}}{\omega}{\Omega_\text{params}}\\\\
%   \segOK{\segof{\ce}}{b}\\
%   \cvalidEP{\Omega_\text{params}}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\ce}{e}{\tau_\text{proto}}
% }{
%   \expandsP{\uOmega}{\uPsi}{\uPhi}{\utsmap{\uepsilon}{b}}{[\omega]e}{[\omega]\tau_\text{proto}}
% }
% \end{mathpar}}

% The first two premises simply deconstruct the (unified) unexpanded context $\uOmega$ (which tracks the expansion of expression, constructor and module identifiers, as $\uDelta$ and $\uGamma$ did in $\miniVersePat$) and peTLM context, $\uPsi$. Next, we expand $\uepsilon$ according to straightforward unexpanded peTLM expression expansion rules. The resulting TLM expression, $\epsilon$, must be defined at a type (i.e. no quantification over modules must remain once the literal body is encountered.)

% The fourth premise performs \emph{peTLM expression normalization}, $\tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}$. This is defined in terms of a structural operational semantics \cite{DBLP:journals/jlp/Plotkin04a} with two stepping rules:
% % \begin{equation*}\tag{\ref{rule:tsmexpEvalsExp}}
% % \inferrule{
% %   \tsmexpMultistepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}\\
% %   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon'}
% % }{
% %   \tsmexpEvalsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
% % }
% % \end{equation*}
% % where the multistep judgement, $\tsmexpMultistepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}$, is defined as the reflexive, transitive closure of the stepping judgement defined by the following rules:
% % \begin{equation*}\tag{\ref{rule:tsmexpStepsExp-aptype-1}}
% % \inferrule{
% %   \tsmexpStepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
% % }{
% %   \tsmexpStepsExp{\Omega}{\Psi}{\aeaptype{\tau}{\epsilon}}{\aeaptype{\tau}{\epsilon'}}
% % }
% % \end{equation*}
% % \begin{equation*}\tag{\ref{rule:tsmexpStepsExp-aptype-2}}
% % \inferrule{ }{
% %   \tsmexpStepsExp{\Omega}{\Psi}{\aeaptype{\tau}{\aeabstype{t}{\epsilon}}}{[\tau/t]\epsilon}
% % }
% % \end{equation*}
% {\small\begin{mathpar}
% \inferrule[eps-dyn-apmod-subst-e]{ }{
%   \tsmexpStepsExp{\Omega}{\Psi}{\aeapmod{X}{\aeabsmod{\sigma}{X'}{\epsilon}}}{[X/X']\epsilon}
% }

% \inferrule[eps-dyn-apmod-steps-e]{
%   \tsmexpStepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
% }{
%   \tsmexpStepsExp{\Omega}{\Psi}{\aeapmod{X}{\epsilon}}{\aeapmod{X}{\epsilon'}}
% }
% \end{mathpar}}
% % The peTLM expression normal forms are defined as follows:
% % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-defref}}
% % \inferrule{ }{
% %   \tsmexpNormalExp{\Omega}{\Psi, \petsmdefn{a}{\rho}{\eparse}}{\adefref{a}}
% % }
% % \end{equation*}
% % % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-abstype}}
% % % \inferrule{ }{
% % %   \tsmexpNormalExp{\Omega}{\Psi}{\aeabstype{t}{\epsilon}}
% % % }
% % % \end{equation*}
% % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-absmod}}
% % \inferrule{ }{
% %   \tsmexpNormalExp{\Omega}{\Psi}{\aeabsmod{\sigma}{X}{\epsilon}}
% % }
% % \end{equation*}
% % % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-aptype}}
% % % \inferrule{
% % %   \epsilon \neq \aeabstype{t}{\epsilon'}\\
% % %   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon}
% % % }{
% % %   \tsmexpNormalExp{\Omega}{\Psi}{\aeaptype{\tau}{\epsilon}}
% % % }
% % % \end{equation*}
% % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-apmod}}
% % \inferrule{
% %   \epsilon \neq \aeabsmod{\sigma}{X'}{\epsilon'}\\
% %   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon}
% % }{
% %   \tsmexpNormalExp{\Omega}{\Psi}{\aeapmod{X}{\epsilon}}
% % }
% % \end{equation*}
% \vspace{-5px}

% Normalization eliminates parameters introduced in higher-order abbreviations, leaving only those parameter applications specified by the original TLM definition. Normal forms and progress and preservation theorems are established in the supplement.

% The third row of premises looks up the applied TLM's definition by invoking a simple metafunction to extract its name, $a$, then looking up $a$ within the peTLM definition context, $\Psi$.
% % \begin{align}
% % \tsmdefof{\adefref{a}} & = a \tag{\ref{eqn:tsmdefof-adefref}}\\
% % % \tsmdefof{\aeabstype{t}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-abstype}}\\
% % \tsmdefof{\aeabsmod{\sigma}{X}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-absmod}}\\
% % % \tsmdefof{\aeaptype{\tau}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-aptype}}\\
% % \tsmdefof{\aeapmod{X}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-apmod}}
% % \end{align}


% The fourth row of premises 1) encodes the body as a value of the type $\tBody$; 2) applies the parse function; and 3) decodes the result, producing a \emph{parameterized proto-expression}, $\pce$. Parameterized proto-expressions, $\pce$, are ABTs that serve simply to introduce the parameter bindings into an underlying proto-expression, $\ce$. The syntax of parameterized proto-expressions is given below.

% \vspace{-4px}{\small\[\begin{array}{llcl}
% % \textbf{Sort} & & & \textbf{Operational Form} & \textbf{Stylized Form} & \textbf{Description}\\
% % \LCC \color{Yellow}&\color{Yellow}&\color{Yellow}& \color{Yellow} & \color{Yellow} & \color{Yellow}\\
% \mathsf{PPrExp} & \pce & ::= & \apceexp{\ce} ~\vert~ \apcebindmod{X}{\pce}
% \end{array}\]}%
% \vspace{-6px}%

% There must be one binder in $\pce$ for each TLM parameter specified by $a$. (In Reason, we can insert these binders automatically as a convenience.) 

% The judgement on the fifth row of Rule \textsc{ee-ap-petsm} then \emph{deparameterizes} $\pce$ by peeling away these binders to produce 1) the underlying proto-expression, $\ce$, with the variables that stand for the parameters free; 2) a corresponding deparameterized type, $\tau_\text{proto}$, that uses the same free variables to stand for the parameters; 3) a \emph{substitution}, $\omega$, that pairs the applied parameters from $\epsilon_\text{normal}$ with the corresponding variables generated when peeling away the binders in $\pce$; and 4) a corresponding \emph{parameter context}, $\Omega_\text{params}$, that tracks the signatures of these variables. The two rules governing the proto-expression deparameterization judgement are below:
% {\small\begin{mathpar}
% \inferrule{ }{
%   \prepce{\Omega_\text{app}}{\Psi, \petsmdefn{a}{\rho}{\eparse}}{\apceexp{\ce}}{\ce}{\adefref{a}}{\rho}{\emptyset}{\emptyset}
% }

% \vspace{-4px}\inferrule{
%   \prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon}{\aeallmods{\sigma}{X}{\rho}}{\omega}{\Omega}\\
%   X \notin \domof{\Omega_\text{app}}
% }{
%   \prepce{\Omega_\text{app}}{\Psi}{\apcebindmod{X}{\pce}}{\ce}{\aeapmod{X'}{\epsilon}}{\rho}{(\omega, X'/X)}{(\Omega, X : \sigma)}
% }
% \end{mathpar}}%
% This judgement can be pronounced ``when applying peTLM $\epsilon$, $\pce$ has deparameterization $\ce$ leaving $\rho$ with parameter substitution $\omega$''. 
% Notice based on the second rule that every module binding in $\pce$ must pair with a corresponding module parameter application. Moreover, the variables standing for parameters must not appear in $\Omega_\text{app}$, i.e. $\domof{\Omega_\text{params}}$ must be disjoint from $\domof{\Omega_\text{app}}$ (this requirement can always be discharged by alpha-variation.)

% The final row of premises checks that the segmentation of $\ce$ is valid and  performs proto-expansion validation under the parameter context, $\Omega_\text{param}$ (rather than the empty context, as was the case in $\miniVersePat$.) The conclusion of the rule applies the parameter substitution, $\omega$, to the resulting expression and the deparameterized type.

% Proto-expansion validation operates conceptually as in $\miniVersePat$. The only subtlety has to do with the type annotations on references to spliced terms. As described at the end of Sec. \ref{sec:ptsms-by-example}, these annotations might refer to the parameters, so the parameter substitution, $\omega$, which is tracked by the splicing scene, must be applied to the type annotation before proceeding recursively to expand the referenced unexpanded term. However, the spliced term itself must treat parameters parametrically, so the substitution is not applied in the conclusion of the following rule:
% \begin{mathpar}\label{rule:cvalidE-P-splicede}
% \inferrule{
%   \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\
%     \cvalidC{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\ctau}{\tau}{\akty}\\
%   \expandsP{\uOmega}{\uPsi}{\uPhi}{\ue}{e}{[\omega]\tau}\\\\
%   \uOmega=\uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}\\
%   \domof{\Omega} \cap \domof{\Omega_\text{app}} = \emptyset
% }{
%   \cvalidEP{\Omega}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\acesplicede{m}{n}{\ctau}}{e}{\tau}
% }
% \end{mathpar}
% (This is only sensible because we maintain the invariant that $\Omega$ is always an extension of $\Omega_\text{params}$.)

% The calculus enjoys metatheoretic properties analagous to those described in Sec. \ref{sec:s-metatheory}, modified to account for the presence of modules, kinds and parameterization. The following theorem establishes the abstract reasoning principles available when applying a parametric expression TLM. The clauses are directly analagous to those of Theorem \ref{thm:setlm-reasoning}, so for reasons of space we do not repeat the inline descriptions. The \textbf{Kinding} clauses can be understood by analogy to the \textbf{Typing} clauses. The details of parametric pattern TLMs (ppTLMs) are analagous (see supplement.)
% \vspace{-3px}
% \begin{theorem}[peTLM Reasoning Principles]
% If $\expandsP{\uOmega}{\uPsi}{\uPhi}{\utsmap{\uepsilon}{b}}{e}{\tau}$ then:
% \begin{enumerate}[nolistsep,noitemsep]
%   \item $\uOmega=\uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}$
%   \item $\uPsi=\uAS{\uA}{\Psi}$
%   \item (\textbf{Typing 1}) $\tsmexpExpandsExp{\uOmega}{\uPsi}{\uepsilon}{\epsilon}{\aetype{\tau}}$ and $\hastypeP{\Omega_\text{app}}{e}{\tau}$
%   \item $\tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}$
%   \item $\tsmdefof{\epsilon_\text{normal}}=a$
%   \item $\Psi = \Psi', \petsmdefn{a}{\rho}{\eparse}$
%   \item $\encodeBody{b}{\ebody}$
%     \item $\evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{e_\text{pproto}}}$
%   \item $\decodePCEExp{e_\text{pproto}}{\pce}$
%   \item $\prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon_\text{normal}}{\aetype{\tau_\text{proto}}}{\omega}{\Omega_\text{params}}$
%   \item (\textbf{Segmentation}) $\segOK{\segof{\ce}}{b}$
%   \item $\cvalidEP{\Omega_\text{params}}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\ce}{e'}{\tau_\text{proto}}$
%   \item $e = [\omega]e'$
%   \item $\tau = [\omega]\tau_\text{proto}$
%   \item $
%     \segof{\ce} = \sseq{\acesplicedk{m_i}{n_i}}{\nkind} \cup \sseq{\acesplicedc{m'_i}{n'_i}{\cekappa'_i}}{\ncon} \cup $ \\
%      ~~~~$          \sseq{\acesplicede{m''_i}{n''_i}{\ctau_i}}{\nexp}
%     $
%   \item (\textbf{Kinding 1}) $\sseq{\kExpands{\uOmega}{\parseUKindF{\bsubseq{b}{m_i}{n_i}}}{\kappa_i}}{\nkind}$ and \\ ~~~~$\sseq{\iskind{\Omega_\text{app}}{\kappa_i}}{\nkind}$
%   \item (\textbf{Kinding 2}) $\sseq{\cvalidK{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\cekappa'_i}{\kappa'_i}}{\ncon}$ and $\sseq{\iskind{\Omega_\text{app}}{[\omega]\kappa'_i}}{\ncon}$
%   \item (\textbf{Kinding 3}) $\sseq{\cExpands{\uOmega}{\parseUConF{\bsubseq{b}{m'_i}{n'_i}}}{c_i}{[\omega]\kappa'_i}}{\ncon}$ and $\sseq{\haskind{\Omega_\text{app}}{c_i}{[\omega]\kappa'_i}}{\ncon}$
%   \item (\textbf{Kinding 4}) $\sseq{\cvalidC{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\ctau_i}{\tau_i}{\akty}}{\nexp}$ and $\sseq{\haskind{\Omega_\text{app}}{[\omega]\tau_i}{\akty}}{\nexp}$
%   \item (\textbf{Typing 2}) $\sseq{\expandsP{\uOmega}{\uPsi}{\uPhi}{\parseUExpF{\bsubseq{b}{m''_i}{n''_i}}}{e_i}{[\omega]\tau_i}}{\nexp}$ and $\sseq{\hastypeP{\Omega_\text{app}}{e_i}{[\omega]\tau_i}}{\nexp}$
%   \item (\textbf{Capture Avoidance}) $e = [\sseq{\kappa_i/k_i}{\nkind}, \sseq{c_i/u_i}{\ncon}, \sseq{e_i/x_i}{\nexp}, \omega]e''$ for some $e''$ and fresh $\sseq{k_i}{\nkind}$ and fresh $\sseq{u_i}{\ncon}$ and fresh $\sseq{x_i}{\nexp}$
%   \item (\textbf{Context Independence}) $\mathsf{fv}(e'') \subset \sseq{k_i}{\nkind} \cup \sseq{u_i}{\ncon} \cup \sseq{x_i}{\nexp} \cup \domof{\OParams}$
%   % $\hastypeP{\sseq{\Khyp{k_i}}{\nkind} \cup \sseq{u_i :: [\omega]\kappa'_i}{\ncon} \cup \sseq{x_i : [\omega]\tau_i}{\nexp}}{[\omega]e''}{\tau}$\todo{maybe restate this in terms of free variables of e'' here and elsewhere, because context isn't technically well-formed here?}
% \end{enumerate}
% \end{theorem}

% \vspace{-6px}

% % The following theorem summarizes the abstract reasoning principles available to programmers when applying a pattern TLM. Most of the labeled clauses are analagous to those described above, so we omit their descriptions.
% % % \begingroup
% % % \def\thetheorem{\ref{thm:spTLM-Typing-Segmentation}}
% % \begin{theorem}[spTLM Reasoning Principles]
% % % \label{thm:spTLM-Typing-Segmentation}
% % If $\patExpands{\upctx}{\uPhi}{\utsmap{\tsmv}{b}}{p}{\tau}$ where $\uDelta=\uDD{\uD}{\Delta}$ and $\uGamma=\uGG{\uG}{\Gamma}$ then all of the following hold:
% % \begin{enumerate}[noitemsep,nolistsep]
% %         \item (\textbf{Typing 1}) $\uPhi=\uPhi', \uPhyp{\tsmv}{a}{\tau}{\eparse}$ and $\patType{\pctx}{p}{\tau}$
% %         \item $\encodeBody{b}{\ebody}$
% %         \item $\evalU{\eparse(\ebody)}{\aein{\mathtt{SuccessP}}{\ecand}}$
% %         \item $\decodeCEPat{\ecand}{\cpv}$
% %         \item (\textbf{Segmentation}) $\segOK{\segof{\cpv}}{b}$
% %         \item $\segof{\cpv} = \sseq{\acesplicedt{n'_i}{m'_i}}{\nty} \cup \sseq{\acesplicedp{m_i}{n_i}{\ctau_i}}{\npat}$
% %         \item (\textbf{Typing 2}) $\sseq{
% %               \expandsTU{\uDelta}
% %               {
% %                 \parseUTypF{\bsubseq{b}{m'_i}{n'_i}}
% %               }{\tau'_i}
% %             }{\nty}$ and $\sseq{\istypeU{\Delta}{\tau'_i}}{\nty}$
% %         \item (\textbf{Typing 3}) $\sseq{
% %           \cvalidT{\emptyset}{
% %             \tsceneUP
% %               {\uDelta}{b}
% %           }{
% %             \ctau_i
% %           }{\tau_i}
% %         }{\npat}$ and $\sseq{\istypeU{\Delta}{\tau_i}}{\npat}$
% %         \item (\textbf{Typing 4}) $\sseq{
% %           \patExpands
% %             {\uGG{\uG_i}{\pctx_i}}
% %             {\uPhi}
% %             {\parseUPatF{\bsubseq{b}{m_i}{n_i}}}
% %             {p_i}
% %             {\tau_i}
% %         }{\npat}$  and $\sseq{\patType{\pctx_i}{p_i}{\tau_i}}{\npat}$
% %       \item (\textbf{Visibility}) $\uG = \biguplus_{0 \leq i < \npat} \uG_i$ and $\Gamma = \bigcup_{0 \leq i < \npat} \pctx_i$
% %         \begin{quote}
% %           \begin{grayparbox}
% %           The hypotheses generated by the TLM application are exactly those generated by the spliced patterns.
% %           \end{grayparbox}
% %         \end{quote}
% % \end{enumerate}
% % \end{theorem}
% % 
% % \begin{proof} 
% % The proof, in the supplement, relies on an auxiliary lemma about decomposing proto-patterns.

% % \subsection{Retrospective} Let us step back and briefly review what we have accomplished so far. First, we defined an entirely standard expanded language (Sec. \ref{sec:s-XL}). Then we mirrored the forms in this language to form the common forms of the unexpanded language (Sec. \ref{sec:s-UL}), taking care to handle identifiers carefully (Sec. \ref{sec:s-TE}). We also added forms for defining TLMs (Sec. \ref{sec:s-TLM-def}). We then considered TLM application, which invokes a parse function to programmatically produce an encoding of a proto-expansion (Sec. \ref{sec:s-TLM-ap}). Each proto-expansion is then decoded, validated and inductively expanded  (Sec. \ref{sec:s-PEV}) to establish the abstract reasoning principles just described (Sec. \ref{sec:s-metatheory}). 
% % We write $\tsfrom{\escenev}$ for the type splicing scene constructed by dropping unnecessary contexts from $\escenev$:
% % \[\tsfrom{\esceneUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{b}} = \tsceneUP{\uDelta}{b}\]

% % Figure \ref{fig:U-candidate-terms} defines the syntax of proto-types, $\ctau$, and proto-expressions, $\ce$. Proto-types and -expressions 

% % Each expanded form maps onto a proto-expansion form. We refer to these as the \emph{common proto-expansion forms}. The mapping is given explicitly in Appendix \ref{appendix:proto-expansions-SES}.

% % There are two ``interesting'' proto-expansion forms, highlighted in yellow in Figure \ref{fig:U-candidate-terms}: a proto-type form for \emph{references to spliced unexpanded types}, $\acesplicedt{m}{n}$, and a proto-expression form for \emph{references to spliced unexpanded expressions}, $\acesplicede{m}{n}{\ctau}$, where $m$ and $n$ are natural numbers.%TLM utilize these to splice types and unexpanded expressions out of literal bodies.


% % \subsubsection{Proto-Type Validation}\label{sec:SE-proto-type-validation}
% % The \emph{proto-type validation judgement}, $\cvalidT{\Delta}{\tscenev}{\ctau}{\tau}$, is inductively defined by Rules (\ref{rules:cvalidT-U}).

% % \paragraph{Common Forms} Rules (\ref{rule:cvalidT-U-tvar}) through (\ref{rule:cvalidT-U-sum}) validate proto-types of common form. The first three of these are reproduced below.
% % %Each of these rules is defined based on the corresponding type formation rule, i.e. Rules (\ref{rule:istypeU-var}) through (\ref{rule:istypeU-sum}), respectively. For example, the following proto-types validation rules are based on type formation rules (\ref{rule:istypeU-var}), (\ref{rule:istypeU-parr}) and (\ref{rule:istypeU-all}), respectively: 
% % % \begin{subequations}%\label{rules:cvalidT-U}
% % \begin{equation*}\tag{\ref{rule:cvalidT-U-tvar}}
% % \inferrule{ }{
% %   \cvalidT{\Delta, \Dhyp{t}}{\tscenev}{t}{t}
% % }
% % \end{equation*}
% % \begin{equation*}\tag{\ref{rule:cvalidT-U-parr}}
% %   \inferrule{
% %     \cvalidT{\Delta}{\tscenev}{\ctau_1}{\tau_1}\\
% %     \cvalidT{\Delta}{\tscenev}{\ctau_2}{\tau_2}
% %   }{
% %     \cvalidT{\Delta}{\tscenev}{\aceparr{\ctau_1}{\ctau_2}}{\aparr{\tau_1}{\tau_2}}
% %   }
% % \end{equation*}
% % \begin{equation*}\tag{\ref{rule:cvalidT-U-all}}
% %   \inferrule {
% %     \cvalidT{\Delta, \Dhyp{t}}{\tscenev}{\ctau}{\tau}
% %   }{
% %     \cvalidT{\Delta}{\tscenev}{\aceall{t}{\ctau}}{\aall{t}{\tau}}
% %   }
% % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidT-U-rec}
% % %   \inferrule{
% % %     \cvalidT{\Delta, \Dhyp{t}}{\tscenev}{\ctau}{\tau}
% % %   }{
% % %     \cvalidT{\Delta}{\tscenev}{\acerec{t}{\ctau}}{\arec{t}{\tau}}
% % %   }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidT-U-prod}
% % %   \inferrule{
% % %     \{\cvalidT{\Delta}{\tscenev}{\ctau_i}{\tau_i}\}_{i \in \labelset}
% % %   }{
% % %     \cvalidT{\Delta}{\tscenev}{\aceprod{\labelset}{\mapschema{\ctau}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% % %   }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidT-U-sum}
% % %   \inferrule{
% % %     \{\cvalidT{\Delta}{\tscenev}{\ctau_i}{\tau_i}\}_{i \in \labelset}
% % %   }{
% % %     \cvalidT{\Delta}{\tscenev}{\acesum{\labelset}{\mapschema{\ctau}{i}{\labelset}}}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% % %   }
% % % \end{equation*}

% % These rules, like the rules for common unexpanded type forms,  mirror the corresponding type formation rules, i.e. Rules (\ref{rules:istypeU}). The type splicing scene, $\tscenev$, passes opaquely through these rules. 
% % % We can express this scheme more precisely with the following rule transformation. For each rule in Rules (\ref{rules:istypeU}), 
% % % \begin{mathpar}
% % % % \refstepcounter{equation}
% % % % \label{rule:cvalidT-U-rec}
% % % % \refstepcounter{equation}
% % % % \label{rule:cvalidT-U-prod}
% % % % \refstepcounter{equation}
% % % % \label{rule:cvalidT-U-sum}
% % % % \inferrule{J_1\\\cdots\\J_k}{J}
% % % \end{mathpar}
% % % the corresponding proto-types validation rule is
% % % \begin{mathpar}
% % % \inferrule{
% % %   \VTypof{J_1}\\
% % %   \cdots\\
% % %   \VTypof{J_k}
% % % }{
% % %   \VTypof{J}
% % % }
% % % \end{mathpar}
% % % where 
% % % \[\begin{split}
% % % \VTypof{\istypeU{\Delta}{\tau}} & = \cvalidT{\Delta}{\tscenev}{\VTypof{\tau}}{\tau}\\
% % % \VTypof{\{J_i\}_{i \in \labelset}} & = \{\VTypof{J_i}\}_{i \in \labelset}
% % % \end{split}\]
% % % and where $\VTypof{\tau}$, when $\tau$ is a metapattern of sort $\mathsf{Typ}$, is a metapattern of sort $\mathsf{CETyp}$ defined as follows:
% % % \begin{itemize}
% % % \item When $\tau$ is of definite form, $\VTypof{\tau}$ is defined as follows:
% % % \begin{align*}
% % % \VTypof{t} & = t\\
% % % \VTypof{\aparr{\tau_1}{\tau_2}} & = \aceparr{\VTypof{\tau_1}}{\VTypof{\tau_2}}\\
% % % \VTypof{\aall{t}{\tau}} & = \aceall{t}{\VTypof{\tau}}\\
% % % \VTypof{\arec{t}{\tau}} & = \acerec{t}{\VTypof{\tau}}\\
% % % \VTypof{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}} & = \aceprod{\labelset}{\mapschemax{\VTypofv}{\tau}{i}{\labelset}}\\
% % % \VTypof{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}} & = \acesum{\labelset}{\mapschemax{\VTypofv}{\tau}{i}{\labelset}}
% % % \end{align*}
% % % \item When $\tau$ is of indefinite form, $\VTypof{\tau}$ is a uniquely corresponding metapattern also of indefinite form. For example, $\VTypof{\tau_1}=\ctau_1$ and $\VTypof{\tau_2}=\ctau_2$.
% % % \end{itemize}

% % % It is instructive to use this rule transformation to generate Rules (\ref{rule:cvalidT-U-tvar}) through (\ref{rule:cvalidT-U-all}) above. We omit the remaining rules, i.e. Rules (\ref*{rule:cvalidT-U-rec}) through (\ref*{rule:cvalidT-U-sum}). 

% % Notice that in Rule (\ref{rule:cvalidT-U-tvar}), only type variables tracked by $\Delta$, the expansion's local type validation context, are well-formed. Type variables tracked by the application site unexpanded type formation context, which is a component of the type splicing scene, $\tscenev$, are not validated. %Indeed, $\tscenev$ passes opaquely through the rules above. %This achieves \emph{context-independent expansion} as described in Sec. \ref{sec:splicing-and-hygiene} for type variables -- seTLMs cannot impose ``hidden constraints'' on the application site unexpanded type formation context, because the type variables bound at the application site are simply not directly available to proto-types.

% % \paragraph{References to Spliced Types} The only proto-type form that does not correspond to a type form is $\acesplicedt{m}{n}$, which is a \emph{reference to a spliced unexpanded type}, i.e. it indicates that an unexpanded type should be parsed out from the literal body, which appears in the type splicing scene $\tscenev$, beginning at position $m$ and ending at position $n$, where $m$ and $n$ are natural numbers. Rule (\ref{rule:cvalidT-U-splicedt}) governs this form:
% % \begin{equation*}\tag{\ref{rule:cvalidT-U-splicedt}}
% %   \inferrule{
% %     \parseUTyp{\bsubseq{b}{m}{n}}{\utau}\\
% %     \expandsTU{\uDD{\uD}{\Delta_\text{app}}}{\utau}{\tau}\\
% %     \Delta \cap \Delta_\text{app} = \emptyset
% %   }{
% %     \cvalidT{\Delta}{\tsceneU{\uDD{\uD}{\Delta_\text{app}}}{b}}{\acesplicedt{m}{n}}{\tau}
% %   }
% % \end{equation*}
% % The first premise of this rule extracts the indicated subsequence of $b$ using the partial metafunction $\bsubseq{b}{m}{n}$ and parses it using the partial metafunction $\mathsf{parseUTyp}(b)$, characterized in Sec. \ref{sec:syntax-U}, to produce the spliced unexpanded type itself, $\utau$.

% % The second premise of Rule (\ref{rule:cvalidT-U-splicedt}) performs type expansion of $\utau$ under the application site unexpanded type formation context, $\uDD{\uD}{\Delta_\text{app}}$, which is a component of the type splicing scene. The hypotheses in the expansion's local type formation context, $\Delta$, are not made available to $\tau$. %This enforces the injunction on shadowing as described in Sec. \ref{sec:splicing-and-hygiene} for type variables that appear in proto-types. 

% % The third premise of Rule (\ref{rule:cvalidT-U-splicedt}) imposes the constraint that the proto-expansion's type formation context, $\Delta$, be disjoint from the application site type formation context, $\Delta_\text{app}$. This premise can always be discharged by $\alpha$-varying the proto-expansion that the reference to the spliced type appears within. 

% % Together, these two premises enforce the injunction on type variable capture as described in Sec. \ref{sec:uetsms-validation} -- the TLM provider can choose type variable names freely within a proto-expansion. We will consider this formally in Sec. \ref{sec:SE-metatheory} below. %, because the language prevents them from shadowing type variables at the application site (by $\alpha$-varying the proto-expansion as needed.)%Such a change in bound variable names is possible again because variables bound by the seTLM provider in a proto-expansion cannot ``leak into'' spliced terms because the hypotheses in $\Delta$ are not made available to the spliced type, $\tau$. 

% % Rules (\ref{rules:cvalidT-U}) validate the following lemma, which establishes that the final expansion of a valid proto-type is a well-formed type under the combined type formation context.
% % \begingroup
% % \def\thetheorem{\ref{lemma:candidate-expansion-type-validation}}
% % \begin{lemma}[Proto-Expansion Type Validation]
% % If $\cvalidT{\Delta}{\tsceneU{\uDD{\uD}{\Delta_\text{app}}}{b}}{\ctau}{\tau}$ and $\Delta \cap \Delta_\text{app}=\emptyset$ then $\istypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\tau}$.
% % \end{lemma}
% % \endgroup

% % \subsubsection{Proto-Expression Validation}
% % The \emph{proto-expression validation judgement}, $\cvalidE{\Delta}{\Gamma}{\escenev}{\ce}{e}{\tau}$, is defined mutually inductively with the typed expansion judgement by Rules (\ref{rules:cvalidE-U}) as follows.% This is necessary because a typed expansion judgement appears as a premise in Rule (\ref{rule:cvalidE-U-splicede}) below, and a proto-expression validation judgement appears as a premise in Rule (\ref{rule:expandsU-tsmap}) above.

% % \paragraph{Common Forms} Rules (\ref{rule:cvalidE-U-var}) through (\ref{rule:cvalidE-U-case}) validate proto-expressions of common form, as well as ascriptions and let binding. The first five of these rules are reproduced below:
% % %For each expanded expression form defined in Figure \ref{fig:U-expanded-terms}, Figure \ref{fig:U-candidate-terms} defines a corresponding proto-expression form. The validation rules for proto-expressions of these forms are each based on the corresponding typing rule in Rules (\ref{rules:hastypeU}). For example, the validation rules for proto-expressions of variable, function and function application form  are based on Rules (\ref{rule:hastypeU-var}) through (\ref{rule:hastypeU-ap}), respectively:
% % %\begin{subequations}%\label{rules:cvalidE-U}
% % \begin{equation*}\tag{\ref{rule:cvalidE-U-var}}
% % \inferrule{ }{
% %   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{x}{x}{\tau}
% % }
% % \end{equation*}
% % \begin{equation*}\tag{\ref{rule:cvalidE-U-asc}}
% % \inferrule{
% %   \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
% %   \cvalidE{\Delta}{\Gamma}{\escenev}{\ce}{e}{\tau}
% % }{
% %   \cvalidE{\Delta}{\Gamma}{\escenev}{\aceasc{\ctau}{\ce}}{e}{\tau}
% % }
% % \end{equation*}
% % \begin{equation*}\tag{\ref{rule:cvalidE-U-letsyn}}
% %   \inferrule{
% %     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_1}{e_1}{\tau_1}\\
% %     \cvalidE{\Delta}{\Gamma, x : \tau_1}{\ce_2}{e_2}{\tau_2}
% %   }{
% %     \cvalidE{\Delta}{\Gamma}{\escenev}{\aceletsyn{x}{\ce_1}{\ce_2}}{
% %       \aeap{\aelam{\tau_1}{x}{e_2}}{e_1}
% %     }{\tau_2}
% %   }
% % \end{equation*}
% % \begin{equation*}\tag{\ref{rule:cvalidE-U-lam}}
% % \inferrule{
% %   \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
% %   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{\ce}{e}{\tau'}
% % }{
% %   \cvalidE{\Delta}{\Gamma}{\escenev}{\acelam{\ctau}{x}{\ce}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}
% % }
% % \end{equation*}
% % \begin{equation*}\tag{\ref{rule:cvalidE-U-ap}}
% %   \inferrule{
% %     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_1}{e_1}{\aparr{\tau}{\tau'}}\\
% %     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_2}{e_2}{\tau}
% %   }{
% %     \cvalidE{\Delta}{\Gamma}{\escenev}{\aceap{\ce_1}{\ce_2}}{\aeap{e_1}{e_2}}{\tau'}
% %   }
% % \end{equation*}
% % Once again, the rules for common forms mirror the typing rules, i.e. Rules (\ref{rules:hastypeU}). The expression splicing scene, $\escenev$, passes opaquely through these rules.


% % Notice that in Rule (\ref{rule:cvalidE-U-var}), only variables tracked by the proto-expansion typing context, $\Gamma$, are validated. Variables  in the application site unexpanded typing context, which appears within the expression splicing scene $\escenev$, are not validated. This achieves \emph{context independence} as described in Sec. \ref{sec:uetsms-validation} -- seTLMs cannot impose ``hidden constraints'' on the application site unexpanded typing context, because the variable bindings at the application site are not directly available to proto-expansions. We will consider this formally in Sec. \ref{sec:SE-metatheory} below.

% % \paragraph{References to Spliced Unexpanded Expressions} The only proto-expression form that does not correspond to an expanded expression form is $\acesplicede{m}{n}{\ctau}$, which is a \emph{reference to a spliced unexpanded expression}, i.e. it indicates that an unexpanded expression should be parsed out from the literal body beginning at position $m$ and ending at position $n$. Rule (\ref{rule:cvalidE-U-splicede}) governs this form:
% % \begin{equation*}\tag{\ref{rule:cvalidE-U-splicede}}
% % \inferrule{
% %   \cvalidT{\emptyset}{\tsfrom{\escenev}}{\ctau}{\tau}\\
% %   \escenev=\esceneU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{b}\\
% %   \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\
% %   \expandsU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{\ue}{e}{\tau}\\\\
% %   \Delta \cap \Delta_\text{app} = \emptyset\\
% %   \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset
% % }{
% %   \cvalidE{\Delta}{\Gamma}{\escenev}{\acesplicede{m}{n}{\ctau}}{e}{\tau}
% % }
% % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-splicede}
% % % \inferrule{
% % %   \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\\\
% % %   \expandsU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{\ue}{e}{\tau}\\
% % %   \Delta \cap \Delta_\text{app} = \emptyset\\
% % %   \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset
% % % }{
% % %   \cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\splicede{m}{n}}{e}{\tau}
% % % }
% % % \end{equation*}

% % The first premise of this rule validates and expands the type annotation. This type must be context independent.

% % The second premise of this rule serves simply to reveal the components of the expression splicing scene.

% % The third premise of this rule extracts the indicated subsequence of $b$ using the partial metafunction $\bsubseq{b}{m}{n}$ and parses it using the partial metafunction $\mathsf{parseUExp}(b)$, characterized in Sec. \ref{sec:syntax-U}, to produce the referenced spliced unexpanded expression, $\ue$.

% % The fourth premise of Rule (\ref{rule:cvalidE-U-splicede}) performs typed expansion of $\ue$ assuming the application site contexts that appear in the expression splicing scene. Notice that the hypotheses in $\Delta$ and $\Gamma$ are not made available to $\ue$. 

% % The fifth premise of Rule (\ref{rule:cvalidE-U-splicede}) imposes the constraint that the proto-expansion's type formation context, $\Delta$, be disjoint from the application site type formation context, $\Delta_\text{app}$. Similarly, the sixth premise requires that the proto-expansion's typing context, $\Gamma$, be disjoint from the application site typing context, $\Gamma_\text{app}$. These two premises can always be discharged by $\alpha$-varying the proto-expression that the reference to the spliced unexpanded expression appears within. 
% % Together, these premises enforce the prohibition on capture as described in Sec. \ref{sec:uetsms-validation} -- the TLM provider can choose variable names freely within a proto-expansion, because the language prevents them from shadowing those at the application site.
% % %\end{subequations}
% % % \begin{subequations}\label{rules:cvalidE-U}
% % % \begin{equation*}\label{rule:cvalidE-U-var}
% % % \inferrule{ }{
% % %   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{x}{x}{\tau}
% % % }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-lam}
% % % \inferrule{
% % %   \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
% % %   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{\ce}{e}{\tau'}
% % % }{
% % %   \cvalidE{\Delta}{\Gamma}{\escenev}{\acelam{\ctau}{x}{\ce}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}
% % % }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-ap}
% % %   \inferrule{
% % %     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_1}{e_1}{\aparr{\tau}{\tau'}}\\
% % %     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_2}{e_2}{\tau}
% % %   }{
% % %     \cvalidE{\Delta}{\Gamma}{\escenev}{\aceap{\ce_1}{\ce_2}}{\aeap{e_1}{e_2}}{\tau'}
% % %   }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-tlam}
% % %   \inferrule{
% % %     \cvalidE{\Delta, \Dhyp{t}}{\Gamma}{\escenev}{\ce}{e}{\tau}
% % %   }{
% % %     \cvalidEX{\acetlam{t}{\ce}}{\aetlam{t}{e}}{\aall{t}{\tau}}
% % %   }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-tap}
% % %   \inferrule{
% % %     \cvalidEX{\ce}{e}{\aall{t}{\tau}}\\
% % %     \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau'}{\tau'}
% % %   }{
% % %     \cvalidEX{\acetap{\ce}{\ctau'}}{\aetap{e}{\tau'}}{[\tau'/t]\tau}
% % %   }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-fold}
% % %   \inferrule{
% % %     \cvalidT{\Delta, \Dhyp{t}}{\escenev}{\ctau}{\tau}\\
% % %     \cvalidEX{\ce}{e}{[\arec{t}{\tau}/t]\tau}
% % %   }{
% % %     \cvalidEX{\acefold{t}{\ctau}{\ce}}{\aefold{e}}{\arec{t}{\tau}}
% % %   }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-unfold}
% % %   \inferrule{
% % %     \cvalidEX{\ce}{e}{\arec{t}{\tau}}
% % %   }{
% % %     \cvalidEX{\aceunfold{\ce}}{\aeunfold{e}}{[\arec{t}{\tau}/t]\tau}
% % %   }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-tpl}
% % %   \inferrule{
% % %     \{\cvalidEX{\ce_i}{e_i}{\tau_i}\}_{i \in \labelset}
% % %   }{
% % %     \cvalidEX{\acetpl{\labelset}{\mapschema{\ce}{i}{\labelset}}}{\aetpl{\labelset}{\mapschema{e}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% % %   }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-pr}
% % %   \inferrule{
% % %     \cvalidEX{\ce}{e}{\aprod{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}}
% % %   }{
% % %     \cvalidEX{\acepr{\ell}{\ce}}{\aepr{\ell}{e}}{\tau}
% % %   }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-in}
% % %   \inferrule{
% % %     \{\cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau_i}{\tau_i}\}_{i \in \labelset}\\
% % %     \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
% % %     \cvalidEX{\ce}{e}{\tau}
% % %   }{
% % %     \left\{\shortstack{$\Delta~\Gamma \vdash_\uPsi \acein{\labelset, \ell}{\ell}{\mapschema{\ctau}{i}{\labelset}; \mapitem{\ell}{\ctau}}{\ce}$\\$\leadsto$\\$\aein{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}{e} : \asum{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}$\vspace{-1.2em}}\right\}
% % %   }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-case}
% % %   \inferrule{
% % %     \cvalidEX{\ce}{e}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}\\
% % %     \{\cvalidE{\Delta}{\Gamma, \Ghyp{x_i}{\tau_i}}{\escenev}{\ue_i}{e_i}{\tau}\}_{i \in \labelset}
% % %   }{
% % %     \cvalidEX{\acecase{\labelset}{\ce}{\mapschemab{x}{\ce}{i}{\labelset}}}{\aecase{\labelset}{e}{\mapschemab{x}{e}{i}{\labelset}}}{\tau}
% % %   }
% % % \end{equation*}
% % % \begin{equation*}\label{rule:cvalidE-U-splicede}
% % % \inferrule{
% % %   \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\\\
% % %   \Delta \cap \Delta_\text{app} = \emptyset\\
% % %   \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset\\
% % %   \expandsU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{\ue}{e}{\tau}
% % % }{
% % %   \cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\acesplicede{m}{n}}{e}{\tau}
% % % }
% % % \end{equation*}
% % % \end{subequations}

% % % Each form of expanded expression, $e$, corresponds to a form of proto-expression, $\ce$ (compare Figure \ref{fig:U-expanded-terms} and Figure \ref{fig:U-candidate-terms}). For each typing rule in Rules \ref{rules:hastypeU}, there is a corresponding proto-expression validation rule -- Rules (\ref{rule:cvalidE-U-var}) to (\ref{rule:cvalidE-U-case}) -- where the proto-expression and expanded expression correspond. The premises also correspond.


% % %Candidate expansions cannot themselves define or apply TLMs. This simplifies our metatheory, though it can be inconvenient at times for TLM providers. We discuss adding the ability to use TLMs within proto-expansions in Sec. \ref{sec:tsms-in-expansions}.


% % \subsection{Metatheory}\label{sec:SE-metatheory}
% % \subsubsection{Typed Expansion}
% % Let us now consider Theorem \ref{thm:typed-expansion-short-U}, which was mentioned at the beginnning of Sec. \ref{sec:typed-expansion-U} and is reproduced below:
% % \begingroup
% % \def\thetheorem{\ref{thm:typed-expansion-short-U}}
% % \begin{theorem}[Typed Expression Expansion] \hspace{-3px}If $\expandsU{\uDD{\uD}{\Delta}\hspace{-3px}}{\uGG{\uG}{\Gamma}\hspace{-3px}}{\uPsi}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
% % \end{theorem}
% % \endgroup

% %  To prove this theorem, we must  prove the following stronger theorem, because the proto-expression validation judgement is defined mutually inductively with the typed expansion judgement:

% % \begingroup
% % \def\thetheorem{\ref{thm:typed-expansion-full-U}}
% % \begin{theorem}[Typed Expansion (Full)] ~
% % \begin{enumerate}
% % \item If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uAS{\uA}{\Psi}}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
% % \item If $\cvalidE{\Delta}{\Gamma}{\esceneU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uAS{\uA}{\Psi}}{b}}{\ce}{e}{\tau}$ and $\Delta \cap \Delta_\text{app} = \emptyset$ and $\domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset$ then $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$.
% % \end{enumerate}
% % \end{theorem}
% % \endgroup
% % \begin{proof}
% % By mutual rule induction over Rules (\ref{rules:expandsU}) and Rules (\ref{rules:cvalidE-U}). The full proof is given in Appendix \ref{appendix:SES-typed-expression-expansion-metatheory}. We will reproduce the interesting cases below. 

% % The proof of part 1 proceeds by inducting over the typed expansion assumption. The only interesting cases are those related to seTLM definition and application, reproduced below. In the following cases, let $\uDelta=\uDD{\uD}{\Delta}$ and $\uGamma=\uGG{\uG}{\Gamma}$ and $\uPsi=\uAS{\uA}{\Psi}$.

% % \begin{byCases}
% % \item[\text{(\ref{rule:expandsU-syntax})}] We have 
% % \begin{pfsteps}
% %   \item \ue=\uesyntax{\tsmv}{\utau'}{\eparse}{\ue'} \BY{assumption}
% %   \item \expandsTU{\uDelta}{\utau'}{\tau'} \BY{assumption} \pflabel{expandsTU}
% %  \item \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultExp}} \BY{assumption}\pflabel{eparse}
% %   \item \expandsU{\uDelta}{\uGamma}{\uPsi, \uShyp{\tsmv}{a}{\tau'}{\eparse}}{\ue'}{e}{\tau} \BY{assumption}\pflabel{expandsU}
% % %  \item \uetsmenv{\Delta}{\Psi} \BY{assumption}\pflabel{uetsmenv1}
% %  \item \istypeU{\Delta}{\tau'} \BY{Lemma \ref{lemma:type-expansion-U} to \pfref{expandsTU}} \pflabel{istype}
% % %  \item \uetsmenv{\Delta}{\Psi, \xuetsmbnd{\tsmv}{\tau'}{\eparse}} \BY{Definition \ref{def:seTLM-def-ctx-formation} on \pfref{uetsmenv1}, \pfref{istype} and \pfref{eparse}}\pflabel{uetsmenv3}
% %   \item \hastypeU{\Delta}{\Gamma}{e}{\tau} \BY{IH, part 1(a) on \pfref{expandsU}}
% % \end{pfsteps}
% % \resetpfcounter 

% % \item[\text{(\ref{rule:expandsU-tsmap})}] We have 
% % \begin{pfsteps}
% %   \item \ue=\utsmap{\tsmv}{b} \BY{assumption}
% %   \item \uA = \uA', \vExpands{\tsmv}{a} \BY{assumption}
% %   \item \Psi=\Psi', \xuetsmbnd{a}{\tau}{\eparse} \BY{assumption}
% %   \item \encodeBody{b}{\ebody} \BY{assumption}
% %   \item \evalU{\eparse(\ebody)}{\aein{\lbltxt{SuccessE}}{\ecand}} \BY{assumption}
% %   \item \decodeCondE{\ecand}{\ce} \BY{assumption}
% %   \item \cvalidE{\emptyset}{\emptyset}{\esceneU{\uDelta}{\uGamma}{\uPsi}{b}}{\ce}{e}{\tau} \BY{assumption}\pflabel{cvalidE}
% % %  \item \uetsmenv{\Delta}{\Psi} \BY{assumption} \pflabel{uetsmenv}
% %   \item \emptyset \cap \Delta = \emptyset \BY{finite set intersection} \pflabel{delta-cap}
% %   \item {\emptyset} \cap \domof{\Gamma} = \emptyset \BY{finite set intersection} \pflabel{gamma-cap}
% %   \item \hastypeU{\emptyset \cup \Delta}{\emptyset \cup \Gamma}{e}{\tau} \BY{IH, part 2 on \pfref{cvalidE}, \pfref{delta-cap}, and \pfref{gamma-cap}} \pflabel{penultimate}
% %   \item \hastypeU{\Delta}{\Gamma}{e}{\tau} \BY{finite set and finite function identity over \pfref{penultimate}}
% % \end{pfsteps}
% % \resetpfcounter
% % \end{byCases}

% % The proof of part 2 proceeds by induction over the proto-expression validation assumption. The only interesting case governs references to spliced expressions. In the following cases, let $\uDelta_\text{app}=\uDD{\uD}{\Delta_\text{app}}$ and $\uGamma_\text{app}=\uGG{\uG}{\Gamma_\text{app}}$ and $\uPsi = \uAS{\uA}{\Psi}$.
% % \begin{byCases}
% % % \item[\text{(\ref{rule:cvalidE-U-var})}] ~
% % % \begin{pfsteps*}
% % %   \item $\ce=x$ \BY{assumption}
% % %   \item $e=x$ \BY{assumption}
% % %   \item $\Gamma=\Gamma', \Ghyp{x}{\tau}$ \BY{assumption}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gamma', \Ghyp{x}{\tau}}{x}{\tau}$ \BY{Rule (\ref{rule:hastypeU-var})} \pflabel{hastypeU}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma', \Ghyp{x}{\tau}}{\Gamma_\text{app}}}{x}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} over $\Gamma_\text{app}$ to \pfref{hastypeU}}
% % % \end{pfsteps*}
% % % \resetpfcounter

% % % \item[\text{(\ref{rule:cvalidE-U-lam})}] ~
% % % \begin{pfsteps*}
% % %   \item $\ce=\acelam{\ctau_1}{x}{\ce'}$ \BY{assumption}
% % %   \item $e=\aelam{\tau_1}{x}{e'}$ \BY{assumption}
% % %   \item $\tau=\aparr{\tau_1}{\tau_2}$ \BY{assumption}
% % %   \item $\cvalidT{\Delta}{\tsceneU{\uDelta_\text{app}}{b}}{\ctau_1}{\tau_1}$ \BY{assumption} \pflabel{cvalidT}
% % %   \item $\cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau_1}}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce'}{e'}{\tau_2}$ \BY{assumption} \pflabel{cvalidE}
% % % %  \item $\uetsmenv{\Delta_\text{app}}{\Psi}$ \BY{assumption} \pflabel{uetsmenv}
% % %   \item $\Delta \cap \Delta_\text{app}=\emptyset$ \BY{assumption} \pflabel{delta-disjoint}
% % %   \item $\domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{assumption} \pflabel{gamma-disjoint}
% % %   \item $x \notin \domof{\Gamma_\text{app}}$ \BY{identification convention} \pflabel{x-fresh}
% % %   \item $\domof{\Gamma, x : \tau_1} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{\pfref{gamma-disjoint} and \pfref{x-fresh}} \pflabel{gamma-disjoint2}
% % %   \item $\istypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\tau_1}$ \BY{Lemma \ref{lemma:candidate-expansion-type-validation} on \pfref{cvalidT} and \pfref{delta-disjoint}} \pflabel{istype}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma, \Ghyp{x}{\tau_1}}{\Gamma_\text{app}}}{e'}{\tau_2}$ \BY{IH, part 2 on \pfref{cvalidE}, \pfref{delta-disjoint} and \pfref{gamma-disjoint2}} \pflabel{hastype1}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}, \Ghyp{x}{\tau_1}}{e'}{\tau_2}$ \BY{exchange over $\Gamma_\text{app}$ on \pfref{hastype1}} \pflabel{hastype2}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aelam{\tau_1}{x}{e'}}{\aparr{\tau_1}{\tau_2}}$ \BY{Rule (\ref{rule:hastypeU-lam}) on \pfref{istype} and \pfref{hastype2}}
% % % \end{pfsteps*}
% % % \resetpfcounter

% % % \item[\text{(\ref{rule:cvalidE-U-ap})}] ~
% % % \begin{pfsteps*}
% % %   \item $\ce=\aceap{\ce_1}{\ce_2}$ \BY{assumption}
% % %   \item $e=\aeap{e_1}{e_2}$ \BY{assumption}
% % %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce_1}{e_1}{\aparr{\tau_2}{\tau}}$ \BY{assumption} \pflabel{cvalidE1}
% % %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce_2}{e_2}{\tau_2}$ \BY{assumption} \pflabel{cvalidE2}
% % % %  \item $\uetsmenv{\Delta_\text{app}}{\Psi}$ \BY{assumption} \pflabel{uetsmenv}
% % %   \item $\Delta \cap \Delta_\text{app}=\emptyset$ \BY{assumption} \pflabel{delta-disjoint}
% % %   \item $\domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{assumption} \pflabel{gamma-disjoint}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_1}{\aparr{\tau_2}{\tau}}$ \BY{IH, part 2 on \pfref{cvalidE1}, \pfref{delta-disjoint} and \pfref{gamma-disjoint}} \pflabel{hastypeU1}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_2}{\tau_2}$ \BY{IH, part 2 on \pfref{cvalidE2}, \pfref{delta-disjoint} and \pfref{gamma-disjoint}} \pflabel{hastypeU2}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aeap{e_1}{e_2}}{\tau}$ \BY{Rule (\ref{rule:hastypeU-ap}) on \pfref{hastypeU1} and \pfref{hastypeU2}}
% % % \end{pfsteps*}
% % % \resetpfcounter

% % % \item[\text{(\ref{rule:cvalidE-U-tlam})}] ~
% % % \begin{pfsteps}
% % %   \item \ce=\acetlam{t}{\ce'} \BY{assumption}
% % %   \item e = \aetlam{t}{e'} \BY{assumption}
% % %   \item \tau = \aall{t}{\tau'}\BY{assumption}
% % %   \item \cvalidE{\Delta, \Dhyp{t}}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce'}{e'}{\tau'} \BY{assumption} \pflabel{cvalidE}
% % % %  \item \uetsmenv{\Delta_\text{app}}{\Psi} \BY{assumption} \pflabel{uetsmenv}
% % %   \item \Delta \cap \Delta_\text{app}=\emptyset \BY{assumption} \pflabel{delta-disjoint}
% % %   \item \domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset \BY{assumption} \pflabel{gamma-disjoint}
% % %   \item \Dhyp{t} \notin \Delta_\text{app} \BY{identification convention}\pflabel{t-fresh}
% % %   \item \Delta, \Dhyp{t} \cap \Delta_\text{app} = \emptyset \BY{\pfref{delta-disjoint} and \pfref{t-fresh}}\pflabel{delta-disjoint2}
% % %   \item \hastypeU{\Dcons{\Delta, \Dhyp{t}}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e'}{\tau'} \BY{IH, part 2 on \pfref{cvalidE}, \pfref{delta-disjoint2} and \pfref{gamma-disjoint}}\pflabel{hastype1}
% % %   \item \hastypeU{\Dcons{\Delta}{\Delta_\text{app}, \Dhyp{t}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e'}{\tau'} \BY{exchange over $\Delta_\text{app}$ on \pfref{hastype1}}\pflabel{hastype2}
% % %   \item \hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aetlam{t}{e'}}{\aall{t}{\tau'}} \BY{Rule (\ref{rule:hastypeU-tlam}) on \pfref{hastype2}}
% % % \end{pfsteps}
% % % \resetpfcounter

% % % \item[{\text{(\ref{rule:cvalidE-U-tap})}}~\textbf{through}~{\text{(\ref{rule:cvalidE-U-case})}}] These cases follow analagously, i.e. we apply the IH, part 2 to all proto-expression validation judgements, Lemma \ref{lemma:candidate-expansion-type-validation} to all proto-type validation judgements, the identification convention to ensure that extended contexts remain disjoint, weakening and exchange as needed, and the corresponding typing rule in Rules (\ref{rule:hastypeU-tap}) through (\ref{rule:hastypeU-case}).
% % % \\

% % \item[\text{(\ref{rule:cvalidE-U-splicede})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=\acesplicede{m}{n}{\ctau}$ \BY{assumption}
% %   \item $  \escenev=\esceneU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{b}$ \BY{assumption}
% %   \item   $\cvalidT{\emptyset}{\tsfrom{\escenev}}{\ctau}{\tau}$ \BY{assumption}
% %   \item $\parseUExp{\bsubseq{b}{m}{n}}{\ue}$ \BY{assumption}
% %   \item $\expandsU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{\ue}{e}{\tau}$ \BY{assumption} \pflabel{expands}
% % %  \item $\uetsmenv{\Delta_\text{app}}{\Psi}$ \BY{assumption} \pflabel{uetsmenv}
% %   \item $\Delta \cap \Delta_\text{app}=\emptyset$ \BY{assumption} \pflabel{delta-disjoint}
% %   \item $\domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{assumption} \pflabel{gamma-disjoint}
% %   \item $\hastypeU{\Delta_\text{app}}{\Gamma_\text{app}}{e}{\tau}$ \BY{IH, part 1 on \pfref{expands}} \pflabel{hastype}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} over $\Delta$ and $\Gamma$ and exchange on \pfref{hastype}}
% % \end{pfsteps*}
% % \resetpfcounter
% % \end{byCases}

% % The mutual induction can be shown to be well-founded by showing that the following numeric metric on the judgements that we induct over is decreasing:
% % \begin{align*}
% % \sizeof{\expandsU{\uDelta}{\uGamma}{\uPsi}{\ue}{e}{\tau}} & = \sizeof{\ue}\\
% % \sizeof{\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce}{e}{\tau}} & = \sizeof{b}
% % \end{align*}
% % where $\sizeof{b}$ is the length of $b$ and $\sizeof{\ue}$ is the sum of the lengths of the literal bodies in $\ue$ (see Appendix \ref{appendix:SES-body-lengths}.)

% % The only case in the proof of part 1 that invokes part 2 is Case (\ref{rule:expandsU-tsmap}). There, we have that the metric remains stable: \begin{align*}
% %  & \sizeof{\expandsU{\uDelta}{\uGamma}{\uPsi}{\utsmap{\tsmv}{b}}{e}{\tau}}\\
% % =& \sizeof{\cvalidE{\emptyset}{\emptyset}{\esceneU{\uDelta}{\uGamma}{\uPsi}{b}}{\ce}{e}{\tau}}\\
% % =&\sizeof{b}\end{align*}

% % The only case in the proof of part 2 that invokes part 1 is Case (\ref{rule:cvalidE-U-splicede}). There, we have that $\parseUExp{\bsubseq{b}{m}{n}}{\ue}$ and the IH is applied to the judgement $\expandsU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{\ue}{e}{\tau}$ where $\uDelta_\text{app}=\uDD{\uD}{\Delta_\text{app}}$ and $\uGamma_\text{app}=\uGG{\uG}{\Gamma_\text{app}}$ and $\uPsi=\uAS{\uA}{\Psi}$. Because the metric is stable when passing from part 1 to part 2, we must have that it is strictly decreasing in the other direction:
% % \[\sizeof{\expandsU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{\ue}{e}{\tau}} < \sizeof{\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\acesplicede{m}{n}{\ctau}}{e}{\tau}}\]
% % i.e. by the definitions above, 
% % \[\sizeof{\ue} < \sizeof{b}\]

% % This is established by appeal to the following two conditions. The first condition states that an unexpanded expression constructed by parsing a textual sequence $b$ is strictly smaller, as measured by the metric defined above, than the length of $b$, because some characters must necessarily be used to invoke a TLM and delimit each literal body.
% % \begingroup
% % \def\thetheorem{\ref{condition:body-parsing}}
% % \begin{condition}[Expression Parsing Monotonicity] If $\parseUExp{b}{\ue}$ then $\sizeof{\ue} < \sizeof{b}$.\end{condition}
% % \endgroup
% % The second condition simply states that subsequences of $b$ are no longer than $b$.
% % \begingroup
% % \def\thetheorem{\ref{condition:body-subsequences}}
% % \begin{condition}[Body Subsequencing] If $\bsubseq{b}{m}{n}=b'$ then $\sizeof{b'} \leq \sizeof{b}$. \end{condition}
% % \endgroup

% % Combining these two conditions, we have that $\sizeof{\ue} < \sizeof{b}$ as needed.
% % \end{proof}

% % % We need to define the following theorem about proto-expression validation mutually with Theorem \ref{thm:typed-expansion-U}. 
% % % \begin{theorem}[Proto-Expansion Expression Validation]\label{thm:candidate-expansion-validation-U}
% % % If $\cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce}{e}{\tau}$ and $\uetsmenv{\Delta_\text{app}}{\uPsi}$ then $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$.
% % % \end{theorem}
% % % \begin{proof} By rule induction over Rules (\ref{rules:cvalidE-U}).
% % % \begin{byCases}
% % % \item[\text{(\ref{rule:cvalidE-U-var})}] ~
% % % \begin{pfsteps*}
% % %   \item $\ce=x$ \BY{assumption}
% % %   \item $e=x$ \BY{assumption}
% % %   \item $\Gamma=\Gamma', \Ghyp{x}{\tau}$ \BY{assumption}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gamma', \Ghyp{x}{\tau}}{x}{\tau}$ \BY{Rule (\ref{rule:hastypeU-var})} \pflabel{hastypeU}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma', \Ghyp{x}{\tau}}{\Gamma_\text{app}}}{x}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} over $\Gamma_\text{app}$ to \pfref{hastypeU}}
% % % \end{pfsteps*}
% % % \resetpfcounter

% % % \item[\text{(\ref{rule:cvalidE-U-lam})}] ~
% % % \begin{pfsteps*}
% % %   \item $\ce=\acelam{\ctau_1}{x}{\ce'}$ \BY{assumption}
% % %   \item $e=\aelam{\tau_1}{x}{e'}$ \BY{assumption}
% % %   \item $\tau=\aparr{\tau_1}{\tau_2}$ \BY{assumption}
% % %   \item $\cvalidT{\Delta}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ctau_1}{\tau_1}$ \BY{assumption} \pflabel{cvalidT}
% % %   \item $\cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau_1}}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce'}{e'}{\tau_2}$ \BY{assumption} \pflabel{cvalidE}
% % %   \item $\uetsmenv{\Delta_\text{app}}{\uPsi}$ \BY{assumption} \pflabel{uetsmenv}
% % %   \item $\istypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\tau_1}$ \BY{Lemma \ref{lemma:candidate-expansion-type-validation} on \pfref{cvalidT}} \pflabel{istype}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma, \Ghyp{x}{\tau_1}}{\Gamma_\text{app}}}{e'}{\tau_2}$ \BY{IH on \pfref{cvalidE} and \pfref{uetsmenv}} \pflabel{hastype1}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}, \Ghyp{x}{\tau_1}}{e'}{\tau_2}$ \BY{exchange over $\Gamma_\text{app}$ on \pfref{hastype1}} \pflabel{hastype2}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aelam{\tau_1}{x}{e'}}{\aparr{\tau_1}{\tau_2}}$ \BY{Rule (\ref{rule:hastypeU-lam}) on \pfref{istype} and \pfref{hastype2}}
% % % \end{pfsteps*}
% % % \resetpfcounter

% % % \item[\text{(\ref{rule:cvalidE-U-ap})}] ~
% % % \begin{pfsteps*}
% % %   \item $\ce=\aceap{\ce_1}{\ce_2}$ \BY{assumption}
% % %   \item $e=\aeap{e_1}{e_2}$ \BY{assumption}
% % %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce_1}{e_1}{\aparr{\tau_1}{\tau}}$ \BY{assumption} \pflabel{cvalidE1}
% % %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce_2}{e_2}{\tau_1}$ \BY{assumption} \pflabel{cvalidE2}
% % %   \item $\uetsmenv{\Delta_\text{app}}{\uPsi}$ \BY{assumption} \pflabel{uetsmenv}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_1}{\aparr{\tau_1}{\tau}}$ \BY{IH on \pfref{cvalidE1} and \pfref{uetsmenv}} \pflabel{hastypeU1}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_2}{\tau_1}$ \BY{IH on \pfref{cvalidE2} and \pfref{uetsmenv}} \pflabel{hastypeU2}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aeap{e_1}{e_2}}{\tau}$ \BY{Rule (\ref{rule:hastypeU-ap}) on \pfref{hastypeU1} and \pfref{hastypeU2}}
% % % \end{pfsteps*}
% % % \resetpfcounter

% % % \item[\VExpof{\text{\ref{rule:hastypeU-tlam}}}~\text{through}~\VExpof{\text{\ref{rule:hastypeU-case}}}] These cases follow analagously, i.e. we apply the IH to all proto-expression validation premises, Lemma \ref{lemma:candidate-expansion-type-validation} to all proto-types validation premises, weakening and exchange as needed, and then apply the corresponding typing rule.
% % % \\

% % % \item[\text{(\ref{rule:cvalidE-U-splicede})}] ~
% % % \begin{pfsteps*}
% % %   \item $\ce=\acesplicede{m}{n}$ \BY{assumption}
% % %   \item $\parseUExp{\bsubseq{b}{m}{n}}{\ue}$ \BY{assumption}
% % %   \item $\expandsU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{\ue}{e}{\tau}$ \BY{assumption} \pflabel{expands}
% % %   \item $\uetsmenv{\Delta_\text{app}}{\uPsi}$ \BY{assumption} \pflabel{uetsmenv}
% % %   \item $\hastypeU{\Delta_\text{app}}{\Gamma_\text{app}}{e}{\tau}$ \BY{Theorem \ref{thm:typed-expansion-U} on \pfref{expands} and \pfref{uetsmenv}} \pflabel{hastype}
% % %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} on \pfref{hastype}}
% % % \end{pfsteps*}
% % % \resetpfcounter
% % % \end{byCases}
% % % \end{proof}


% % %\qed


% % \subsubsection{Abstract Reasoning Principles}\label{sec:uetsms-reasoning-principles}
% % The following theorem summarizes the abstract reasoning principles that programmers can rely on when applying an seTLM. In particular, the programmer can be sure that:
% % \begin{enumerate} 
% % \item \textbf{Segmentation}: The segmentation determined by the proto-expansion actually segments the literal body (i.e. each segment is in-bounds and the segments are non-overlapping.)
% % \item \textbf{Typing 1}: The type of the expansion is consistent with the type annotation on the seTLM definition.
% % \item \textbf{Typing 2}: Each spliced type has a well-formed expansion at the application site.
% % \item \textbf{Typing 3}: Each type annotation on a reference to a spliced expression has a well-formed expansion at the application site.
% % \item \textbf{Typing 4}: Each spliced expression has a well-typed expansion consistent with its type annotation.
% % \item \textbf{Capture Avoidance}: The final expansion can be decomposed into a  term with variables in place of each spliced type or expression. The expansions of these spliced types and expressions can be substituted into this term in the standard capture avoiding manner.
% % \item \textbf{Context Independence}: The decomposed term is indeed well-typed independent of the application site contexts.

% % \end{enumerate}

% % \begingroup
% % \def\thetheorem{\ref{thm:tsc-SES}}
% % \begin{theorem}[seTLM Abstract Reasoning Principles]
% % If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uPsi}{\utsmap{\tsmv}{b}}{e}{\tau}$ then:
% % \begin{enumerate}
% % \item (\textbf{Typing 1}) $\uPsi = \uPsi', \uShyp{\tsmv}{a}{\tau}{\eparse}$ and $\hastypeU{\Delta}{\Gamma}{e}{\tau}$
% % \item $\encodeBody{b}{\ebody}$
% % \item $\evalU{\ap{\eparse}{\ebody}}{\aein{\lbltxt{SuccessE}}{\ecand}}$
% % \item $\decodeCondE{\ecand}{\ce}$
% % \item (\textbf{Segmentation}) $\segOK{\segof{\ce}}{b}$
% % \item $\segof{\ce} = \sseq{\acesplicedt{m'_i}{n'_i}}{\nty} \cup \sseq{\acesplicede{m_i}{n_i}{\ctau_i}}{\nexp}$
% % \item \textbf{(Typing 2)} $\sseq{
% %       \expandsTU{\uDD{\uD}{\Delta}}
% %       {
% %         \parseUTypF{\bsubseq{b}{m'_i}{n'_i}}
% %       }{\tau'_i}
% %     }{\nty}$ and $\sseq{\istypeU{\Delta}{\tau'_i}}{\nty}$
% % \item \textbf{(Typing 3)} $\sseq{
% %   \cvalidT{\emptyset}{
% %     \tsceneUP
% %       {\uDD
% %         {\uD}{\Delta}
% %       }{b}
% %   }{
% %     \ctau_i
% %   }{\tau_i}
% % }{\nexp}$ and $\sseq{\istypeU{\Delta}{\tau_i}}{\nexp}$
% % \item \textbf{(Typing 4)} $\sseq{
% %   \expandsU
% %     {\uDD{\uD}{\Delta}}
% %     {\uGG{\uG}{\Gamma}}
% %     {\uPsi}
% %     {\parseUExpF{\bsubseq{b}{m_i}{n_i}}}
% %     {e_i}
% %     {\tau_i}
% % }{\nexp}$ and $\sseq{\hastypeU{\Delta}{\Gamma}{e_i}{\tau_i}}{\nexp}$
% % \item (\textbf{Capture Avoidance}) $e = [\sseq{\tau'_i/t_i}{\nty}, \sseq{e_i/x_i}{\nexp}]e'$ for some $\sseq{t_i}{\nty}$ and $\sseq{x_i}{\nexp}$ and $e'$
% % \item (\textbf{Context Independence}) $\mathsf{fv}(e') \subset \sseq{t_i}{\nty} \cup \sseq{x_i}{\nexp}$
% %   % $\hastypeU
% %   % {\sseq{\Dhyp{t_i}}{\nty}}
% %   % {\sseq{x_i : \tau_i}{\nexp}}
% %   % {e'}{\tau}$
% % \end{enumerate}
% % \end{theorem}
% % \begin{proof} The proof, which involves auxiliary lemmas about the decomposition of proto-types and proto-expressions, is given in Appendix \ref{appendix:SES-reasoning-principles}.
% % \end{proof}
% % \endgroup
\section{Implementation}
\label{sec:implementation}
The implementation, \li{Relit}, which is an ongoing effort based on the finished design described in this paper, consists of a few changes to the context-free grammar of Reason together with a parse tree pre-processor for the OCaml compiler, currently using its PPX system (discussed in Sec.~\ref{sec:existing-approaches}).

\paragraph{TLM Definitions} The Reason grammar extension turns TLM definitions into module definitions of a stereotyped form. For example, the module \li{RegexNotation} defining \li{$regex} from Fig.~\ref{fig:regex-tlm-def} is, eliding a few minor details, shown below. The comments describe how the components correspond. 
\begin{lstlisting}
  module RegexNotation = {
    module Notation_regex___relit /* = $regex */ = { 
      type t = Regex.t; /* expansion type */
      module Lexer = RegexLexer; /* lexer module must match ocamllex sig */
      module Parser = RegexParser; /* parser module must match menhir sig */
      module Dependencies = { module Regex = Regex; } /* see text */
      exception Apply(string); /* used in TLM applications, see below */
    } 
  }
\end{lstlisting}

TLM definitions are constrained so that the signature of the generated module is a \emph{singleton}, i.e. that it uniquely determines the module. This implies that the dependencies cannot list a value---only module paths and concrete types can be listed. As such, TLM definition equality is decidable and TLM definitions can be tracked in module types (signatures) without any special effort. Note that this is more restrictive than in the formal system in the previous section, where parse functions were included inline, rather than by path, and dependencies were arbitrary values. It must be possible to resolve the implementation of the lexer and parser modules, not just their signatures, at expansion time, so lexer and module paths are lifted to the top level of the compilation unit.

\paragraph{TLM Application} For a TLM application, the Reason grammar extension produces an expression annotated with \li{relit} that raises the corresponding \li{Apply} exception, supplying a string containing the literal body. For example, a fragment of the example from Fig.~\ref{fig:first-tlm-example} is below:
\begin{lstlisting}
  open RegexNotation; /* ... */
  let restriction_template = (gene) => [%relit raise(Notation_regex___relit.Apply(
      "$(bisA)$(DNA.any_base)*$$(gene)$(DNA.any_base)*$(bisA)"))]
\end{lstlisting}

The Relit preprocessor rewrites TLM applications by following the specification in the previous sections differing only in that the ``dependency variable'' is a ``dependency path'', here to \li{Notation_regex___relit.Dependencies}. The only difficult is in finding the TLM definition at all, which due to our integration into the module system may have been accessed indirectly, e.g. here via the \li{open} directive and in other situations via module synonyms, functors and so on. The trick is in the singleton restriction just described---if we can determine the signature for the module path that \li{Apply} is accessed from, we have the full definition in hand. To do so, we run the OCaml typechecker on this unexpanded representation. Because the TLM application raises an exception, its type can be generalized arbitrarily. As long as unrelated type errors at a particular depth are resolved (treating the TLM applications at that depth as ``holes'') expansion at that depth can proceed. This occasionally causes types that incidentally appear in error messages to be more general than the programmer might expect (e.g. here \li{restriction_template} will have type \li{'a -> 'b} during this ``pre-typing'' phase). 
%If this type ends up in an error message incidentally, e.g. due to a signature mismatch elsewhere, it may be somewhat confusing. 
We are working to improve error reporting and, more generally, improving integration with various other tools in the Reason / OCaml ecosystem.

\paragraph{Proto-Expansion Validation} The typing phase of proto-expansion validation also defers to the OCaml typechecker, with spliced expressions  represented as variables of the corresponding segment type, resolved relative to the typing context determined at the call site. The recursively generated substitutions are, after validation is complete, substituted in the standard capture-avoiding manner.

\paragraph{OCaml Support} Although our efforts are focused on Reason, it would not be difficult to add TLM-related  forms to OCaml's standard grammar, and no changes to the preprocessor just described should be needed. The only caveat is that TLMs that support splicing would then, ideally, also be agnostic to the base language grammar in use. We are exploring various ways for TLM providers to opt in to this, e.g. by defining a version of \li{Relit.read_to} from Fig.~\ref{fig:relit-util} for both grammars.

% The system interprets the 
% lexer and parser names that appear in TLM definitions, e.g. \li{RegexLexer} and \li{RegexParser} in Fig.~\ref{fig:regex-tlm-def}, as if they were module paths at the top level of the compilation unit. The modules must have already been compiled to OCaml bytecode by the time the compiler reaches the TLM definition (because ``compile time'' for a program that uses TLMs is ``run-time'' for the lexer and parser). Concretely, the system will look for \li{RegexLexer.cmo} and \li{RegexParser.cmo} on the build path. The lexer module must have the same interface as modules generated by the \li{ocamllex} tool, and the parser must have the same interface as modules generated by Menhir \cite{menhir}\todo{cite menhir}{}, which is a mature parser generator based on Yacc \cite{johnson1975yacc,TarditiDR:mly}. Reason is itself implemented using Menhir. The most straightforward way to satisfy these requirements is, of course, to use these tools. However, the TLM mechanism never looks at the original lexer specification, i.e. the \li{.mll} file, nor at the grammar itself, i.e. the \li{.mly} file, nor does it invoke these tools directly. This is the job of the build system of the user's choice. There are many popular build systems in the OCaml ecosystem. For the build system bundled with the Reason platform, \li{rebuild}, which is based on \li{ocamlbuild}, the \li{-use-menhir} flag will invoke \li{ocamllex} and Menhir as necessary.


% Note that even when a TLM needs only a single helper function, it must be placed into a named module. This is to avoid needing to reason about the equivalence of arbitrary expressions when determining whether two signatures are compatible. 


% More specifically, the internal representation can be pretty printed as follows, where \li{start_pos} and \li{end_pos} are integer literals corresponding to the start and end position of the provided segment, and \li{ann_ty} is the provided type annotation after the references to the dependencies specified by the TLM have been appropriately resolved.
% \begin{lstlisting}[numbers=none]
%   ((raise ((start_pos, end_pos); failwith "<final expansion here>")) : ann_ty) 
%     [@relit.spliced]
% \end{lstlisting}
% An exception with a type annotation will always have the specified type, so this internal representation allows the system to check the proto-expansion against the expansion type specified by the TLM before moving on to recursively generate the final expansion.


\section{Related Work}
\label{sec:existing-approaches}

In this section, we will give an overview of the many existing mechanisms that library providers might use to define new notation, and more specifically, new literal notation. Rather than evaluating these mechanisms by asking ``how much syntactic power do they give the library provider?'', however, our approach is to ask ``what do these mechanisms quite reasonably \textbf{not} allow a library provider to express?'', using the rubric of six reasoning principles that this paper has developed.

\paragraph{Syntax Definition Systems}\label{sec:syntax-dialects-intro}
One approach available to library providers seeking to introduce new literal forms is to use a syntax definition system to extend the syntax of an existing language directly. % For example, Ur/Web's syntax (Figure 1) is a library-specific dialect of Ur's syntax \cite{conf/pldi/Chlipala10,conf/popl/Chlipala15}.
 % Such dialects are sometimes qualitatively taxonomized as amongst the ``domain-specific language'' for this reason \cite{fowler2010domain}. %Syntactic cost is often assessed qualitatively \cite{lavender1996usability}, though quantitative metrics can be defined. 
% Syntax definition systems , which we will discuss in Sec. \ref{sec:syntax-dialects}, have simplified the task of defining ``library-specific'' (a.k.a. ``domain-specific'') syntax dialects like Ur/Web, and have thereby contributed to their ongoing proliferation.
% Many have argued that a proliferation of syntax dialects constructed using these tools is harmless or even desirable, because programmers can simply choose the right syntax dialect for each job at hand \cite{journals/stp/Ward94}. However, we argue that in fact 
There are hundreds of syntax definition systems of various design, and the parsers generated by these systems can be invoked to preprocess program text in various ways, e.g. by invoking them from within a build script, by using a preprocessor-aware compiler (e.g. \li{ocamlc}), or via language-integrated preprocessing directives, e.g. the import mechanism of SugarJ \cite{erdweg2011sugarj}, or Racket's \li{#lang} directive \cite{Flatt:2012:CLR:2063176.2063195}. %or its reader macros\todo{citation} \cite{Flatt:2012:CLR:2063176.2063195}.% the directives of language-integrated ``mixfix'' systems \cite{wieland2009parsing,missura1997higher,5134} like Coq \cite{Coq:manual}.)
%library-specific (a.k.a. ``domain-specific'') 

These systems give a large amount of syntactic power to the notation provider, but this comes at a cost: with few exceptions, these systems make it difficult to reason abstractly about the six topics from Sec.~\ref{sec:intro}. Rather than reiterating the points made there, let us focus on the exceptional cases.

A grammar extension system that has confronted the problem of \textbf{Responsibility} (but not the other problems) is Copper \cite{conf/pldi/SchwerdfegerW09,schwerdfeger2010context}. Copper performs a modular grammar analysis that guarantees that determinism is conserved (i.e. ambiguities are not possible) when extensions of a certain restricted class are combined. The caveat is that the constituent extensions must prefix all newly introduced forms with marking tokens drawn from disjoint sets. To be confident that the marking tokens used are disjoint, providers must base them on, for example, the domain name system. Because the mechanism operates at the level of the context-free grammar, it is difficult for the client to define scoped abbreviations for these verbose marking tokens. TLMs can, in contrast, be abbreviated and distributed within modules following the usual scoping structure of the language. Composition is via splicing, rather than direct grammar composition.

Some programming languages, notably including theorem provers like Coq \cite{Coq:manual} and Agda \cite{norell2007towards}, support ``mixfix'' notation directives \cite{wieland2009parsing,missura1997higher,5134}. Many of these systems enforce \textbf{Capture Avoidance} and application-site \textbf{Context Independence} \cite{5134,DBLP:conf/gpce/TahaJ03,Coq:manual,DBLP:conf/ifl/DanielssonN08}. The problem is that mixfix notation requires a fixed number of sub-trees, e.g. \li{if _ then _ else _}. Coq has some \emph{ad hoc} extensions for list-like literals \cite{Coq:manual}. These systems cannot express the example literal forms from this paper, because they can have any number of spliced terms.

Racket allows \li{#lang} definitions and reader macros to gain complete control over lexing and parsing the remainder of a file \cite{Flatt:2012:CLR:2063176.2063195}. However, this flexibility makes it difficult to reason about segmentation and even responsibility (''is a reader macro active?'') Providers can optionally generate fresh variables to avoid capture, but this is not enforced. We say more about macros below.

\citet{conf/icfp/LorenzenE13,conf/popl/LorenzenE16}'s SoundExt is a grammar-based syntax extension system where extension providers can equip their new forms with derived typing rules. The system then attempts to automatically verify that the expansion logic, expressed using a rewrite system, rather than an arbitrary function, is sound relative to these derived rules, so it is possible to reason about \textbf{Expansion Typing}. SoundExt does not enforce hygiene, i.e. expansions might depend on the context and intentionally induce capture. A client can only indirectly reason about binding by inspecting the derived typing rules. There is no abstract segmentation discipline. Unlike TLMs, SoundExt supports type-dependent expansions \cite{conf/popl/LorenzenE16}. The trade-off is that TLMs can generate proto-expansions, and therefore segmentations, even when the spliced expressions are ill-typed. Another important distinction is that TLMs rely on proto-expansion validation, rather than verification as in SoundExt (though providers can use MetaOCaml and related systems to reason about typing, as described in Sec. \ref{sec:metaocaml}). The trade-off is that TLMs do not require a fully mechanized host language definition. Finally, there is no clear notion of parameterization or partial application in SoundExt or other syntax definition systems, so it would be difficult to define notation for, for example, all finite map implementations as we demonstrated in Sec.~\ref{sec:ptsms}.



%In other words, encountering an unfamiliar derived form has made it difficult for the programmer to maintain the usual \emph{type discipline} and \emph{binding discipline}. %Compelling the programmer to examine the desugaring directly defeat the purpose of defining the derived form -- decreasing cognitive cost. Indeed, it substantially increases cognitive cost.

% In contrast, when a programmer encounters, for example, a function call like the call to \li{read_data} on Line 3, the analagous questions can be answered by following clear protocols that become ``cognitive reflexes'' after sufficient experience with the language, even if the programmer has no experience with the library defining \li{read_data}:
% \begin{enumerate}
% \item The language's syntax definition determines that \li{read_data(a)} is an expression of function application form.
% \item Similarly, \li{read_data} and \li{a} are definitively expressions of variable form.
% \item The variable \li{a} can only refer to the binding of \li{a} on Line 1.
% \item The variable \li{w} can be renamed without knowing anything about the values that \li{read_data} and \li{a} stand for.
% \item The type of \li{x} can be determined to be \li{B} by determining that the type of \li{read_data} is \li{A -> B} for some \li{A} and \li{B}, and checking that \li{a} has type \li{A}. Nothing else needs to be known about the values that \li{read_data} and \li{a} stand for. In Reynolds' words \cite{B304}:
% \begin{quote}
% \emph{Type structure is a syntactic discipline for enforcing levels of abstraction.}
% \end{quote}
% \end{enumerate}

\paragraph{Term Rewriting Systems}
Another approach -- and the approach that TLMs are rooted in -- is to leave the context-free syntax of the language fixed, and instead contextually rewrite existing literal forms. For example, OCaml's textual syntax now includes \emph{preprocessor extension (PPX) points} used to identify terms that some external term rewriter will rewrite \cite{ocaml-manual}. For example, we could  mark a string literal as follows:
\begin{lstlisting}[numbers=none]
    [%xml "<h1>Hello, {[first_name]}!</h1>"]
\end{lstlisting}
This does help with reasoning about responsibility, but technically more than one applied preprocessor might recognize this annotation (there are, in practice, many XML/HTML libraries), and annotations do not follow scoping rules. It is impossible to reason abstractly about the other issues because the code that the preprocessor generates is unconstrained.% For these reasons, users of this mechanism warn that they should be used sparingly.\todo{citation}

Term-rewriting macro systems require that the client explicitly apply the intended rewriting, implemented by a scoped macro, to the term that is to be rewritten. This addresses the issue of \textbf{Responsibility} more thoroughly. However, unhygienic, untyped macro systems, like the earliest variants of the Lisp macro system \cite{Hart63a}, and, more recently, Template Haskell \cite{SheardPeytonJones:Haskell-02} and GHC's quotation system \cite{mainland2007s} (which is based on Template Haskell), do not allow clients to reason abstractly about the remaining issues, again because the expansion that they produce is unconstrained. There is typically some way for library providers to opt in to a capture avoidance discipline by asking for fresh variables, but this is not enforced. Note that it is not enough that with Template Haskell / GHC quotation, the generated expansion is typechecked---to satisfy the \textbf{Expansion Typing} criterion, it must be possible to reason abstractly about \emph{what the type of the generated expansion is}. 

\emph{Hygienic} macro systems prevent, or abstractly account for \cite{DBLP:conf/esop/HermanW08,Herman10:Theory}, \textbf{Capture}, and they enforce application-site \textbf{Context Independence} \cite{Kohlbecker86a,DBLP:conf/popl/Adams15,DBLP:conf/popl/ClingerR91,DBLP:journals/lisp/DybvigHB92}. The critical problem, detailed in Sec.~\ref{sec:context-dependence}, is that the standard context independence discipline makes it impossible to repurpose string literal forms to introduce compositional literal forms at other types. 

TLMs also consider the problem of definition-site context independence by specifying expansion dependencies explicitly. This sidesteps problems faced in prior attempts to integrate macros into module systems \cite{culpepper2005syntactic} related to ``smuggling'' definition site values to application sites, without violating the abstraction discipline of the module system. It also prevents issues related to cross-stage persistence, since macro definitions do not refer to definition-site values directly but rather program against the dependency signature.

Much of the research on macro systems has been for languages in the LISP tradition \cite{mccarthy1978history} that do not have rich static type structure. The formal macro calculus studied by \citet{DBLP:conf/esop/HermanW08} (which is not capable of expressing new literal forms, for the reason just discussed) uses types only as a technical advice in reasoning about the binding structure of the generated expansion. The Scala macro system does support reasoning abstractly about \textbf{Expansion Typing} due to the return type annotations. The full calculus we have defined is the first detailed type-theoretic account of a typed, hygienic macro system of any design for an ML- or Scala-like language, i.e. one with a rich static type structure and support for pattern matching. Many of the basic mechanisms would be relevant even if support for parsing literal bodies was removed and replaced with term rewriting. Segmentations can be considered a refinement of the tree paths in prior work \cite{Herman10:Theory,gorn1965explicit}.

Research on typed \emph{staging macro systems} \cite{DBLP:conf/popl/DaviesP96} like MetaML \cite{Sheard:1999:UMS}, MetaOCaml \cite{DBLP:conf/flops/Kiselyov14}, MacroML \cite{ganz2001macros,DBLP:conf/gpce/TahaJ03} and Typed Template Haskell \cite{tth} is not directly applicable to the problem of defining new literal forms because the syntax tree of the arguments cannot be inspected at all (staging macros are used mainly for optimization). Sec.~\ref{sec:metaocaml} discussed how the typed quotations from these systems can help TLM providers reason about the type-correctness of their parsers. Future work could help address some of the mentioned limitations.


  Some languages, including Scala \cite{odersky2008programming}, build in \emph{string splicing} (a.k.a. \emph{string interpolation}) forms, or similar but more general \emph{fragmentary quotation forms} \cite{conf/icfp/Slind91}, e.g. SML/NJ \cite{SML/Quote}. These designate a particular delimiter to escape out into the expression language. The problem with using these together with macros as vehicles to introduce literal forms at various other types is 1) there is no ``one-size-fits-all'' escape delimiter, and 2) typing is problematic because every escaped term is checked against the same type. For example, in Fig.~\ref{fig:urweb}, we have splicing at two different types. %These forms also cannot appear in patterns. Scala's pattern matching machinery incorporates a dynamic metaobject protocol that can approximate pattern literal notation in some cases, though this makes it difficult to reason about exhaustiveness and redundancy. %In general, e.g. when defining syntax for a programming language with many sorts of terms, the most appropriate choice of delimiters might depend on where each spliced term appears.


This brings us to the most closely related work, that of \citet{TSLs} on \emph{type-specific languages} (TSLs). Like simple expression TLMs (Sec. \ref{sec:setlms}), TSLs allow library providers to programmatically control the parsing of expressions of generalized literal forms. With TSLs, parse functions are associated directly with nominal types and invoked according to a bidirectionally typed protocol. In contrast, TLMs are separately defined and explicitly applied. Accordingly, different TLMs can operate at the same type, and they can operate at any type, including structural types.  In a subsequent short paper, \citet{sac15} suggested explicit application of simple expression TLMs  also in a bidirectional typed setting \cite{Pierce:2000:LTI:345099.345100}, but this paper did not have any formal content. With TLMs, it is not necessary for the language to be bidirectionally typed (see Sec. \ref{sec:segment-typing} on ML-style type inference). % It should be possible to implicitly invoke TLMs based on the expected type in situations where the inference engine had enough information, but we leave this as future work. 
The metatheory presented by \citet{TSLs} establishes only that generated expansions are of the expected type (i.e. a variant of the Typed Expression Expansion theorem from Sec. \ref{sec:s-metatheory}), though the induction principle used in the proof is not clear. It does not establish the remaining abstract reasoning principles that have been the major focus of this paper. There appears to be a context independence condition but it does not allow for any dependencies whatsoever, which is unreasonably restrictive. There is no formal hygiene theorem and indeed the formal system in the paper does not correctly handle substitution or capture avoidance, issues we emphasized because they were non-obvious in Sec. 5. Moreover, the TLM does not guarantee that a valid segmentation will exist, nor associate types with spliced segments. Finally, this prior work did not consider pattern matching, module system integration or type- or module-parameterized type families. This paper addresses all of these, resulting in a design suitable for integration into Reason/OCaml.

% These existing forms normally have other meanings, so this can be confusing \cite{pane1996usability}.

\section{Discussion}
\label{sec:discussion}
\label{sec:conclusion}

The importance of specialized notation as a ``tool for thought'' has long been recognized \cite{DBLP:journals/cacm/Iverson80}. According to Whitehead, a good notation ``relieves the brain of unnecessary work'' and ``sets it free to concentrate on more advanced problems'' \cite{cajori1928history}, and indeed, advances in mathematics, science and programming have often been accompanied by new notation. 

Of course, this desire to ``relieve the brain of unnecessary work'' has motivated not only the syntax but also the semantics of languages like ML and Scala -- these languages maintain a strong type and binding discipline so that programmers, and their tools, can hold certain implementation details abstract when reasoning about program behavior.  In the words of \citet{B304}, ``type structure is a syntactic discipline for enforcing levels of abstraction.''

Previously, these two relief mechanisms were in tension---mechanisms  that allowed programmers to express new notation would obscure the type and binding structure of the program text. TLMs resolve this tension for the broad class of literal forms that generalized literal forms subsume. This class includes all of the examples enumerated in Sec. \ref{sec:intro} (up to the choice of outermost delimiter), the varied and non-trivial case studies outlined in this paper and in the supplement, and the examples collected from the empirical study by \citet{TSLs}. We anticipate many more interesting examples will emerge as the Reason community explores the mechanism.

Of course, not all possible literal notation will prove to be in good taste. %TLMs leave the burden of establishing the value of any particular literal notation to individual library providers, rather than to the language designer. 
The reasoning principles that TLMs provide, which are the primary contributions of this paper, allow clients to "reason around" poor literal designs, using principles analagous to those already familiar to programmers in languages like ML and Scala. Although we emphasized integration with modules, our formal system demonstrates that modules are not necessary to capture the fundamental ideas in this paper. We intend this paper to be useful to the designers of languages far from the ML tradition. More generally, we intend for the ``reasoning principles first'' approach advocated in this paper to be a useful intellectual framework for language designers considering the merits of other ``conveniences''. % We must leave a detailed empirical evaluation of the impact of particular TLMs on various quality attributes, like programmer productivity and program comprehensibility, as future work. 
% Our own implementation efforts have focused on an emerging alternative front-end for OCaml called Reason, but Scala supports analagous features \cite{conf/oopsla/AminRO14} and would also be a suitable host for the TLM mechanism. The intended audience for this paper is language designers seeking a reasonable solution to the problem of user-defined literal notation.% We also plan to implement TLMs into \li{typy}, a typed functional language embedded into Python as a library \cite{gpce/Omar16}.%We have evaluated this claim by stating the available abstract reasoning principles formally and proving that they hold for the calculi that we have described.

% The examples in this paper are written in an open source alternative syntactic front-end for OCaml called Reason because we are incorporating TLMs into Reason. However, the essential ideas are not Reason-, OCaml- or ML-specific -- it should be possible to adapt TLMs to other languages that take a similar approach to types and binding (e.g. Haskell, Scala and others.) Languages that have a disciplined binding structure but that lack a rich static type structure, e.g. standard Racket, would also benefit from TLMs -- they can apply these results by deploying the usual trick of viewing the language as statically ``{unityped}'' \cite{pfpl,scott1980lambda}.

% The intended audience for this paper is language designers who want to decentralize control over literal forms but seek a detailed understanding, rooted in the first principles of type theory, of a reasonable mechanism by which to achieve that goal. 

%(To summarize Sec. \ref{sec:existing-approaches}, the closest existing typed, hygienic macro system -- the Scala macro system \cite{ScalaMacros2013} -- is not formally specified, the work of \cite{DBLP:conf/esop/HermanW08} uses types only to reason abstractly about binding, and the specification of TSLs in the work of \citet{TSLs} did not formalize hygiene nor handle these advanced language features.)

% his limitation could be resolved by allowing identifiers that appear in literal bodies to be explicitly designated as bound within spliced expressions in the segmentation. The prior work by \citet{DBLP:conf/esop/HermanW08} considered a similar approach in a setting where the locations of sub-terms is known ahead of time (building on the \emph{tree locations} of \citet{gorn1965explicit}, which are conceptually similar to our spliced segment locations.)  We leave as future work the details of this proposal and a consideration of whether this additional expressive power would justify the increased reasoning complexity for clients (it is presently the author's opinion that it would not.)  

\paragraph{Limitations} Not all interesting properties of a program will, in general, be apparent from its type, particularly in a language like OCaml. As usual, programmers will sometimes need to peek behind the abstraction boundaries or rely on informal documentation to understand, in detail, what a literal notation is doing (e.g. with respect its equational properties, its side effects and so on). TLMs in languages with more expressive type systems would be commensurately more reasonable.

Notation that intentionally introduces bindings into spliced terms, like Haskell's \li{do}-notation at types equipped with monadic structure, cannot be expressed using TLMs as defined in this paper, because splicing is capture avoiding. Although we considered weakening the capture avoidance condition in various somewhat reasonable ways, e.g. by communicating which identifiers are explicitly captured by each spliced segment, it is quite difficult to communicate this structure to client programmers even with tool support. Given that \li{do}-notation is already general and comes equipped with well-understood reasoning principles, the most reasonable approach is perhaps to  build it in primitively, suitably parameterized over the implementation of the \li{MONAD} signature.




% A correct parse function never returns an encoding of a proto-expansion that fails validation given well-typed splices, but this invariant cannot be enforced by the ML type system.
% % Using a proof assistant, it would be possible to verify that
% % a parse function generates only encodings of valid proto-expansions. 
% Under a
% richer type system, the return type of the parse function itself could be refined so as to
% enforce this invariant intrinsically. This problem -- of typed first-class typed term representations -- has been studied in a variety of settings, e.g. in MetaML \cite{Sheard:1999:UMS} and in the modal logic tradition \cite{DBLP:conf/popl/DaviesP96}. % We leave integration of these approaches into the TLM mechanism as future work.
%  Our present efforts aim to leave the semantics of OCaml unchanged.

%Another topic of interest has to do with intentional capture. 
Another future direction has to do with automated refactoring. The unexpanded language does not come with context-free notions of renaming and substitution. However, given a segmentation, it should be possible to ``backpatch'' refactorings into literal bodies. Recent work by \citet{wand2017inferring} on tracking bindings ``backwards'' from an expansion to the source program is likely relevant. The challenge is that the TLM's splicing logic might not be invariant to refactorings.

At several points in the paper, we allude to editor integration. However, several important questions having to do with TLM-specific syntax highlighting, incremental parsing and error recovery \cite{graham1979practical} remain to be considered. Another interesting direction would be to generalize TLMs to support non-textual notation in the setting of a structure editor \cite{DBLP:conf/popl/OmarVHAH17}.

% Let us conclude by considering the oft-uttered phrase ``syntax doesn't matter'', which is generally taken to mean that syntactic concerns are orthogonal to various more important semantic concerns. Our hope is that the reader will leave this paper with an understanding that this is a rather simplistic proclamation, because semantic concerns are relevant to notation design as well! % We hope that this work will lead to more ``reasonable'' variants of other syntactic conveniences, e.g. the ubiquitious ``deriving'' clauses on datatype definitions.

% Other directions for future work are given in the supplement\todo{do this?}.

% Another interesting question is what this mechanism would look like in the setting of a projectional structure editor, i.e. one where the syntax is not textual but rather tree-shaped and projected in an interative manner to the programmer. This paper taken together with work by \citet{Omar:2012:ACC:2337223.2337324,DBLP:conf/popl/OmarVHAH17} can serve to guide such an effort.

% To conclude, TLMs give substantial syntactic control to library providers while leaving programmers with strong abstract reasoning principles. We believe TLMs therefore occupy a ``sweet spot'' in the  design space. %This paper serves to guide those who hope to incorporate TLMs into their future language designs.

% \newcommand{\concSec}{Conclusion}
% \section{\protect\concSec}
% \label{sec:conc}

\paragraph{Concluding Remarks} 
To conclude, let us briefly reiterate the key ideas of this paper. The focus was on ensuring that client programmers can follow simple protocols when they have questions about the syntactic structure, type structure or binding structure of a program. In answering these sorts of questions,  the client is not made to look at the generated expansion itself, nor inspect the parser implementation. Instead, the programmer need only be given knowledge of the expansion type from the TLM definition and the segmentation inferred by the expander at each application site, which carries a small volume of information that can be communicated straightforwardly by standard editor services. Certain questions related to the binding structure simply do not need to be asked due to the strict context independence and capture avoidance discipline of the system, enabled by the explicit tracking of dependencies, and of spliced segments. 

Despite these semantic constraints, the system is able to express a number of non-trivial examples with few compromises because there is only one simple lexical constraint on literal bodies. The mechanism integrates cleanly into Reason/OCaml, with full support for its type system, module system and pattern matching system. TLM providers can use existing, mature parser generators and parse tree representations, and can opt-in to a stronger typing discipline by using various implementations of MetaOCaml. Overall, we believe that TLMs represent a distinctly \emph{reasonable} and \emph{expressive} new point in the design space of literal notation definition systems.


% Unhygienic term rewriting systems like OCaml's PPX system and Template Haskell can be used to rewrite string literals, but these systems also do no adequately address any of these reasoning criteria. Hygienic macro systems, like those available in various Lisp-family languages and in Scala, are also unsuitable because the hygiene constraints prevent the macros from surfacing spliced expressions from literal bodies. The system of \emph{type-specific languages} (TSLs) developed in prior work by \citet{TSLs} has made perhaps the most headway toward these ideals, but it too does not adequately maintain these reasoning criteria. Moreover, it is not clear how to integrate this approach, which was developed for a simple monomorphic language with nominal types and local type inference, into ML, which supports structural types, parameterized types, abstract types, modules, pattern matching and non-local type inference. We will say more about these existing approaches in Sec. \ref{sec:existing-approaches}. For now, it suffices to say that after evaluating existing approaches, we found that none of them were suitable for integration into Reason.

% An approach that has made some headway toward this ideal is Omar et al.'s \emph{type-specific languages} (TSLs) \cite{TSLs}. A language that supports TSLs delegates control over the parsing and expansion of certain \emph{generalized literal forms} to a parser associated with the type that the form is being checked against. So if we add TSLs to our language, our example then requires a type annotation on \li{y}:
% \begin{lstlisting}[numbers=none,basicstyle=\ttfamily\fontsize{9pt}{1em}\selectfont]
%   let y : KQuery.t = `((!R)@&{&/x!/:2_!x}'!R)`
% \end{lstlisting}
% The parser associated with the type \li{KQuery.t} is statically invoked to lex, parse and expand the literal body, i.e. the sequence of characters between the outer delimiters, here \li{`(} and \li{)`}. The literal body is constrained by the context-free grammar of the language only in that nested delimiters must be matched (like comments in OCaml). TSLs are closely related to the mechanism that we will propose, so let us analyze TSLs from the perspective of the six reasoning criteria just outlined.
% \begin{enumerate}[leftmargin=12px]
%   \item[\LEFTcircle] \textbf{Responsibility}: The type-directed dispatch mechanism allows the client to easily determine which parser is responsible for each literal form: the parser that was associated with the type when it was defined. Clients do not need to worry about different TSLs conflicting syntactically because the context-free grammar of the language remains fixed and composition is mediated by splicing. The main limitation here is that it is impossible to define literal notation after a type has been defined, and it is also impossible to define multiple notations at a single type.
%   \item[\LEFTcircle] \textbf{Segmentation}: The parser can splice expressions out of the literal body, but there is no clear way to associate each spliced term with some particular segment (i.e. subsequence) of the literal body, nor is there a guarantee that spliced terms are non-overlapping. As such, there is no way to indicate to the programmer where base language expressions are located within a literal form.
%   \item[\LEFTcircle] \textbf{Capture}: The TSL mechanism as described enforces complete capture avoidance (though the formal specification given in the paper did not adequately enforce this constraint).
%   \item[\LEFTcircle] \textbf{Context Dependence}: The TSL mechanism as described requires that the expansion be completely closed, so it is trivially context independent. However, this comes at a significant expressive cost: the generated expansions cannot make use of any libraries whatsoever.
%   \item[\LEFTcircle] \textbf{Typing}: The expansion must necessarily be of the associated type, so abstract reasoning about the type of the expansion is straightforward. However, there is no easy way to determine the type expected for each spliced expression. In addition, the prior work considered only a monomorphic, nominally-typed language with local type inference, leaving open a number of problems that came up as we considered integrating TSLs into Reason/OCaml:
% \begin{enumerate}[leftmargin=15px,nolistsep,noitemsep]
%   \item \textbf{Structural Types}: There is no way to define literal notation at structural types, e.g. tuple and arrow types, because there is no ``definition site'' for such types.
%   \item \textbf{Parameterized \& Abstract Types}: There is no way to define literal notation over a type-parameterized family of types, e.g. at all types \li{list('a)}. Similarly, there is no way to define literal notation over a module-parameterized family of abstract types, e.g. at every abstract type defined by a module implementing the \li{QUEUE} signature. Parameterized and abstract type families are ubiquitous in ML-family languages.% Programmers in ML-family languages commonly define various implementations of abstract data types in this way.
%   \item \textbf{Pattern Literals}: Pattern matching is ubiquitous in ML-family languages but the prior work on TSLs considered only expression literals. %Pattern literals are dual to expression literals in that expression literals support value construction, while pattern literals support value deconstruction.
%   \item \textbf{ML-Style Type Inference}: It is not clear that a type-directed dispatch scheme can be cleanly  reconciled with ML-style type inference.
% \end{enumerate}
% \end{enumerate}


%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
   This work was
supported in part by AFRL and DARPA under agreement \#FA8750-16-2-0042;
 by the NSA under lablet contract \#H98230-14-C-0140; and by Ravi Chugh
 at the University of Chicago. The authors
would like to thank Robert Harper, Karl Crary and Eric Van Wyk for 
valuable feedback on the first author's doctoral dissertation, which forms the basis of this work; 
Charles Chamberlain, 
for his crucial contributions to the implementation and for valuable feedback on the
paper; and members of the PL Reading Group at University of Chicago, and the anonymous referees
at ICFP and at prior conferences, for their valuable critiques, which substantially improved the paper.

  % This material is based upon work supported by the
  % \grantsponsor{GS100000001}{National Science
  %   Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  % No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  % No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  % conclusions or recommendations expressed in this material are those
  % of the author and do not necessarily reflect the views of the
  % National Science Foundation.
\end{acks}

\clearpage
%% Bibliography
\bibliography{../../../papers/research}

\begin{comment}\appendix
\subsection{Parametric TLMs, Formally}
\vspace{-2px}
% \begin{figure}[p]
% \[\begin{array}{llcl}
% \mathsf{UMType} & \urho & ::= 
% %& \autype{\utau} 
% & \utau & \text{type annotation}\\
% % &&
% %& \aualltypes{\ut}{\urho} 
% % & \alltypes{\ut}{\urho} & \text{type parameterization}\\
% &&
% %& \auallmods{\usigma}{\uX}{\urho} 
% & \allmods{\uX}{\usigma}{\urho} & \text{module parameterization}\\
% \mathsf{UMExp} & \uepsilon & ::= 
% %& \abindref{\tsmv} 
% & \tsmv & \text{TLM identifier reference}\\
% % &&
% %& \auabstype{\ut}{\uepsilon} 
% % & \abstype{\ut}{\uepsilon} & \text{type abstraction}\\
% &&
% %& \auabsmod{\usigma}{\uX}{\uepsilon} 
% & \absmod{\uX}{\usigma}{\uepsilon} & \text{module abstraction}\\
% % &&
% %& \auaptype{\utau}{\uepsilon} 
% % & \aptype{\uepsilon}{\utau} & \text{type application}\\
% &&
% %& \auapmod{\uM}{\uepsilon} 
% & \apmod{\uepsilon}{\uX} & \text{module application}\ECC
% \end{array}
% \]
% \caption{Syntax of unexpanded TLM types and expressions.}
% \label{fig:P-macro-expressions-types-u}
% \end{figure}

We will now outline $\miniVerseParam$, a calculus that extends $\miniVersePat$ with parametric TLMs. This calculus is organized, like $\miniVersePat$, as an unexpanded language (UL) defined by typed expansion to an expanded language (XL). There is not enough space to describe $\miniVerseParam$ with the same level of detail as in Sec. \ref{sec:setlms-formally}, so we highlight only the most important concepts below. The details are in the supplement.

The XL consists of 1) module expressions, $M$, classified by signatures, $\sigma$; 2) constructions, $c$, classified by kinds, $\kappa$; and 3) expressions classified by types, which are constructions of kind $\akty$ (we use metavariables $\tau$ instead of $c$ for types by convention.) Metavariables $X$ ranges over module variables and $u$ or $t$ over construction variables. The module and construction languages are based closely on those defined by \citet{pfple1}, which in turn are based on early work by \citet{MacQueen:1984:MSM:800055.802036,DBLP:conf/popl/MacQueen86}, subsequent work on the phase splitting interpretation of modules \cite{harper1989higher} and on using dependent singleton kinds to track type identity \cite{stone2006extensional,DBLP:conf/lfmtp/Crary09}, and finally on formal developments by \citet{dreyer2005understanding} and \citet{conf/popl/LeeCH07}. A complete account of these developments is unfortunately beyond the scope of this paper. The expression language extends the language of $\miniVersePat$ only to allow projection out of modules.

The main conceptual difference between $\miniVersePat$ and $\miniVerseParam$ is that $\miniVerseParam$ introduces the notion of unexpanded and expanded TLM expressions and types, as shown in Figure \ref{fig:P-macro-expressions-types}. 

\begin{figure}[h]
\vspace{-6px}
\small
\begin{minipage}{0.38\textwidth}
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{UMType} & \urho & ::= & \utau ~\vert~ \allmods{\uX}{\usigma}{\urho}\\
\mathsf{UMExp} & \uepsilon & ::= & \tsmv ~\vert~ \absmod{\uX}{\usigma}{\uepsilon}~\vert~ \apmod{\uepsilon}{\uX}
\end{array}$
\end{minipage}
\begin{minipage}{0.6\textwidth}
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{MType} & \rho & ::= & \aetype{\tau} ~\vert~ \aeallmods{\sigma}{X}{\rho} \\
\mathsf{MExp} & \epsilon & ::= & \adefref{a} ~\vert~ \aeabsmod{\sigma}{X}{\epsilon} ~\vert~\aeapmod{X}{\epsilon} 
\end{array}$
\end{minipage}
\vspace{-10px}
\caption{Syntax of unexpanded and expanded TLM types and expressions in $\miniVerseParam$}
\label{fig:P-macro-expressions-types}
\vspace{-10px}
\end{figure}
The TLM type $\aeallmods{\sigma}{X}{\rho}$ classifies TLM expressions that have one module parameter matching $\sigma$. For simplicity, we formalize only module parameters. Type parameters can be expressed as module parameters having exactly one abstract type member.

The rule governing expression TLM application, reproduced below, touches all of the main ideas in $\miniVerseParam$, so we will refer to it throughout the remainder of this section.%It might be useful to compare this rule to Rule \textsc{ee-ap-sptsm}.
{\small\begin{mathpar}
\inferrule[ee-ap-petsm]{
  \uOmega = \uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}\\
  \uPsi=\uAS{\uA}{\Psi}\\\\
  \tsmexpExpandsExp{\uOmega}{\uPsi}{\uepsilon}{\epsilon}{\aetype{\tau_\text{final}}}\\
  \tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}\\\\
  \tsmdefof{\epsilon_\text{normal}}=a\\
  \Psi = \Psi', \petsmdefn{a}{\rho}{\eparse}\\\\
  \encodeBody{b}{\ebody}\\
  \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{e_\text{pproto}}}\\
  \decodePCEExp{e_\text{pproto}}{\pce}\\\\
  \prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon_\text{normal}}{\aetype{\tau_\text{proto}}}{\omega}{\Omega_\text{params}}\\\\
  \segOK{\segof{\ce}}{b}\\
  \cvalidEP{\Omega_\text{params}}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\ce}{e}{\tau_\text{proto}}
}{
  \expandsP{\uOmega}{\uPsi}{\uPhi}{\utsmap{\uepsilon}{b}}{[\omega]e}{[\omega]\tau_\text{proto}}
}
\end{mathpar}}

The first two premises simply deconstruct the (unified) unexpanded context $\uOmega$ (which tracks the expansion of expression, constructor and module identifiers, as $\uDelta$ and $\uGamma$ did in $\miniVersePat$) and peTLM context, $\uPsi$. Next, we expand $\uepsilon$ according to straightforward unexpanded peTLM expression expansion rules. The resulting TLM expression, $\epsilon$, must be defined at a type (i.e. no quantification over modules must remain once the literal body is encountered.)

The fourth premise performs \emph{peTLM expression normalization}, $\tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}$. This is defined in terms of a structural operational semantics \cite{DBLP:journals/jlp/Plotkin04a} with two stepping rules:
% \begin{equation*}\tag{\ref{rule:tsmexpEvalsExp}}
% \inferrule{
%   \tsmexpMultistepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}\\
%   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon'}
% }{
%   \tsmexpEvalsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
% }
% \end{equation*}
% where the multistep judgement, $\tsmexpMultistepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}$, is defined as the reflexive, transitive closure of the stepping judgement defined by the following rules:
% \begin{equation*}\tag{\ref{rule:tsmexpStepsExp-aptype-1}}
% \inferrule{
%   \tsmexpStepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
% }{
%   \tsmexpStepsExp{\Omega}{\Psi}{\aeaptype{\tau}{\epsilon}}{\aeaptype{\tau}{\epsilon'}}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpStepsExp-aptype-2}}
% \inferrule{ }{
%   \tsmexpStepsExp{\Omega}{\Psi}{\aeaptype{\tau}{\aeabstype{t}{\epsilon}}}{[\tau/t]\epsilon}
% }
% \end{equation*}
{\small\begin{mathpar}
\inferrule[eps-dyn-apmod-subst-e]{ }{
  \tsmexpStepsExp{\Omega}{\Psi}{\aeapmod{X}{\aeabsmod{\sigma}{X'}{\epsilon}}}{[X/X']\epsilon}
}

\inferrule[eps-dyn-apmod-steps-e]{
  \tsmexpStepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
}{
  \tsmexpStepsExp{\Omega}{\Psi}{\aeapmod{X}{\epsilon}}{\aeapmod{X}{\epsilon'}}
}
\end{mathpar}}
% The peTLM expression normal forms are defined as follows:
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-defref}}
% \inferrule{ }{
%   \tsmexpNormalExp{\Omega}{\Psi, \petsmdefn{a}{\rho}{\eparse}}{\adefref{a}}
% }
% \end{equation*}
% % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-abstype}}
% % \inferrule{ }{
% %   \tsmexpNormalExp{\Omega}{\Psi}{\aeabstype{t}{\epsilon}}
% % }
% % \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-absmod}}
% \inferrule{ }{
%   \tsmexpNormalExp{\Omega}{\Psi}{\aeabsmod{\sigma}{X}{\epsilon}}
% }
% \end{equation*}
% % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-aptype}}
% % \inferrule{
% %   \epsilon \neq \aeabstype{t}{\epsilon'}\\
% %   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon}
% % }{
% %   \tsmexpNormalExp{\Omega}{\Psi}{\aeaptype{\tau}{\epsilon}}
% % }
% % \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-apmod}}
% \inferrule{
%   \epsilon \neq \aeabsmod{\sigma}{X'}{\epsilon'}\\
%   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon}
% }{
%   \tsmexpNormalExp{\Omega}{\Psi}{\aeapmod{X}{\epsilon}}
% }
% \end{equation*}
\vspace{-5px}

Normalization eliminates parameters introduced in higher-order abbreviations, leaving only those parameter applications specified by the original TLM definition. Normal forms and progress and preservation theorems are established in the supplement.

The third row of premises looks up the applied TLM's definition by invoking a simple metafunction to extract its name, $a$, then looking up $a$ within the peTLM definition context, $\Psi$.
% \begin{align}
% \tsmdefof{\adefref{a}} & = a \tag{\ref{eqn:tsmdefof-adefref}}\\
% % \tsmdefof{\aeabstype{t}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-abstype}}\\
% \tsmdefof{\aeabsmod{\sigma}{X}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-absmod}}\\
% % \tsmdefof{\aeaptype{\tau}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-aptype}}\\
% \tsmdefof{\aeapmod{X}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-apmod}}
% \end{align}


The fourth row of premises 1) encodes the body as a value of the type $\tBody$; 2) applies the parse function; and 3) decodes the result, producing a \emph{parameterized proto-expression}, $\pce$. Parameterized proto-expressions, $\pce$, are ABTs that serve simply to introduce the parameter bindings into an underlying proto-expression, $\ce$. The syntax of parameterized proto-expressions is given below.

\vspace{-4px}{\small\[\begin{array}{llcl}
% \textbf{Sort} & & & \textbf{Operational Form} & \textbf{Stylized Form} & \textbf{Description}\\
% \LCC \color{Yellow}&\color{Yellow}&\color{Yellow}& \color{Yellow} & \color{Yellow} & \color{Yellow}\\
\mathsf{PPrExp} & \pce & ::= & \apceexp{\ce} ~\vert~ \apcebindmod{X}{\pce}
\end{array}\]}%
\vspace{-6px}%

There must be one binder in $\pce$ for each TLM parameter specified by $a$. (In Reason, we can insert these binders automatically as a convenience.) 

The judgement on the fifth row of Rule \textsc{ee-ap-petsm} then \emph{deparameterizes} $\pce$ by peeling away these binders to produce 1) the underlying proto-expression, $\ce$, with the variables that stand for the parameters free; 2) a corresponding deparameterized type, $\tau_\text{proto}$, that uses the same free variables to stand for the parameters; 3) a \emph{substitution}, $\omega$, that pairs the applied parameters from $\epsilon_\text{normal}$ with the corresponding variables generated when peeling away the binders in $\pce$; and 4) a corresponding \emph{parameter context}, $\Omega_\text{params}$, that tracks the signatures of these variables. The two rules governing the proto-expression deparameterization judgement are below:
{\small\begin{mathpar}
\inferrule{ }{
  \prepce{\Omega_\text{app}}{\Psi, \petsmdefn{a}{\rho}{\eparse}}{\apceexp{\ce}}{\ce}{\adefref{a}}{\rho}{\emptyset}{\emptyset}
}

\vspace{-4px}\inferrule{
  \prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon}{\aeallmods{\sigma}{X}{\rho}}{\omega}{\Omega}\\
  X \notin \domof{\Omega_\text{app}}
}{
  \prepce{\Omega_\text{app}}{\Psi}{\apcebindmod{X}{\pce}}{\ce}{\aeapmod{X'}{\epsilon}}{\rho}{(\omega, X'/X)}{(\Omega, X : \sigma)}
}
\end{mathpar}}%
This judgement can be pronounced ``when applying peTLM $\epsilon$, $\pce$ has deparameterization $\ce$ leaving $\rho$ with parameter substitution $\omega$''. 
Notice based on the second rule that every module binding in $\pce$ must pair with a corresponding module parameter application. Moreover, the variables standing for parameters must not appear in $\Omega_\text{app}$, i.e. $\domof{\Omega_\text{params}}$ must be disjoint from $\domof{\Omega_\text{app}}$ (this requirement can always be discharged by alpha-variation.)

The final row of premises checks that the segmentation of $\ce$ is valid and  performs proto-expansion validation under the parameter context, $\Omega_\text{param}$ (rather than the empty context, as was the case in $\miniVersePat$.) The conclusion of the rule applies the parameter substitution, $\omega$, to the resulting expression and the deparameterized type.

Proto-expansion validation operates conceptually as in $\miniVersePat$. The only subtlety has to do with the type annotations on references to spliced terms. As described at the end of Sec. \ref{sec:ptsms-by-example}, these annotations might refer to the parameters, so the parameter substitution, $\omega$, which is tracked by the splicing scene, must be applied to the type annotation before proceeding recursively to expand the referenced unexpanded term. However, the spliced term itself must treat parameters parametrically, so the substitution is not applied in the conclusion of the following rule:
\begin{mathpar}\label{rule:cvalidE-P-splicede}
\inferrule{
  \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\
    \cvalidC{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\ctau}{\tau}{\akty}\\
  \expandsP{\uOmega}{\uPsi}{\uPhi}{\ue}{e}{[\omega]\tau}\\\\
  \uOmega=\uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}\\
  \domof{\Omega} \cap \domof{\Omega_\text{app}} = \emptyset
}{
  \cvalidEP{\Omega}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\acesplicede{m}{n}{\ctau}}{e}{\tau}
}
\end{mathpar}
(This is only sensible because we maintain the invariant that $\Omega$ is always an extension of $\Omega_\text{params}$.)

The calculus enjoys metatheoretic properties analagous to those described in Sec. \ref{sec:s-metatheory}, modified to account for the presence of modules, kinds and parameterization. The following theorem establishes the abstract reasoning principles available when applying a parametric expression TLM. The clauses are directly analagous to those of Theorem \ref{thm:setlm-reasoning}, so for reasons of space we do not repeat the inline descriptions. The \textbf{Kinding} clauses can be understood by analogy to the \textbf{Typing} clauses. The details of parametric pattern TLMs (ppTLMs) are analagous (see supplement.)
\vspace{-3px}
\begin{theorem}[peTLM Reasoning Principles]
If $\expandsP{\uOmega}{\uPsi}{\uPhi}{\utsmap{\uepsilon}{b}}{e}{\tau}$ then:
\begin{enumerate}[nolistsep,noitemsep]
  \item $\uOmega=\uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}$
  \item $\uPsi=\uAS{\uA}{\Psi}$
  \item (\textbf{Typing 1}) $\tsmexpExpandsExp{\uOmega}{\uPsi}{\uepsilon}{\epsilon}{\aetype{\tau}}$ and $\hastypeP{\Omega_\text{app}}{e}{\tau}$
  \item $\tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}$
  \item $\tsmdefof{\epsilon_\text{normal}}=a$
  \item $\Psi = \Psi', \petsmdefn{a}{\rho}{\eparse}$
  \item $\encodeBody{b}{\ebody}$
    \item $\evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{e_\text{pproto}}}$
  \item $\decodePCEExp{e_\text{pproto}}{\pce}$
  \item $\prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon_\text{normal}}{\aetype{\tau_\text{proto}}}{\omega}{\Omega_\text{params}}$
  \item (\textbf{Segmentation}) $\segOK{\segof{\ce}}{b}$
  \item $\cvalidEP{\Omega_\text{params}}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\ce}{e'}{\tau_\text{proto}}$
  \item $e = [\omega]e'$
  \item $\tau = [\omega]\tau_\text{proto}$
  \item $
    \segofexisitng-{\ce} = \sseq{\acesplicedk{m_i}{n_i}}{\nkind} \cup \sseq{\acesplicedc{m'_i}{n'_i}{\cekappa'_i}}{\ncon} \cup $ \\
     ~~~~$          \sseq{\acesplicede{m''_i}{n''_i}{\ctau_i}}{\nexp}
    $
  \item (\textbf{Kinding 1}) $\sseq{\kExpands{\uOmega}{\parseUKindF{\bsubseq{b}{m_i}{n_i}}}{\kappa_i}}{\nkind}$ and \\ ~~~~$\sseq{\iskind{\Omega_\text{app}}{\kappa_i}}{\nkind}$
  \item (\textbf{Kinding 2}) $\sseq{\cvalidK{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\cekappa'_i}{\kappa'_i}}{\ncon}$ and $\sseq{\iskind{\Omega_\text{app}}{[\omega]\kappa'_i}}{\ncon}$
  \item (\textbf{Kinding 3}) $\sseq{\cExpands{\uOmega}{\parseUConF{\bsubseq{b}{m'_i}{n'_i}}}{c_i}{[\omega]\kappa'_i}}{\ncon}$ and $\sseq{\haskind{\Omega_\text{app}}{c_i}{[\omega]\kappa'_i}}{\ncon}$
  \item (\textbf{Kinding 4}) $\sseq{\cvalidC{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\ctau_i}{\tau_i}{\akty}}{\nexp}$ and $\sseq{\haskind{\Omega_\text{app}}{[\omega]\tau_i}{\akty}}{\nexp}$
  \item (\textbf{Typing 2}) $\sseq{\expandsP{\uOmega}{\uPsi}{\uPhi}{\parseUExpF{\bsubseq{b}{m''_i}{n''_i}}}{e_i}{[\omega]\tau_i}}{\nexp}$ and $\sseq{\hastypeP{\Omega_\text{app}}{e_i}{[\omega]\tau_i}}{\nexp}$
  \item (\textbf{Capture Avoidance}) $e = [\sseq{\kappa_i/k_i}{\nkind}, \sseq{c_i/u_i}{\ncon}, \sseq{e_i/x_i}{\nexp}, \omega]e''$ for some $e''$ and fresh $\sseq{k_i}{\nkind}$ and fresh $\sseq{u_i}{\ncon}$ and fresh $\sseq{x_i}{\nexp}$
  \item (\textbf{Context Independence}) $\mathsf{fv}(e'') \subset \sseq{k_i}{\nkind} \cup \sseq{u_i}{\ncon} \cup \sseq{x_i}{\nexp} \cup \domof{\OParams}$
  % $\hastypeP{\sseq{\Khyp{k_i}}{\nkind} \cup \sseq{u_i :: [\omega]\kappa'_i}{\ncon} \cup \sseq{x_i : [\omega]\tau_i}{\nexp}}{[\omega]e''}{\tau}$\todo{maybe restate this in terms of free variables of e'' here and elsewhere, because context isn't technically well-formed here?}
\end{enumerate}
\end{theorem}
\end{comment}

\end{document}
