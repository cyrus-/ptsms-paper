\PassOptionsToPackage{svgnames,dvipsnames,svgnames}{xcolor}
%% For double-blind review submission
\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
% \documentclass[sigplan,review,10pt,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
% \documentclass[acmsmall,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission
%\documentclass[acmlarge,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission
%\documentclass[acmlarge]{acmart}\settopmatter{}

%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format should change 'acmlarge' to
%% 'sigplan,10pt'.


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

%% Cyrus packages
\usepackage{microtype}
\usepackage{todonotes}
\usepackage{mdframed}
\usepackage{colortab}
\usepackage{mathpartir}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{wasysym}
\definecolor{light-gray}{gray}{0.95}

% \newtheorem{theorem}{Theorem}[chapter]
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \newtheorem{condition}[theorem]{Condition}

% \usepackage{titlesec}
% \titlespacing{\paragraph}{0pt}{10pt}{10pt}

% \usepackage{titlesec}

% \titlespacing*\section{0pt}{1pt plus 2pt minus 2pt}{2pt plus 2pt minus 2pt}
% \titlespacing*\subsection{0pt}{1pt plus 2pt minus 2pt}{2pt plus 2pt minus 2pt}
% \titlespacing*\subsubsection{0pt}{1pt plus 2pt minus 2pt}{2pt plus 2pt minus 2pt}
% \titlespacing*\paragraph{0pt}{5pt plus 2pt minus 2pt}{2pt plus 2pt minus 2pt}


\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{0.25ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother


%% Listings
\definecolor{mygray}{rgb}{0.75,0.75,0.75}
\usepackage{listings}
\lstset{tabsize=2, 
basicstyle=\ttfamily\fontsize{8pt}{1em}\selectfont, 
commentstyle=\itshape\ttfamily\color{gray},
stringstyle=\ttfamily\color{purple},
mathescape=false,escapeinside={(~}{~)},
numbers=left, numberstyle=\scriptsize\color{mygray}, language=ML,moredelim=[il][\sffamily]{?},showspaces=false,showstringspaces=false,xleftmargin=0pt, numbersep=-6pt, morekeywords=[1]{try,spliced,tyfam,opfam,let,fn,val,def,casetype,objtype,metadata,of,*,string,lexer,parser,datatype,new,toast,notation,module,switch,where,expansions,require,import,for,ana,syn,opcon,tycon,metasignature,metamodule,metasig,metamod,static,at,by,tycase,mod,macro,match,pattern,in,patterns,expressions,implicit,forall,exptsm,pattsm},deletekeywords={double,structure,of},classoffset=0, xleftmargin=0pt, 
aboveskip=3pt,belowskip=2pt,
moredelim=**[is][\color{red}]{SSTR}{ESTR},
moredelim=**[is][\color{OliveGreen}]{SHTML}{EHTML},
moredelim=**[is][\color{purple}]{SCSS}{ECSS},
moredelim=**[is][\color{brown}]{SSQL}{ESQL},
moredelim=**[is][\color{orange}]{SCOLOR}{ECOLOR},
moredelim=**[is][\color{magenta}]{SPCT}{EPCT}, 
moredelim=**[is][\color{gray}]{SNAT}{ENAT}, 
moredelim=**[is][\color{OliveGreen}]{SURL}{EURL},
moredelim=**[is][\color{SeaGreen}]{SQT}{EQT},
moredelim=**[is][\color{Periwinkle}]{SGRM}{EGRM},
moredelim=**[is][\color{YellowGreen}]{SID}{EID},
moredelim=**[is][\color{Sepia}]{SUS}{EUS},
% deletestring=[d]{"},
}
\lstset{morecomment=[n]{/*}{*/}}

\newcommand{\liv}[1]{\lstinline{#1}}
\newcommand{\li}[1]{\lstinline[basicstyle=\ttfamily\fontsize{9pt}{1em}\selectfont]{#1}}


\makeatletter\if@ACM@journal\makeatother
%% Journal information (used by PACMPL format)
%% Supplied to authors by publisher for camera-ready submission
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{1}
\acmArticle{1}
\acmYear{2017}
\acmMonth{1}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\else\makeatother
%% Conference information (used by SIGPLAN proceedings format)
%% Supplied to authors by publisher for camera-ready submission
\acmConference[PL'17]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2017}{New York, NY, USA}
\acmYear{2017}
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\fi


%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission
\setcopyright{none}             %% For review submission
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2017}           %% If different from \acmYear


%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
% \citestyle{acmauthoryear}   %% For author/year citations

\input{macros}

\setlength{\abovecaptionskip}{4pt plus 3pt minus 2pt} % Chosen fairly arbitrarily
\setlength{\belowcaptionskip}{-2pt plus 3pt minus 2pt} % Chosen fairly arbitrarily

\begin{document}

%% Title information
\title[Reasonably Programmable Literal Notation]{Reasonably Programmable Literal Notation}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
% \titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
% \subtitle{Subtitle}                     %% \subtitle is optional
% \subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Paper note
%% The \thanks command may be used to create a "paper note" ---
%% similar to a title note or an author note, but not explicitly
%% associated with a particular element.  It will appear immediately
%% above the permission/copyright statement.
% \thanks{with paper note}                %% \thanks is optional
                                        %% can be repeated if necesary
                                        %% contents suppressed with 'anonymous'


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
\vspace{-5px}
%Many languages define convenient list literal forms. This paper questions this common practice: why should an otherwise ordinary data structure be privileged in the definition of a general-purpose programming language? 
General-purpose programming languages typically define literal notation for only a small number of common data structures, e.g. lists. This is unsatisfying because there are many other data structures for which literal notation might be useful, e.g. finite maps, regular expressions, HTML data, SQL queries, syntax trees and chemical structures. There may also be different implementations of each of these data structures, perhaps with different performance characteristics, that could all benefit from common literal notation. %A less \emph{ad hoc} design would be one that allows library providers to define new literal forms in a decentralized manner. However, the mechanisms available to programmers today, e.g. Camlp4 \cite{ocaml-manual} and Sugar* \cite{erdweg2011sugarj,erdweg2013framework}, are \emph{unreasonable}. In particular, they do not support modular reasoning about syntactic determinism, so separately defined literal forms can and do conflict syntactically with one another. Moreover, clients cannot reason abstractly about types and binding when they encounter an unfamiliar literal form (like they can when they encounter an unfamiliar function being applied.)% Instead, they must reason transparently, i.e. about the underlying expansion. This increases cognitive cost, defeating much of the purpose of convenient literal forms. %This makes it difficult to program ``in the large'', and these mechanisms have not been widely adopted.% Hygienic, typed term-rewriting systems, e.g. the Scala macro system \cite{ScalaMacros2013}, are only somewhat more reasonable, and do not offer direct syntactic control.
This paper introduces \emph{typed literal macros (TLMs)}, which allow library providers to define new literal notation of nearly arbitrary design at any specified type or parameterized family of types. 
% TLMs give library providers programmatic control over the parsing and expansion of expressions and patterns of a flexible \emph{generalized literal form} at {a} specified type or parameteric family of types. % The mechanism is {strictly hygienic}, meaning that 1) the expansion must be context-independent; and 2)  splicing is capture avoiding. 
% Partial parameter application lowers the syntactic cost of this strict style. 
Compared to existing approaches, TLMs are uniquely \emph{reasonable}: TLM providers can reason modularly about syntactic ambiguity, and TLM clients can reason abstractly, i.e. without examining the underlying expansion, about types and binding. The system only needs to convey to clients, via secondary notation, the inferred \emph{segmentation} of each literal, which gives the locations and types of spliced subterms. 
%Maintaining information about segmentation is necessary for the novel TLM hygiene mechanism, i.e. the mechanism that ensures that clients can reason abstractly about binding (previously, only unhygienic macro systems supported splicing of terms out of string literal bodies.) 
%The system can determine this {segmentation} automatically and convey it to the client via syntax highlighting, or by presenting the client with a context-free grammar (without revealing the semantic actions associated with each production.) 
% This mechanism captures many common syntactic idioms while avoiding the problem of syntactic conflicts by construction and supplying clients with clear abstract reasoning principles. 
This paper establishes these abstract reasoning principles formally with a calculus of typed expressions, pattern matching and ML-style modules. 
This calculus is the first detailed type-theoretic account of a hygienic macro system, of any design, for a language with these essential features of ML. 
We are integrating TLMs into Reason, an emerging alternative front-end for OCaml.% This calculus is the first such formalization of a typed macro system.

% \emph{Typed syntax matcros (TLMs)}, proposed in a recent short paper by Omar et al. \cite{sac15}, give library providers programmatic control over the parsing and expansion of only terms of {(generalized) literal form}. This appears to occupy a ``sweet spot'' in that it captures many common syntactic idioms while avoiding the problem of conflict. TLMs also maintain a reasonable type and binding discipline.% In particular, clients can use any combination of TLMs in a program without needing to consider conflicts between them, and the language validates each expansion that a TLM generates to maintain 1) a \emph{type discipline} (meaning clients can determine the type of an unexpanded expression without examining its expansion directly); 
% %and 2) a \emph{hygienic binding discipline}.
% %, meaning that the expansion cannot make any assumptions about bindings at the application site, nor  introduce ``hidden bindings'' into subterms. 

% TLMs have only been described minimally -- it is not clear how they should be adapted for integration into languages like ML that support {pattern matching}, {parameterized datatypes}, {modules} and abstract types. Moreover, the prior work makes several simplifying assumptions related to binding that are impractically restrictive.% bno mechanism for binding values for use across TLM definitions has been described and the hygiene mechanism makes giving the expansions that they generate access to ``helper functions'' awkward.

% This paper gives a complete account of TLMs that addresses these deficiencies. In particular, we 1) integrate TLMs with pattern matching; 2) introduce a distinct static phase of evaluation, which gives TLM definitions access to libraries; and 3) introduce type and module parameters, which serve two purposes: they allow for TLMs that operate uniformly at a parameterized family of types (rather than only at a single type), and they give expansions explicit, hygienic access to libraries. Support for partial application  of parameters lowers the syntactic cost of this explicit approach. 

% Put succinctly, we design a programming language in the ML tradition with a \emph{reasonably} programmable syntax.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
% \begin{CCSXML}
% <ccs2012>
% <concept>
% <concept_id>10011007.10011006.10011008</concept_id>
% <concept_desc>Software and its engineering~General programming languages</concept_desc>
% <concept_significance>500</concept_significance>
% </concept>
% <concept>
% <concept_id>10003456.10003457.10003521.10003525</concept_id>
% <concept_desc>Social and professional topics~History of programming languages</concept_desc>
% <concept_significance>300</concept_significance>
% </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Software and its engineering~General programming languages}
% \ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
% \keywords{keyword1, keyword2, keyword3}  %% \keywords is optional


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle

\vspace{-5px}
\newcommand{\introSec}{Introduction}
\section{\protect\introSec}
\label{sec:intro}
\vspace{-2px}
% The surface syntax of a programming language is its user interface. 
When designing the surface syntax of a general-purpose programming language, it is common practice to define shorthand \emph{literal notation}, i.e. notation that decreases the syntactic cost of constructing and pattern matching over values of a particular data structure or parameterized family of data structures. For example, many languages in the ML family support list literals like \li{[x1, x2, x3]} in both expression and pattern position \cite{harper1997programming,mthm97-for-dart}. 
%\footnote{In SML, list literals work even in contexts where the list expression constructors have been shadowed by other bindings, i.e. the meaning of literal forms is \emph{context independent}. 
% We will return to this concept again later.} 
While lists are common across problem domains, other literal notation is more specialized. For example, Ur/Web extends the surface syntax of Ur (an ML-like language \cite{conf/pldi/Chlipala10}) with expression and pattern literals for encodings of XML and HTML data~\cite{conf/popl/Chlipala15}. The example in Fig. \ref{fig:urweb} shows two HTML literals, one that ``splices in'' a string expression delimited by \li{\{[} and \li{]\}} and the other an HTML expression delimited by \li{\{} and \li{\}}.
%The desugarings of these literal expressions, not shown, are substantially more verbose and less readable to programmers familiar with XHTML. 

\begin{figure}[h]
\vspace{-9px}
\begin{lstlisting}
  fun heading first_name = SURL<xml><h1>Hello, {[EURLfirst_nameSURL]}!</h1></xml>EURL
  val body = SURL<xml><body>{EURLheading "World"SURL} ...</body></xml>EURL
\end{lstlisting}
\vspace{-4px}
\caption{HTML literals with support for splicing at two different types are built primitively into Ur/Web \cite{conf/popl/Chlipala15}.}
\label{fig:urweb}
\vspace{-7px}
\end{figure}

This design practice, where the language designer privileges certain library constructs with built-in literal notation, is \emph{ad hoc} in that it is easy to come up with other examples of data structures for which mathematicians, scientists or programmers have invented specialized notation \cite{DBLP:journals/cacm/Iverson80,cajori1928history,TSLs}. For example, 1) clients of a ``collections'' library might want not just list literals, but also literal notation for matrices, finite sets, maps and so on; 2) clients of a ``web programming'' library might want CSS literals (which Ur/Web lacks); 3) a compiler author might want ``quotation'' literals for the terms of the object language and various intermediate languages of interest; and 4) clients of a ``chemistry'' library might want chemical structure literals based on the SMILES standard \cite{anderson1987smiles}.

Although requests for specialized literal notation are easy to dismiss as superficial, the reality is that literal notation, or the absence thereof, can have a substantial influence on software quality. For example, \citet{Bravenboer:2007:PIA:1289971.1289975} finds that literal notation for structured encodings of queries, like the SQL-based query literals now found in many languages \cite{meijer2006linq}, reduce the temptation to use string encodings of queries and therefore reduce the risk of catastrophic string injection attacks~\cite{owasp2017}. More generally, evidence suggests that programmers frequently resort to ``stringly-typed programming'', i.e. they choose strings instead of composite data structures largely for reasons of notational convenience. In particular, \citet{TSLs} sampled strings from open source projects and found that at least 15\% of them could be parsed by some readily apparent type-specific grammar, e.g. for URLs, paths, regular expressions and many others. 
Literal notation, with support for splicing, would decrease the syntactic cost of composite encodings, which are more amenable to programmatic manipulation and compositional reasoning. %Literal forms would  preferable to string encodings because they support compositional construction and decomposition by pattern matching via splicing.



Of course, it would not be scalable to ask general-purpose language designers to build in support for all known notations \emph{a priori}. Instead, there has been persistent interest in mechanisms that allow library providers to define new literal notation on their own. For example, direct grammar extension systems like Camlp4 \cite{ocaml-manual} and Sugar* \cite{erdweg2011sugarj,erdweg2013framework}, macro-based systems like Template Haskell~\cite{SheardPeytonJones:Haskell-02,mainland2007s} and the \emph{type-specific languages} (TSLs) of \citet{TSLs}, and other systems that we will discuss below can all be used to define new literal notation (and, in some cases, other forms of new notation, such as new infix expression forms, control flow operations or type declaration forms, which we leave beyond the scope of this paper). 

\paragraph{Problem} The problem that specifically motivates this paper is that these existing systems  make it difficult or impossible to reason abstractly about such fundamental issues as types and variable binding. Instead, programmers and editor services can only reason transparently, i.e. by inspecting the underlying expansion or the implementation details of the collection of extensions responsible for producing the expansion.% In some existing systems, even determining which syntax extension is responsible for a literal form is difficult.
%, i.e. when developing large programs ``consisting of many small programs o(modules), possibly written by different people''
\begin{figure}[t!]
\begin{subfigure}[t]{0.48\textwidth}
\fcolorbox{gray!20}{gray!20}{\makebox[0.97\textwidth][l]{\textsc{existing grammar extension systems}}}
\vspace{-12px}
\begin{lstlisting}[xleftmargin=-2pt, morekeywords={EXTEND}]
  EXTEND /* loaded by, e.g., camlp4 */
    expr:
      |  "`(" q = kquery ")`" -> q
    kquery: 
      | /* ...K query grammar... */
  /* ...more extensions defined... */
  let w = compute_w();
  let x = compute_x(w);
  let y = `((!R)@&{&/x!/:2_!x}'!R)`;
\end{lstlisting}
\vspace{-5px}
\caption{It is difficult to reason abstractly about programs that use unfamiliar grammar extensions (see the six reasoning criteria in the text).}
\label{fig:K-dialect}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.51\textwidth}
\fcolorbox{gray!20}{gray!20}{\makebox[\textwidth][l]{\textsc{\textbf{this paper}: typed literal macros}}}
\vspace{-12px}
\begin{lstlisting}[xleftmargin=-2pt]
  notation $kq at KQuery.t 
    lexer  KQueryLexer
    parser KQueryParser.start
    expansions require
      module KQuery as KQuery;
  /* ...more notations defined... */
  let w = compute_w();
  let x = compute_x(w);
  let y = $kq `(SURL(!R)@&{&/EURLxSURL!/:2_!EURLxSURL}'!REURL)`;
\end{lstlisting}
\vspace{-5px}
\caption{TLMs make examples like these more reasonable by leaving the base grammar fixed and making the type and binding structure and the segmentation explicit.}
\label{fig:K-tsm-example}
\end{subfigure}
\vspace{4px}
\caption{Two of the possible ways to introduce literal notation for encodings of K queries}
\vspace{-10px}
\end{figure}

Consider, for example, the perspective of a programmer attempting to comprehend the program text in Figure \ref{fig:K-dialect}, which is written in a dialect of OCaml's surface syntax called Reason \cite{reason-what} that has, hypothetically, been extended with some number of new literal forms by a grammar extension system --- Lines 1-5 show the grammar extension syntax from Camlp4 \cite{ocaml-manual}, but Sugar*/SugarHaskell is  similar \cite{erdweg2011sugarj,erdweg2013framework,erdweg2012layout}. 
Line 9 uses one of the active syntax extensions to construct an encoding of a query in the niche database query language K, using its intentionally terse notation~\cite{Whitney:2001:LOR:376284.375783}. The problem is that a programmer looking at the program as presented, and unfamiliar with (i.e. holding abstract) the details elided on Lines 5-6, cannot easily answer questions like the following:

\begin{enumerate}[leftmargin=15px]
\item \textbf{Responsibility}: Which syntax extension determined the expansion of the literal on Line~9? Might activating a new extension generate a conflicting expansion for the same literal?
\item \textbf{Expansion Typing}: What type does \li{y} have?
\item \textbf{Context Dependence}: Which bindings does the expansion of Line~9 invisibly depend on? If I shadow or remove a module or other binding, could that break or change the meaning of Line~9 because its expansion depended invisibly on the original binding?
\item \textbf{Segmentation}: Are the characters \li{x}, \li{R} and \li{2} on Line~9 parsed as spliced terms, meaning that they appear directly in the underlying expansion, or are they parsed in some other way peculiar to this literal notation (e.g. as operators in the K query language)?
\item \textbf{Segment Typing}: What type is each spliced term expected to have? How can I infer a type for a variable that appears in a spliced term without looking at where it ends up in the expansion?
\item \textbf{Capture}: If \li{x} is in fact a spliced term, does it refer to the binding of \li{x} on Line 5, or might it capture an invisible binding of the same identifier in the expansion of Line 9?
\end{enumerate}

% In short, the problem is that library providers cannot reason modularly about syntactic determinism (because other extensions might define overlapping forms), and if the desugaring of the program text is held abstract, programmers can no longer reason about types, binding and segmentation (i.e. answer questions like those above.) This is burdensome at all scales, but particularly when programming in the large, where it is common to encounter a wide variety of libraries \cite{DBLP:conf/sac/LammelPS11}. 
Forcing the programmer to reason transparently to answer basic questions like these defeats the ultimate purpose of syntactic sugar: decreasing cognitive cost \cite{Green89}. Analagous problems do not arise when programming without syntax extensions in languages like ML --- programmers can reason lexically about where variables and other symbols are bound, and types mediate abstraction over function and module implementations \cite{B304}. Ideally, the programmer would be able to abstract in some analagous manner over the implementation of an unfamiliar notation. % From this perspective, it is unsurprising that the use of Camlp4 has been deprecated in the OCaml system.

Given these issues, we concluded that direct grammar extension systems were not ideally suited for integration into Reason. We also evaluated various approaches that are based not on direct grammar extension but on term rewriting over a fixed grammar. We give a full account of this evaluation in Sec.~\ref{sec:existing-approaches}. Briefly, we found that:
\begin{itemize}[leftmargin=15px]
\item Unhygienic approaches, like OCaml's preprocessor extension point (PPX) rewriters \cite{ocaml-manual} and Template Haskell \cite{SheardPeytonJones:Haskell-02,mainland2007s} allow us to define new literal notation with support for splicing by repurposing existing string literal forms. They also partially or completely solve the problem of reasoning about \textbf{Responsibility} by using explicit annotations on terms that are being rewritten. However, they do not satisfy the remaining five reasoning criteria.
\item Hygienic staging macro systems, like MetaOCaml and related systems \cite{DBLP:conf/flops/Kiselyov14,ganz2001macros,Sheard:1999:UMS} do not give the expander access to the syntax trees of arguments, i.e. the rewriting is parametric in the arguments, so they are not suitable for the problem of defining new literal notation.  
\item Hygienic term rewriting macro systems, like those in various Lisp-family languages \cite{DBLP:conf/esop/HermanW08,Herman10:Theory,Kohlbecker86a,DBLP:conf/popl/Adams15,DBLP:conf/popl/ClingerR91,DBLP:journals/lisp/DybvigHB92,mccarthy1978history,Flatt:2012:CLR:2063176.2063195} and Scala \cite{ScalaMacros2013}, do not allow us to repurpose string literal forms to define literal forms at other types because the hygiene discipline does not allow us to splice terms out of literal bodies via parsing. Only existing composite forms can therefore be repurposed.
\item An approach that came close was Omar et al.'s \emph{type-specific languages} (TSLs) \cite{TSLs}, but it fell short with regard to the six reasoning principles just discussed in ways that we will return to and in any case, the mechanism was designed for a simple monomorphic, nominally-typed language and relied critically on a particular local type inference scheme. It is not suitable for a language with an ML-like semantics, meaning a language with support for structural types (like tuple types and arrow types in ML), parameterized types, abstract types, modules, pattern matching and non-local type inference.
\end{itemize}

% Based on this evaluation, we concluded that the existing approaches were not ideally suited for integration into Reason, an increasingly popular open source project that seeks to provide a more clear and practical surface syntax for the OCaml programming language.

\paragraph{Contributions} This paper introduces \emph{typed literal macros} (TLMs): the first system for defining new literal notation, of nearly arbitrary design, that (1) provides the ability to reason abstractly about all six of the topics just outlined; and (2) is semantically expressive enough for integration into Reason/OCaml and other full-scale statically typed functional languages. We evaluate these claims with (1) a series of increasingly ML-like calculi equipped with proofs of the claimed abstract reasoning principles; and (2) a number of non-trivial examples that appear throughout the paper and involve the language features mentioned above. In describing these examples, we also demonstrate that literal parsing logic can be defined using standard, unmodified parser generators, so the burden on notation providers is comparable to that of existing systems despite these stronger client-side reasoning principles. % From the client's perspective, which is the focus of our contributions, TLMs are uniquely reasonable. 

%Unique to our approach in this paper is the focus, both formal and informal, on maintaining client-side abstract reasoning principles.

% The empirical study in the prior work on TSLs \cite{TSLs} supports the claim that a macro-based mechanism for defining new literal notation is broadly useful, so This paper is targeted at language designers that seek a deeper understanding of the reasoning principles at play, particularly as they relate to features common to ML and other full-scale statically typed languages (e.g. Haskell, Scala, Rust, etc.)

\paragraph{Brief Overview} To finish up our discussion of the K query example above, let us give a brief overview of the TLM-based solution outlined in Figure \ref{fig:K-tsm-example}. On lines 1-5, we define a TLM named \li{$kq}. On Line 9, we apply this TLM. We can reason abstractly about this program as follows:
\begin{enumerate}[leftmargin=15pt]
\item \textbf{Responsibility}: The lexer and parser specified by the applied TLM on Lines 2-3 are exclusively responsible for lexing, parsing and expanding the body of the generalized literal form, i.e. the characters between \li{`(} and \li{)`}. We will give more details on generalized literal forms, and on constructing a TLM lexer and parser in the next section. For now, it suffices to say that our design goal is to provide a mechanism where the programmer does not normally need to look up the definitions of \li{KQueryLexer} and \li{KQueryParser} to reason about types and binding. % To briefly summarize, our system uses Menhir, which is the most popular parser generator in the OCaml ecosystem and integrated into its standard build tools. %the context-free grammar of the language is fixed, so the TLM provider can modularly establish that the notation that the TLM implements is unambiguous (even in situations where a spliced term itself applies another TLM, which we will show in a later example).
\item \textbf{Expansion Typing}: The type annotation on the definition of \li{$kq} (Line 1) determines the type that the expansion must have, here \li{KQuery.t}.
\item \textbf{Context Dependence}: The external dependencies of the generated expansion are explicitly specified. Here, Lines 4-5 specify that expansions generated by \li{$kq} require the module \li{KQuery}. The system ensures that this dependency is  bound as specified even if the identifier \li{KQuery} has been shadowed at the application site. By making dependencies explicit, we can be sure that the expansion does not rely on the fact that, e.g., \li{w} is in scope at the application site. 
\item \textbf{Segmentation}: The intermediate output that the TLM generates is structured so that the system can infer from it an accurate \emph{segmentation} of the literal body that distinguishes spliced terms, which appear directly in the expansion, from segments that are parsed in some other way by the TLM. The segmentation is all that needs to be communicated to editor services downstream of the expander, and ultimately to the programmer using secondary notation, e.g. colors in this document. So by examining Fig. \ref{fig:K-tsm-example}, the programmer knows that the two instances of \li{x} on Line 3 are parsed as spliced expressions (because they are black), whereas the \li{R}'s must be parsed in some other way, e.g. as operators of the K language (because they are green).
\item \textbf{Segment Typing}: Each spliced segment in the inferred segmentation also has  a type annotation. This, together with the context independence property, ensures that type inference at the TLM application site can be performed (by editor services or in the programmer's mind) abstractly, i.e. by requesting just the type annotations on the spliced segments.  %This information is usually not necessary to reason about typing, but it can be conveyed to the programmer upon request by the program editor if desired. %TLM definitions follow the usual scoping rules, so it is easy to ``jump to the definition'' of \li{$kq}.
\item \textbf{Capture}: Splicing is guaranteed to be capture-avoiding, so the spliced expression \li{x} must refer to the binding of \li{x} on Line 2. It cannot have captured a coincidental binding of \li{x} in the expansion. % We will say more about capture later on.
\end{enumerate}

% We will see examples of TLMs that make use of the more advanced features of the OCaml type system -- notably, pattern matching, parameterized types and modules -- later.

%\footnote{In general, spliced expressions might themselves apply TLMs, in which case the convention is to use a distinct color for unspliced segments at each depth. For example, if strings were not primitive, we might write \li{`SURL<body>\{EURLheading ($str "SSTRWorld!ESTR")SURL\} ...</body>EURL`}.}





\paragraph{Paper Outline} 
% \begin{itemize}
% \item 
Sec.~\ref{sec:setsms} introduces expression TLMs with a substantially more detailed case study. 
% \item 
Sec.~\ref{sec:sptsms} then introduces {pattern TLMs} and describes the special reasoning conditions in pattern position. %The hygiene condition for pattern TLMs is interesting.
% \item 
Sec.~\ref{sec:ptsms} introduces the more general {parametric TLMs}, i.e. TLMs that can take type and module parameters, which allow us to define literal notation over a type- or module-parameterized family of types. This section also reveals that the \li{expansions require} clause can be understood as parameterization followed immediately by partial parameter application. 
Having introduced the basic machinery, we continue in Sec.~\ref{sec:more-examples} with examples that further demonstrate the expressive power of this approach and various parser implementation strategies that notation providers can consider. Notably, this section gives our take on the example from  Fig.~\ref{fig:urweb} of Ur/Web-style HTML literals as well as a TLM that implements quasiquotation for Reason language terms, which is useful for writing other TLMs. 
Next, Sec.~\ref{sec:setsms-formally} defines a type-theoretic calculus of simple expression and pattern TLMs and formally establishes the reasoning principles implied above in their simplest form. 
Sec.~\ref{sec:ptlms-formally} adds type functions and an ML-style module system to this calculus, and gives a more general variant of the reasoning principles theorem. 
% \item 
Sec.~\ref{sec:existing-approaches} compares TLMs to related work, guided by the rubric of  reasoning principles just discussed. 
% \item 
Sec.~\ref{sec:discussion} concludes with a summary of the contributions of this paper and a discussion of limitations and future work.
Certain technical details, proofs and extensions to the calculi presented in this paper are given in the supplement. % Also available is an additional series of case studies: notation for an encoding of regular expressions as an abstract data type, implemented using TLMs for quasiquotation and grammar-based parser generators. %are also available in the supplement. Sec.~\ref{sec:static-eval} considers the topic of term evaluation during the  expansion phase, i.e. \emph{static evaluation}, in  more detail. Sec \ref{sec:static-eval} also gives examples of TLMs that are useful for defining other TLMs, e.g. TLMs that implement parser generators and quasiquotation. 

%  We say more about LISP macros in Sec. \ref{sec:existing-approaches}.% For the present purposes, we can consider Reason as essentially the calculus defined in the supplement extended with various conveniences that are commonly found in other ML-like languages and, notionally, orthogonal to TLMs. %Reason is, as its name suggests, a conceptual descendent of ML. It diverges from other dialects of ML that have a similar type structure in that it has a bidirectional type system \cite{Pierce:2000:LTI:345099.345100} (like, for example, Scala \cite{OdeZenZen01}) for reasons that have to do with the mechanism of TLM implicits described in Chapters \ref{chap:tsls} and \ref{chap:ptsms}. 
%The reason we will not follow Standard ML \cite{mthm97-for-dart} in giving a complete formal definition of Reason in this work is both to emphasize that the primitives we introduce are ``insensitive'' to the details of the underlying type structure of the language (so TLMs can be considered for inclusion in a variety of languages, not only dialects of ML), and to avoid distracting the reader (and the author) with definitions that are already well-understood in the literature and that are orthogonal to those that are the focus of this work. 
% We will not formally define these features mainly to avoid unnecessarily complicating our presentation with details that are not essential to the ideas presented herein. As such, 
% All examples written in Reason should be understood to be informal motivating material for the subsequent formal material. %We anticipate that future full-scale language specifications will be able to combine the ideas  in the proposed work without trouble. %The purpose of the work being proposed is to serve as a reference for those interested in the new constructs we introduce, not to serve as a language specification. 
%We will give a brief overview of these languages are organized in Sec. \ref{sec:Reason}.




% \subsection{Contributions}\label{sec:contributions}
% This work introduces a system of \textbf{typed literal macros (TLMs)} that gives library providers substantially more syntactic control than existing typed term-rewriting macro systems while maintaining the ability to reason abstractly about types, binding and segmentation.% abstract reasoning principles. % comparable to the level of control they have when defining a syntax dialect.

% Clients apply TLMs to \emph{generalized literal forms}. 

% Because the context-free syntax is never extended, syntactic conflicts are not a concern.

%As such, the semantics can take the type and binding structure of the surrounding program into account when validating the expansion that the TLM programmatically generates to ensure that clients can answer critical questions related to types and binding, like those enumerated in Section \ref{sec:abs-reasoning-intro}. Clients need not have knowledge of the implementation of the TLM or of the generated expansion, i.e. there are useful principles of syntactic abstraction.

% The primary technical challenge has to do with the fact that the applied TLM needs to be able to splice terms out of the literal body for inclusion in the expansion. For example, 

 % We design our mechanism such that these locations can easily be determined from the output of the TLM. This is essential for our hygiene mechanism, and it is also useful in that this information can be presented to the user (e.g. as shown in Figure \ref{fig:first-tsm-example-marked}). %As such, we must develop a mechanism where 1) the positions of spliced subterms can be determined without examining the macro implementation (e.g. so that they can be presented to the user differently by an editor or pretty-printer, ;  and 2) the hygiene mechanism must give only portions of the expansion that correspond to these spliced subterms access to the application site context. 



%In order to reason about types and binding, client programmers need only have knowledge of 1) the segmentation (e.g. by examining a figure like this presented by a code editor or pretty-printer) and 2) a type annotation on the definition of the applied TLM. No other details about the applied TLM's implementation or the expansion that it generates need to be revealed to the client programmer. In other words, 


% \begin{figure}[h]
% \begin{lstlisting}
% PElement Nil Seq(
%   TextNode "Hello, ", 
%   Seq(TextNode (join(" ", Cons(first, Cons(second, Nil)))), 
%   TextNode "!"))
% \end{lstlisting}
% \caption{The desugaring.}
% \end{figure}


% There is also no ambiguity with regard to which TLM has control over each form, and searching for the definition of a TLM is no more difficult than searching for any other binding, i.e. there are well-defined scoping rules.

% In other words, TLMs maintain a useful notion of syntactic abstraction. %More specifically, TLMs maintain a \emph{hygienic binding discipline}, meaning that questions Questions 4 and 5 above were concerned with are disallowed entirely. 
% We will, of course, make this notion more technically precise as we continue.


% \begin{figure}[h]
% \begin{lstlisting}[numbers=none,xleftmargin=0px]
% let syntax $strlist = $list string in 
% $html `SURL<p>Hello, {[EURLjoin ($str ' ') ($strlist [firstSURL,EURL last])SURL]}</p>EURL`
% \end{lstlisting} 
% \caption{The example from Figure \ref{fig:first-tsm-example-marked}, expressed using a parametric TLM.}
% \label{fig:first-ptsm-example-marked}
% \end{figure}

% In Secs. \ref{chap:uetsms} and \ref{chap:uptsms}, we assume for the sake of technical simplicity that each TLM definition is self-contained, needing no access to libraries or to other TLMs. This is an impractical assumption in practice. We relax this assumption in In 

% %\item \textbf{Type-specific languages}, or \textbf{TSLs}. TSLs, described 
% In Chapter \ref{chap:tsls}, we develop a mechanism of \emph{TLM implicits} that allows library clients to contextually designate, for any type, a privileged TLM at that type. The semantics applies this privileged TLM implicitly to unadorned literal forms that appear where a term of the associated type is expected. For example, if we designate 
% %\li{$str} 
% as the privileged TLM at the \li{string} type and 
% %\li{$strlist}
% as the privileged TLM at the \li{list(string)} type, we can express the example from Figure \ref{fig:first-tsm-example-marked} instead as shown in Figure \ref{fig:first-tsm-example-implicit} (assuming \li{join} has type \li{string -> list(string) -> string}.) 
% \begin{figure}[h]
% \begin{lstlisting}[numbers=none]
% $html`SURL<p>Hello, {[EURLjoin ' ' [firstSURL,EURL last]SURL]}</p>EURL`
% \end{lstlisting}
% \caption{The example from Figure \ref{fig:first-tsm-example-marked} drawn to take advantage of TLM implicits.}
% \label{fig:first-tsm-example-implicit}
% \end{figure}

% \noindent This approach is competitive in cost with library-specific syntax dialects (e.g. compare Figure \ref{fig:first-tsm-example-implicit} to Figure \ref{fig:urweb}), while maintaining the abstract reasoning principles characteristic of our approach.
%\item \textbf{Metamodules}, introduced in Sec. \ref{sec:metamodules}, reduce the need to primitively build in the type structure of constructs like records (and variants thereof),  labeled sums and other interesting constructs that we will introduce later by giving library providers programmatic ``hooks'' directly into the semantics, which are specified as a \emph{type-directed translation semantics} targeting a small \emph{typed internal language} (introduced in Sec. \ref{sec:Reason}). %For example, a library provider can implement the type structure of records with a metamodule that:
%\begin{enumerate}
%\item introduces a type constructor, \lstinline{record}, parameterized by finite mappings from labels to types, and defines, programmatically, a translation to unary and binary product types (which are built in to the internal language); and 
%\item introduces operators used to work with records, minimally record introduction and elimination (but perhaps also various functional update operators), and directly implements the logic governing their typechecking and translation to the IL (which builds in only nullary and binary products). 
%\end{enumerate}
%We will see direct analogies between ML-style modules (which our mechanisms also support) and metamodules later.
%\end{enumerate} 


% As vehicles for this work, we will define a small programming language in each of the three parts just mentioned, each building conceptually upon the previous language. All of our formal contributions are relative to these small languages.


%TLMs, like other macro systems, perform \emph{static code generation} (also sometimes called \emph{static} or \emph{compile-time metaprogramming}), meaning that the relevant rules in the static semantics of the language call for the evaluation of \emph{static functions} that generate term encodings. Static functions are functions that are evaluated statically, i.e. during typing. %Library providers write these static functions using the Reason \emph{static language} (SL).  
%Maintaining a separation between the static (or ``compile-time'') phase and the dynamic (or ``run-time'') phase is an important facet of Reason's design. % static code generation. %We will  also introduce a simple variant of each of these primitives that leverages Reason's support for local type inference to further reduce syntactic cost in certain common situations. 

\vspace{-3px}
\newcommand{\seTLMsSec}{Expression TLMs}
\section{\protect\seTLMsSec}
\label{sec:setsms}
\vspace{-2px}

Consider the recursive datatype \li{Regex.t} defined by the module in Fig.~\ref{fig:Regex-module-def}, which encodes \emph{regular expressions} (regexes) into Reason \cite{Thompson:1968:PTR:363347.363387}. Regular expressions are  common in fields like bioinformatics, where they are used to express patterns in DNA sequences. For example, we can construct a regex that matches the strings \li{"A"}, \li{"T"}, \li{"G"} or \li{"C"}, which represent the four bases in DNA, as follows:
\begin{lstlisting}[numbers=none]
  let any_base = Regex.(Or(Str "A", Or(Str "T", Or(Str "G", Or(Str "C")))))
\end{lstlisting}
In Reason, the notation \li{Regex.(}$e$\li{)} locally opens the module \li{Regex} for use within the expression $e$, so we do not need to qualify the constructors \li{Regex.Or} and \li{Regex.Str}. Even allowing for this shorthand, however, this notation for constructing regexes is rather unwieldy. Instead, we would like to be able to use the common POSIX-style notation for constructing values of type \li{Regex.t} \cite{STD95954}.

% One approach would be to simply define a function \li{parse_rx : string -> Regex.t} that parses a string at run-time to produce a regex. This approach works for simple expressions like the one above -- we can write \li{parse_rx "A|T|G|C"}. However, this approach breaks down when one needs to construct regexes compositionally. For example, the DNA pattern that the BisI restriction enzyme recognizes is \li{AT}$x$\li{GC}, where $x$ is any of the four DNA bases. However, one cannot pass \li{any_base} to the string concatenation operator, because \li{any_base} is already of type \li{Regex.t}. These kinds of situations cause programmers to engage in \emph{stringly-typed programming}, as discussed in Sec.~\ref{sec:intro} -- they simply use strings to encode regexes throughout their application, rather than structured encodings like \li{Regex.t}. This can be catastrophic if the programmer is not extremely careful with string concatenation because strings that happen to contain special characters recognized by the POSIX notation can be misinterpreted because they are not distinguished in any way from strings that intentionally use POSIX notation. When some of these strings come from untrusted sources, e.g. web forms as is common in bioinformatics applications, the result can be a breach in privacy. 





% Moreover, there already exists a broadly adopted shorthand notational standard -- the POSIX standard for regular expressions \cite{STD95954}. For the purposes of this section, we will stick to the subset of the POSIX standard that applies to the encoding in Fig.~\ref{fig:Regex-module-def}, but there is nothing that would fundamentally prevent us from implementing the full POSIX standard using TLMs.
\begin{figure}[t]
\begin{subfigure}[t]{0.45\textwidth}
\vspace{-1px}
\begin{lstlisting}[mathescape=~]
  module Regex = {
    type t = AnyChar | Str(string)
           | Seq(t, t) | Or(t, t) 
           | Star(t);
  };
\end{lstlisting}
\vspace{-5px}
\caption{The \li{Regex} module, which defines the recursive datatype \li{Regex.t}.}
\label{fig:Regex-module-def}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.53\textwidth}
\vspace{-1px}
\begin{lstlisting}[mathescape=|]
  notation $regex at Regex.t
    lexer   RegexLexer
    parser  RegexParser.start
    expansions require
      module Regex as Regex;
\end{lstlisting}
\vspace{-5px}
\caption{The definition of the \li{$regex} TLM. Figure \ref{fig:lexer-and-parser} defines \li{RegexLexer} and \li{RegexParser}.}
\label{fig:regex-tlm-def}
\end{subfigure}

\begin{subfigure}[t]{\textwidth}
\vspace{4px}
\begin{lstlisting}
  let any_base = $regex `(SURLA|T|G|CEURL)`;
  let bisA = $regex `(SURLGC$(EURLany_baseSURL)GCEURL)`;
  let gene_restriction_template(gene) => 
    $regex `(SURL$(EURLbisASURL)$(EURLany_baseSURL)*$$(EURLgeneSURL)$(EURLany_baseSURL)*$(EURLbisASURL)EURL)`
\end{lstlisting}
\vspace{-5px}
\caption{Examples of the \li{$regex} TLM being applied in a bioinformatics application. %In each case, literal body, between backticks, is initially left unparsed according to the language's context-free syntax. %The applied TLM determines a segmentation and expansion during the typed expansion phase, which generalizes the usual typing phase.
}
\label{fig:first-tlm-example}
\end{subfigure}
\vspace{3px}
\caption{Case Study: POSIX-style regex literal notation, with support for string and regex splicing.}
\vspace{-5px}
\end{figure}


\vspace{-3px}
\subsection{TLM Definition and Application}
Figure \ref{fig:regex-tlm-def} defines a TLM named \li{$regex} for values of type \li{Regex.t} that supports a POSIX-style literal notation extended with splice forms for regexes and strings. Line 1 of Fig. \ref{fig:first-tlm-example} applies this TLM to construct the regex \li{any_base} just described. Line 2 of Fig. \ref{fig:first-tlm-example} then applies \li{$regex} again, using its regex splice form to construct a regex \li{bisA} matching the DNA sequences recognized by the BisA restriction enzyme, where the middle base can be any base. Finally, Lines 3-4 of Fig. \ref{fig:first-tlm-example} define a function that constructs a more complex regex from these two regexes and a given gene sequence represented as a string, which is spliced in using a string splice form delimited by \li{$$(} and \li{)}. 

Let us consider the second of these three TLM applications more closely:
\begin{lstlisting}[numbers=none]
    $regex `(GC$(any_base)GC)`
\end{lstlisting}
According to the context-free grammar of the language, this form is simply a leaf of the unexpanded syntax tree, like a string literal would be. TLM names are prefixed by \li{$} 
 to distinguish them from variables. There are no lexical constraints on the body of the generalized literal form except that any occurrences of \li{`(} must be balanced by \li{)`}, much like nested comments in OCaml, Reason and other languages. Generalized literal forms, which first arose in the prior work on TSLs \cite{TSLs}, therefore syntactically subsume other literal forms. The prior work specified several other choices of outer delimitation, including layout-sensitive delimitation, but for the purposes of this work, we will use \li{`(} and \li{)`} exclusively. The fact that the context-free grammar of the base language is fixed ensures that notation providers can reason modularly about syntactic ambiguity in their own grammars.

Notice that in the unexpanded expression above, we did not distinguish the spliced expression from the rest of the literal. This is to emphasize that it is only during the subsequent \emph{expansion} phase that the body of the generalized literal form is lexed, parsed and expanded to produce an expanded syntax tree where spliced expressions, like \li{any_base}, have been revealed. Responsibility for lexing, parsing and expanding the literal body is delegated to the lexer and parser specified by the applied TLM by the clauses \li{lexer} and \li{parser}, respectively.

\vspace{-4px}
\subsection{Abstract Reasoning Principles} 
Rather than looking immediately at Fig. \ref{fig:lexer-and-parser}, which defines the lexer and parser specified by \li{$regex}, let us first consider the perspective of a client programmer who does not want to delve into the details of lexing and parsing for each piece of literal notation that they encounter. What can this programmer deduce just by reasoning from the information presented in Figures \ref{fig:Regex-module-def} through \ref{fig:first-tlm-example}? 

% TLM definitions follow well-defined scoping rules, so the client programmer can follow the binding structure of the language in the usual manner (manually or assisted by a ``go to definition'' editor service) to determine which TLM is uniquely responsible for each literal form. In Sec. \ref{sec:ptsms}, we will see an example of TLM synonyms (as a trivial case of partial parameter application).

From the definition of the \li{$regex} TLM, the client can immediately determine the type of the expansion being generated at each application site because it is specified explicitly by the clause \li{at Regex.t} on Line 1 of Fig. \ref{fig:regex-tlm-def}. This is analagous to the return type of a function. The identity of \li{Regex.t} is determined relative to the TLM definition site, not at each application site (so the module \li{Regex} need not be in scope at the application site, or it can be shadowed by a different module).

The TLM mechanism enforces an similarly strong context independence property on generated expansions by requiring that the TLM explicitly specify the modules that the expansions it generates might internally depend on. In this case, Lines 4-5 of Fig. \ref{fig:regex-tlm-def} specify that generated expansion might use the module \li{Regex}, again as it is bound at the definition site, using the module variable \li{Regex} internally. These happen to coincide in this case, but in general, the dependency can be an arbitrary module path while internally it must be bound to a module variable, for example:
\begin{lstlisting}[numbers=none]
  expansions require 
    module Regex as Regex
    module Core.List as L
\end{lstlisting}
The module variable \li{L} will be bound internally to \li{Core.List} in all expansions generated by this TLM, including those where \li{L} or \li{Core.List} are not bound, or otherwise bound. All other bindings, whether at the TLM definition site or the TLM application site, are not internally available to the expansion. This allows client programmers to freely rename and shadow variables as they are used to, without needing to inspect each parser to determine whether it requires certain modules be bound to particular names at each use site (as is common with, e.g., syntax extensions based on \li{camlp4} or Sugar*, or when using unhygienic term rewriting approaches like OCaml's PPX system or Template Haskell \cite{SheardPeytonJones:Haskell-02}, as further discussed in Sec. \ref{sec:existing-approaches}).

Enforcing this strong context independence condition is quite subtle because TLM parsers need to be able to extract spliced expressions from the literal body and place them in the final expansion. Na\"ively checking that the final expansion is closed except for the explicitly named dependencies would inappropriately enforce context independence on application site spliced expressions, which should certainly not be prevented from referring to variables in the application site context. For example, consider the final expansion of the example from Line 2 of Fig. \ref{fig:first-tlm-example} above:
\begin{lstlisting}[numbers=none]
  Regex.Seq(Regex.Str "GC", Regex.Seq(any_base, Regex.Str "GC"))
\end{lstlisting}
In this term, both \li{Regex} and \li{any_base} are free variables. There is nothing to indicate which free variables came from the literal body via an intentional act of splicing by the TLM. 
%\footnote{An exhaustive search would not be feasible because final expansions are abstract syntax trees. There is not a unique mapping back to concrete syntax, e.g. because whitespace can be inserted arbitrarily.} 
As we will discuss more extensively in Sec. \ref{sec:existing-approaches}, this is why traditional hygienic term-rewriting macro systems, like those available in various Lisp-family languages \cite{mccarthy1978history} and in Scala \cite{ScalaMacros2013}, cannot be used to repurpose string literals for literal notation at other types.

To address this problem, the TLM parser does not generate the final expansion directly. Instead, the parser generates a \emph{proto-expansion} that refers to spliced expressions indirectly by location relative to the start of the provided literal body. For example, the proto-expansion generated by \li{$regex} for the example above can be expressed as follows:
\begin{lstlisting}[numbers=none]
  Regex.Seq(Regex.Str "GC", Regex.Seq(spliced<4; 11; Regex.t>, Regex.Str "GC"))
\end{lstlisting}

Here, \li{spliced<4; 11; Regex.t>} is a reference to the spliced expression \li{any_base} by its location relative to the start of the literal body being expanded. The context independence condition can be enforced straightforwardly on the proto-expansion -- the only free variable in the proto-expansion is \li{Regex}, which is an explicitly listed dependency.
% Requiring that TLMs refer to spliced expressions indirectly in this manner ensures that a TLM cannot ``forge'' spliced terms, i.e. mark some sub-term as being spliced when it does not in fact appear at some specified segment of the literal body.

The set of splice references in the proto-expansion generated for a literal body is called the \emph{segmentation} of that literal body. The system checks that the segmentation does indeed segment the literal body, i.e. that the segments are non-overlapping. For the purposes of abstract reasoning, only the segmentation needs to be reported to downstream editor services, and ultimately to the programmer. The rest of the proto-expansion can be held abstract. In this paper, spliced segment locations are reported using colors, with segments that are not part of spliced segment shown in color (green, so far) while spliced segments start out black. When TLM applications are nested, a distinct color can be used at each depth. For example, in Sec. \ref{sec:more-examples}, we will describe a TLM \li{$html} for HTML notation in the style of Ur/Web (see Fig.~\ref{fig:urweb}). We might apply it, together with another TLM \li{$smiles} for chemical structures described using the SMILES standard \cite{anderson1987smiles}, as follows:
\begin{lstlisting}[numbers=none]
    $html `(SURL<div>Chemical structure of sucrose: {EURL
      Smiles.to_svg
        $smiles `(SCOLOR{ECOLORmono_glucoseSCOLOR}-O-{ECOLORmono_fructoseSCOLOR}ECOLOR)` 
    SURL}</div>EURL)`
\end{lstlisting}

Each splice reference in the segmentation gives not just the location of the spliced segment but also a type annotation. Returning to the regex example above, the specified type annotation is \li{Regex.t}. This type annotation, which must also be context independent, constrains the spliced expression. Consequently, type inference can be performed using only the segmentation together with the type annotation on the applied TLM's definition. For example, consider the function \li{gene_restriction_template} on Lines 3-4 of Fig. \ref{fig:first-tlm-example}. The return type of this function can be determined to be \li{Regex.t} because of the type annotation on the \li{$regex} TLM. The type of the argument, \li{gene}, can be inferred to be \li{string} because the segmentation specifies the type \li{string} for the spliced segment where it appears. Context independence implies that \li{gene} cannot appear elsewhere in the expansion, and so no further typing constraints could possibly be collected from examining the portions of the expansion held abstract. Spliced segment types can be communicated directly to the programmer upon request by an editor service, e.g. \li{merlin} for OCaml/Reason \cite{Merlin}.



---------------------- {\color{red} stuff below is notes / not yet revised}

When a TLM definition appears inside a module, it must also appear in the signature with the same specification, up to the usual notions of type and module path equivalence in the type annotation and module dependencies (like datatype definitions in ML). Note that even when a TLM needs only a single helper function, it must be placed into a named module. This is to avoid needing to reason about the equivalence of arbitrary expressions when determining whether two signatures are compatible. 


Notice that regexes are spliced in using \li{$(}$e$\li{)}, where $e$ is any expression of type \li{Regex.t}, while strings are spliced in using \li{$$(}$e${\li)}, where $e$ is any expression of type \li{string}. These choices are made entire

 Every TLM definition also includes a \emph{type annotation}, here \li{at html}, and a \emph{parse function} between \li{by static \{} and \li{\}}. % The TLM definition follows the same scoping rules as type definitions. We will describe how TLM definitions are packaged into libraries in Sec \ref{sec:static-eval}.



Using this TLM, we can express the Ur/Web example from Figure \ref{fig:urweb} as shown in Figure \ref{fig:first-tsm-example}. On both Lines 1 and 2, we apply 
\li{$html} 
 to a \emph{generalized literal form} delimited by \li{[|} and \li{|]}. Generalized literal forms, which first arose in the prior work on TSLs \cite{TSLs}, syntactically subsume other literal forms because the context-free syntax of the language only specifies the outer delimiters. In this paper, we will use  \li{[|} and \li{|]}, but \citet{TSLs} formally specified several other choices, including layout-sensitive delimitation. \emph{Literal bodies} are constrained only in that \li{[|} and \li{|]} must be balanced.  %In particular, there is no pre-specified syntactic to splice out sub-terms.%The initial context-free parser therefore parses generalized literal forms in much the same way as parsers typically parse ``raw'' string literals (i.e. string literals where escape sequences like \li{\\n} have not yet been processed.) 

 The system delegates responsibility over the parsing and expansion of each literal body to the applied TLM's parse function during a semantic phase called \emph{expansion}, which starts just before and continues into the typing phase. 

 Because the parse function is applied during this phase, rather than at run-time, we call it a \emph{static function}. Static functions  cannot refer to the surrounding variable bindings because those variables stand for run-time values. Instead, there is are separate \emph{static bindings} marked by the \li{static} keyword that populate a \emph{static environment} that is discarded after expansion finishes. A more detailed account of static evaluation, both informal and informal, is given in the supplement. An alternative design that allows for the explicit lowering of standard-phase modules to the static phase has also been proposed for OCaml \cite{Ocaml/macros}. %\ificfp We are closely following this work in our implementation. \else \fi
  % return to in Sec. \ref{sec:static-eval}.

 The input type of the parse function, \li{body}, classifies encodings of literal {bodies}. Literal bodies are sequences of characters, so we define \li{body} as a synonym of \li{string} in \autoref{fig:indexrange-and-parseresult}. The return type is a sum type, defined by applying the parameterized type \li{parse_result} defined in \autoref{fig:indexrange-and-parseresult}, that distinguishes between parse errors and successful parses. Let us consider these two possibilities in turn.

% \vspace{-3px}\paragraph{Parse Errors} 
If the parse function determines that the literal body is not well-formed according to the syntax that it implements, it must return \li{ParseError \{msg=}$e_\text{msg}$\li{, loc=}$e_\text{loc}$\li{\}} 
where $e_\text{msg}$ is a custom error message and $e_\text{loc}$ is a value of type \li{segment}, defined in Figure \ref{fig:indexrange-and-parseresult}, that designates a segment of the literal body as the origin of the error \cite{DBLP:journals/jsc/DeursenKT93}.

% \vspace{-3px}\paragraph{Success} 
If instead parsing succeeds, the parse function returns \li{Success} ~$\ecand$, 
where $\ecand$ is called the \emph{encoding of the proto-expansion}. For expression TLMs, the proto-expansion is a \emph{proto-expression} and it is encoded as a value of the recursive datatype \li{proto_expr} that is outlined in Figure \ref{fig:candidate-exp-Reason}. 
Most of the constructors of \li{proto_expr} are individually uninteresting -- they encode OCaml's various expression forms. 
Expressions can mention types, so we also need the type \li{proto_typ} also outlined in Figure \ref{fig:candidate-exp-Reason}. It is only the \li{SplicedE} and \li{SplicedT} constructors that are novel. These are discussed next.



% \section{Simple Expression TLMs By Example}\label{sec:tsms-by-example}
% We begin in this section with a ``tutorial-style'' introduction to seTLMs in Reason. %In particular, we will define an seTLM for constructing values of the recursive labeled sum type \li{rx} that was defined in Figure \ref{fig:datatype-rx}. 
% Sec. \ref{sec:tsms-minimal-formalism} then formally defines a reduced dialect of Reason called $\miniVerseUE$. This will serve as a ``conceptually minimal'' core calculus of TLMs, in the style of the simply typed lambda calculus.   %We conclude in Sec. \ref{sec:uetsms-discussion} 


% \subsection{TLM Application}\label{sec:uetsms-usage}
% The following Reason expression, drawn textually, is of \emph{TLM application} form. Here, a TLM named \li{$rx} is applied to the \emph{generalized literal form} \li{/SURLA|T|G|CEURL/}:
% \begin{lstlisting}[numbers=none,mathescape=|]
% $rx /SURLA|T|G|CEURL/
% \end{lstlisting}
% Generalized literal forms are left unparsed according to the context-free syntax of Reason. Several other outer delimiters are also available, as summarized in Figure \ref{fig:literal-forms}. The client is free to choose any of these for use with any TLM, as long as the \emph{literal body} (shown in green above) satisfies the requirements stated in Figure \ref{fig:literal-forms}. For example, we could have equivalently written the example above as \li{$rx `SURLA|T|G|CEURL`}. (In fact, this would have been convenient if we had wanted to express a regex containing forward slashes but not backticks.) 

% It is only during the subsequent \emph{typed expansion} phase that the applied TLM parses the {body} of the literal form to generate a \emph{proto-expansion}. The language then \emph{validates} this proto-expansion according to criteria that we will describe in Sec. \ref{sec:uetsms-validation}. If proto-expansion validation succeeds, the language generates the \emph{final expansion} (or more concisely, simply the \emph{expansion}) of the TLM application. The behavior of the program is determined by its expansion. 

% For example, the expansion of the TLM application above is equivalent to the following expression when the regex value constructors \li{Or} and \li{Str} are in scope:
% \begin{lstlisting}[numbers=none]
% Or(Str "SSTRAESTR", Or(Str "SSTRTESTR", Or(Str "SSTRGESTR", Str "SSTRCESTR")))
% \end{lstlisting}
% To avoid the assumption that the variables \li{Or} and \li{Str} are in scope at the TLM application site, the expansion actually uses the explicit \li{fold} and \li{inj} operators, as described in Sec. \ref{sec:lists}. In fact, the proto-expansion validation process enforces this notion of context independence -- we will return to proto-expansion validation below. (We will show how TLM parameters can reduce the awkwardness of this requirement in Chapter \ref{chap:ptsms}.)
% %The constructors above are those of the type \li{Rx} that was defined in Figure \ref{fig:datatype-rx}.

% % A number of literal forms, ,  are available in Reason's concrete syntax. Any literal form can be used with any TLM,  TLMs have access only to the literal bodies. Because TLMs do not extend the concrete syntax of the language directly, there cannot be syntactic conflicts between TLMs.

%  %The form does not directly determine the expansion. 

% \begin{figure}
% \begin{lstlisting}
% 'SURLbody cannot contain an apostropheEURL'
% `SURLbody cannot contain a backtickEURL`
% [SURLbody cannot contain unmatched square bracketsEURL]
% {|SURLbody cannot contain unmatched barred curly bracesEURL|}
% /SURLbody cannot contain a forward slashEURL/
% \SURLbody cannot contain a backslashEURL\
% \end{lstlisting}
% %SURL<tag>body includes enclosing tags</tag>EURL
% \caption[Available Generalized Literal Forms]{Generalized literal forms available for use in Reason's textual syntax. The characters in green indicate the literal bodies and describe how the literal body is constrained by the form shown on that line. The Wyvern language defines additional forms, including whitespace-delimited forms \cite{TSLs} and multipart forms \cite{sac15}, but for simplicity we leave these out of Reason.}
% \label{fig:literal-forms}
% \end{figure}

\subsection{Splicing}\label{sec:splicing-and-hygiene}
When the parse function determines that some segment of the literal body is a spliced expression, according to whatever syntactic criteria it deems suitable, it can indirectly refer to it in the encoding it produces using the \li{SplicedE} constructor of \li{proto_expr}, which takes a value of type \li{segment} that indicates the zero-indexed location of the spliced expression relative to the start of the provided literal body. The \li{SplicedE} constructor also requires a value of type \li{proto_typ}, which indicates the type that the spliced expression is expected to have. Types can be spliced out by using the \li{SplicedT} constructor of \li{proto_typ} analagously.

\begin{figure}
\begin{lstlisting}[numbers=none,xleftmargin=0pt]
type body = string;
type segment = {startIdx: int, endIdx: int};
type parse_result('a) 
  = ParseError {msg: string, loc: segment}
  | Success('a);
type proto_typ = Arrow(proto_typ, proto_typ)
               | StringTy
               | /* ... */ 
               | SplicedT(segment);
type proto_expr = Tuple(list(proto_expr))
                | /* ... */
                | SplicedE(segment, proto_typ);
\end{lstlisting}
\caption[Definitions of various types used by TLM definitions.]{Definitions of various types available ambiently to TLM definitions.}
\label{fig:indexrange-and-parseresult}
\label{fig:candidate-exp-Reason}
\end{figure}

For example, consider again the two TLM applications in Figure \ref{fig:first-tsm-example}. In each case, the parse function of the \li{$html} TLM (Figure \ref{fig:html-tlm-def}, Lines 2-4) first sends the literal body through an off-the-shelf HTML parser, \li{parse_html}. It then passes the result to a function \li{html_to_ast}, not shown, which produces the corresponding expression encoding of type \li{proto_expr}. When this function encounters an HTML text node containing matched \li{\{[} and \li{]\}}, then that segment is inserted as a spliced expression of type \li{string}, and similarly text nodes containing matched curly braces produce a spliced expression of type \li{html}. For instance,  the proto-expansion generated for first TLM application in \ref{fig:first-tsm-example}, pretty-printed, is:
\begin{lstlisting}[numbers=none]
    H1Element(Nil, Cons(TextNode "Hello, ", Cons(
      TextNode spliced<13; 22; string>, Nil)))
\end{lstlisting}
Here, \li{spliced<13; 22; string>} is a reference to the spliced string expression \li{first_name} by its location relative to the start of the literal body being expanded (the off-the-shelf HTML parser provides the necessary baseline location information for use by \li{html_to_ast}). It corresponds to the encoding \li{SplicedE(\{startIdx=13, endIdx=22\}, StringTy)}. Requiring that TLMs refer to spliced expressions indirectly in this manner ensures that a TLM cannot ``forge'' spliced terms, i.e. claim that some sub-term of the expansion should be given the privileges of a spliced term, discussed in Sec. \ref{sec:uetsms-validation}, when it does not in fact appear in the literal body.% Our semantics distinguishes spliced expressions when validating the proto-expansion, which is important for reasons of hygiene that we will detail shortly. 


% The parse function can similarly extract \emph{spliced types} from a literal body using the \li{SplicedT} variant of \li{proto_typ}. %In particular, the parse function must provide the index range of spliced subexpressions to the \li{Spliced} constructor of the type \li{MarkedExp}. %Only subexpressions that actually appear in the body of the literal form can be marked as spliced subexpressions.

%For example, had the  would not be a valid expansion, because the  that are not inside spliced subexpressions:
%\begin{lstlisting}[numbers=none]
%Q.Seq(Q.Str(name), Q.Seq(Q.Str ": ", ssn))
%\end{lstlisting}


% \subsection{Splice Summaries and Segmentations}
The \emph{segmentation} inferred from a proto-expansion is the finite set of references to spliced terms contained within. For example, the segmentation inferred from the proto-expression above contains only \li{spliced<13; 22; string>}. % Notice that no information about  the spliced terms appear is communicated by the splice summary.
The system checks that all of the locations in the segmentation are 1) in bounds relative to the literal body; and 2) non-overlapping. 
This resolves the problem of \textbf{Segmentation} described in Sec. \ref{sec:intro}, i.e. every literal body in a well-formed program has a well-defined segmentation. The TSL mechanism  did not maintain this reasoning principle \cite{TSLs}. A program editor or pretty-printer can communicate this segmentation information to the programmer, e.g. by coloring non-spliced segments green as is our convention in this document. In general, spliced expressions might themselves apply TLMs, in which case the convention is to use a distinct color for unspliced segments at each depth. For example, consider a TLM \li{$smiles} for chemical structures \cite{anderson1987smiles} with support for splicing using curly braces:
\begin{lstlisting}[numbers=none]
    $html [|SURLChemical structure of sucrose: {EURL
        $smiles [|SCSS{ECSSm_glucoseSCSS}-O-{ECSSm_fructoseSCSS}ECSS|] 
        |> SMILES.to_svg SURL}EURL|]
\end{lstlisting}
%For example, if strings were not primitive but rather defined as sequences of characters, we might define a TLM \li{$str} to recover string literal notation and write \li{$html [|SURL<body>\{EURLheading $str [|SSTRWorld!ESTR|]SURL\} ...</body>EURL|]}.

A program editor or pretty-printer can communicate the type of each spliced expression, also specified abstractly by the segmentation, upon request (for Reason, via Merlin \cite{Merlin}.) %When there is a type mismatch, the type annotation on the spliced segment also allows the error message to report the problem without revealing the full expansion.

\subsection{Proto-Expansion Validation}\label{sec:uetsms-validation}
Three important concerns described in Sec. \ref{sec:intro} remain: those related to reasoning abstractly about the \emph{hygiene properties}, i.e. \textbf{Capture} and \textbf{Context Dependence}, and \textbf{Typing}. Addressing these concerns is the purpose of the \emph{proto-expansion validation} process, which occurs during the typing phase. 
Proto-expansion validation results in the \emph{final expansion}, which is simply the proto-expansion with the references to spliced segments replaced with their own final expansions. 


\subsubsection{Capture}\label{sec:capture}

Proto-expansion validation ensures that spliced terms have access \emph{only} to the bindings at the application site---spliced terms cannot capture bindings internal to the proto-expansion. For example, consider the following application site:
\begin{lstlisting}[numbers=none]
    let tmp = /* ... application site temp ... */;
    $html [|SURL<h1>{EURLf(tmp)SURL}</h1>EURL|];
\end{lstlisting}
Now consider the scenario where the proto-expansion generated by \li{$html} has the following form:
\begin{lstlisting}[numbers=none]
    let tmp = /* ... expansion-internal temp ... */;
    H1Element(tmp, spliced<5; 10; html>);
\end{lstlisting}
Na\"ively, the binding of the variable \li{tmp} in the proto-expansion could shadow the application-site binding of \li{tmp} in the final expansion. 
To address this problem, splicing is guaranteed to be capture-avoiding. When generating the final expansion, the system discharges the requirement that capture not occur by implicitly alpha-varying the bindings in the proto-expansion as needed. There is no need for TLM providers to explicitly deploy a mechanism that generates fresh variables (as in, e.g., prior \emph{reader macro} systems \cite{Flatt:2012:CLR:2063176.2063195}.
% For example, the final expansion of the example above might take the following form:
%\begin{lstlisting}[numbers=none]
%  let tmp = /* ... application site temporary ... */; 
%  let tmp' = /* ... expansion-internal temporary ... */;
%  H1Element(tmp', f(tmp));
%\end{lstlisting}
%Notice that the expansion-internal binding of \li{tmp} has been alpha-varied to \li{tmp'}. The reference to \li{tmp} in the spliced expression then refers, as intended, to the application site binding. 

% For TLM providers, the benefit of this mechanism is that they can name the variables used internally within expansions freely. TLM clients can, in turn, can reason abstractly about \textbf{Capture}.

Capture avoidance does prevent library providers from intentionally introducing bindings into spliced terms. For example, Haskell's literal notation for monadic values, i.e. \li{do}-notation, cannot be expressed because it introduces a new binding syntax \cite{jones2003haskell}.

In our initial designs, we were able to express this example because we distinguished spliced identifiers, much as we do spliced types and expressions. These could be explicitly bound inside spliced expressions by extending the \li{SplicedE} and \li{SplicedT} constructors to take finite sets of spliced identifiers. This is conceptually similar to the approach taken by \citet{DBLP:conf/esop/HermanW08} for giving (already parsed) sub-terms access to internal variables. However, we ultimately decided to remove this feature because it would disproportionately increase the reasoning burden on clients when they encounter an unfamiliar literal: \emph{might this obscure literal also be introducing bindings?} It is difficult for a program editor to display a set of ``hidden bindings'' to the programmer. Haskell-style infix notation for monadic values, where bind is \li{e >>= f}, can be expressed using TLMs without support for spliced identifiers. Reason 3.0 is adopting more concise function syntax, which will decrease the cost of this syntax.
%We will show an alternative formulation of Haskell's syntax for monadic commands that uses Reason's anonymous function syntax to bind variables in Sec. \ref{sec:application-monadic-commands}. 

\subsubsection{Context Dependence}\label{sec:context-dependence}
%The prohibition on shadowing ensures only that variables that appear in spliced terms do not refer to bindings that appear in the surrounding expansion. 
The proto-expansion validation process also ensures that variables that appear in the proto-expansion do not refer to bindings that appear either at the TLM definition or the application site. In other words, expansions must be completely \emph{context independent} -- they can make no assumptions about the surrounding context whatsoever. 

A minimal example of a ``broken'' TLM that does not generate context-independent proto-expansions is below:
\begin{lstlisting}[numbers=none]
    syntax $broken at t by static { 
      fun(_) => Success (Var "SSTRxESTR") };
\end{lstlisting}
The proto-expansion that this TLM generates (for any literal body) refers to a variable \li{x} that it does not itself bind. If proto-expansion validation permitted such a proto-expansion, it would be well-typed only under those application site typing contexts where \li{x} is bound. This ``hidden assumption'' makes reasoning about binding and renaming difficult.

Of course, this prohibition does not extend into the spliced terms in a proto-expansion -- spliced terms appear at the application site, so they can justifiably refer to application site bindings. (like \li{first_name} in Fig. \ref{fig:first-tsm-example}.) Because proto-expansions refer to spliced terms indirectly, enforcing context independence is straightforward -- we need only that the proto-expansion itself be closed.% In the next section, we will formalize this intuition. % The TLM provider can only refer to them opaquely.

Na\"ively, this restriction, also present in the prior work on TSLs \cite{TSLs}, is quite restrictive -- expansions cannot access any library functions. At best, they can require the client to ``pass in'' required library functions via splicing at every application. In Sec. \ref{sec:ptsms}, we will introduce module parameters and partial parameter application to neatly resolve this problem. 


% This prohibition on context dependence explains why the expansion generated by the TLM application in Sec. \ref{sec:uetsms-usage} cannot make use of the regex value constructors, e.g. \li{Str} and \li{Or}, directly. (In Chapter \ref{chap:ptsms}, we will relax this restriction to allow proto-expansions to access explicit parameters.)

% Collectively, we refer to the prohibition on capture and the prohibition on context dependence as \emph{hygiene properties}, by conceptual analogy to corresponding properties in term-rewriting macro systems (see Sec. \ref{sec:macro-systems}.) The novelty here comes from the fact that spliced terms are being extracted from an initially unparsed sequence of characters.
% In the examples in Sec. \ref{sec:uetsms-usage} and Sec. \ref{sec:splicing-and-hygiene}, the expansion used constructors associated with the \li{Rx} type, e.g. \li{Seq} and \li{Str}. This might appear to violate our prohibition on context-dependent expansions. This is not the case only because in Reason, constructor labels are not variables or scoped symbols. Syntactically, they must begin with a capital letter (like Haskell's datatype constructors). Different labeled sum types can use common constructor labels without conflict because the type the term is being checked against -- e.g. \li{Rx}, due to the type ascription on \li{$rx} -- determines which type of value will be constructed. For dialects of ML where datatype definitions do introduce new variables or scoped symbols, we need parameterized TLMs. We will return to this topic in Chapter \ref{chap:ptsms}. % Indeed, we used the label \li{Spliced} for two different recursive labeled sum types in Figure \ref{fig:candidate-exp-Reason}.

\subsubsection{Typing}\label{sec:typing-e}
Finally, proto-expansion validation maintains a reasonable {typing discipline} by (1) checking that the expansion is of the type specified by the TLM's type annotation; (2) checking that each spliced type is valid; (3) checking that the type annotation on each spliced expression is valid; and (4) checking each spliced expression against the specified type annotation. Context independence implies that ML-style type inference can be performed using only the segmentation (because the remainder of an expansion cannot mention the very variables whose types are being inferred). In the prior work on TSLs, spliced terms did not have type annotations 
%The OCaml type system is not strong enough to allow us to express only contextually well-typed syntax trees. In The details are discussed as future work in Sec. \ref{sec:discussion}.
 % This addresses the problem of reasoning abstractly about \textbf{Typing} described in Sec. \ref{sec:intro}, i.e.:
 % \begin{enumerate}
 %   \item determining the type of an expansion requires examining only the type annotation on the TLM definition (much as determining the type of a function application requires examining only the function's type); and 
 % \item determining the type that a spliced expression must have requires only the information in the splice summary (rather than complete knowledge of the proto-expansion).
 % \end{enumerate}

% The language \emph{validates} proto-expansions before a final expansion is generated. One aspect of proto-expansion validation is checking  the proto-expansion against the type annotation specified by the TLM, e.g. the type \li{Rx} in the example above. This maintains a \emph{type discipline}: if a programmer sees a TLM being applied when examining a well-typed program, they need only look up the TLM's type annotation to determine the type of the generated expansion. Determining the type does not require examining the expansion directly.


% \subsection{Hygiene}
% The spliced subexpressions that the proto-expansion refers to (by their position within the literal body, cf. above) must be parsed, typed and expanded during the proto-expansion validation process (otherwise, the language would not be able to check the type of the proto-expansion). To maintain a useful \emph{binding discipline}, i.e. to allow programmers to reason also about variable binding without examining expansions directly, the validation process maintains two additional properties related to spliced subexpressions: \textbf{context independent expansion} and \textbf{expansion independent splicing}. These are collectively referred to as the \emph{hygiene properties} (because they are conceptually related to the concept of hygiene in term rewriting macro systems, cf. Sec. \ref{sec:term-rewriting}.) 

% \paragraph{Context Independent Expansion} 

% \paragraph{Expansion Independent Splicing} 
% %These properties suffice to ensure that programmers and tools can freely rename a variable without changing the meaning of the program. The only information that is necessary to perform such a \emph{rename refactoring} is the locations of spliced subexpressions within all the literal forms for which the variable being renamed is in scope; the expansions need not otherwise be examined. It would be straightforward to develop a tool and/or editor plugin to indicate the locations of spliced subexpressions to the user, like we do in this document (by coloring spliced subexpressions black). We discuss tool support as future work in Sec. \ref{sec:interaction-with-tools}.

% \subsubsection{Final Expansion}

% For example, the final expansion of the body of \li{lookup_rx} is equivalent to the following, under an environment where the regex value constructors are available:
% \begin{lstlisting}[numbers=none]
% Seq(Str(name), Seq(Str "SSTR: ESTR", ssn))
% \end{lstlisting}
% (Again, due to the prohibition on context dependent expansions, the final expansion actually involves explicit \li{fold} and \li{inj} operators.)


\newcommand{\spTLMsSec}{Pattern TLMs}
\section{\protect\spTLMsSec}
\label{sec:sptsms}

Let us now briefly consider the topic of TLMs that generate \emph{patterns}, rather than expressions. Pattern literals are the dual to expression literals in that expression literals support construction whereas pattern literals support deconstruction \cite{moriconi2008inversion}. For example, we can pattern match on a value \li{x : html} by applying a pattern TLM \li{$html} as follows:
\begin{lstlisting}[numbers=none]
    switch x {
    | $html [|SURL<h1>{EURLcsSURL}</h1>EURL|] => /* cs:list(html) */
    | _ -> None
    };
\end{lstlisting}
Any list pattern, including one generated by another TLM application, can appear where \li{cs} appears in the example pattern above. For longer \li{switch} expressions, the shorthand \li{switch x using $html} applies \li{$html} to every rule where the outermost pattern is of generalized literal form.

Notice that we can use the same name, \li{$html}, for this pattern TLM as for the expression TLM defined in the previous section. It does not make sense to apply an expression TLM in pattern position  (many expression-level constructs, e.g. lambdas, do not correspond even syntactically to patterns), and \emph{vice versa}, so this is unambiguous.

Pattern TLM definitions differ from expression TLM definitions in two ways: (1) a \emph{sort qualifier}, \li{for patterns}, distinguishes them from expression TLM definitions; (2) the return type of the parse function is \li{parse_result(proto_pat)}, rather than \li{parse_result(proto_expr)}. The type \li{proto_pat}, outlined below, classifies encodings of \emph{proto-patterns}.
\begin{lstlisting}[numbers=none]
   type proto_pat = /* no variable pattern form! */
                  | Wild
                  | /* ... */
                  | SplicedP(segment, proto_typ);
\end{lstlisting}

% \subsection{Splicing}
The constructor \li{SplicedP} operates much like \li{SplicedE} to allow a proto-pattern to refer indirectly to spliced patterns.% by their location within the literal body.


%  For example, the proto-pattern generated for the example at the top of this section would be written concretely as follows:
% \begin{lstlisting}[numbers=none]
%   H1Element (_, spliced<6; 7; list(html)>)
% \end{lstlisting}


% \subsection{Proto-Pattern Validation}
% Proto-pattern validation serves, like proto-expression validation, to maintain the ability to reason abstractly about binding and typing. 

To maintain the abstract binding discipline, variable patterns can appear only within spliced patterns. Enforcing this restriction is straightforward: we simply have not defined a variant of the \li{proto_pat} type that encodes variable patterns (wildcards are allowed.)  This restriction ensures that only variables visible to the client in a spliced pattern are bound in the corresponding branch expression. This is analagous to the capture avoidance principle for expression TLMs.

Type annotations on references to spliced patterns could refer to type variables, so we also need to enforce context independence in the manner discussed in the previous section. %Pattern guards are part of pattern matching rules, not patterns themselves, in Reason/OCaml, but were they part of patterns, context independence would also need to be enforced for guard expressions.% In languages like OCaml, which support arbitrary boolean guard expressions, we would also need to enforce both context independence and capture avoidance as discussed in the previous section for spliced expressions that end up in a guard expression. In our formalism, we do not support guard expressions.

% \subsubsection{Typing} 
To maintain an abstract typing discipline, proto-pattern validation checks type annotations much as in Sec. \ref{sec:typing-e}.% (1) that the final expansion is a pattern that matches values of the type specified by the TLM's type annotation; (2) that each spliced pattern matches values of the type indicated in the segmentation; and (3) that each of these types are themselves well-formed types.

\newcommand{\pTLMsSec}{Parametric TLMs}
\section{\protect\pTLMsSec}
\label{sec:ptsms}

The simple TLMs in the previous sections operate only at one specified type, as did the TSLs in the prior work \cite{TSLs}. This is rather limiting. This section introduces \emph{parametric TLMs}, which can operate over a type- and module-parameterized family of types. They also neatly solve the problem discussed in Sec. \ref{sec:context-dependence} of giving expansions access to helper functions.
%Moreover, as discussed at the end of Sec. \ref{sec:context-dependence}, the expansions that they generate have no access to libraries. This section introduces \emph{parametric TLMs} to neatly resolve both of these limitations (which also limited the prior work on TSL \cite{TSLs}.) Parametric TLMs can be defined over a type- and module-parameterized family of types, and the proto-expansions they generate can refer to supplied type and module parameters. Partial parameter application decreases the syntactic cost of this explicit parameter passing style. 
% \subsection{Parametric TLMs By Example}\label{sec:ptsms-by-example}

Consider the following Reason/OCaml module type (a.k.a. signature), which specifies an abstract data type \cite{liskov1974programming,harper1997programming} of string-keyed polymorphic dictionaries:
\begin{lstlisting}[numbers=none]
    module type DICT = {
      type t('a); 
      let empty : t('a); 
      let extend : t('a) -> (string, 'a) -> t('a); 
      /* ... */ 
    };
\end{lstlisting}
We can define a TLM that is parametric over implementations of this signature, \li{D : DICT}, and over choices of the codomain type, \li{'a} as follows:
\begin{lstlisting}[numbers=none]
    syntax $dict (D : DICT) (type 'a) at D.t('a) 
    by static { fun(b) => /* ... */ };
\end{lstlisting}
For example, given some module that implements this signature, \li{HashDict : DICT}, we can apply \li{$dict} as follows:
\begin{lstlisting}[numbers=none]
    $dict HashDict int [|"key1"SURL=>EURL10SURL; EURL"key2"SURL=>EURL15|]
\end{lstlisting}
Notice that the segmentation immediately reveals which punctuation is particular to this TLM and where the spliced key and value expressions appear. Because the context-free syntax of unexpanded terms is never modified, it is possible to reason modularly about syntactic determinism, i.e. we can reason above that \li{=>} does not appear in the follow set of unexpanded expressions \cite{conf/pldi/SchwerdfegerW09}, so there can never be an ambiguity about where a key expression ends.

The proto-expansion generated for the TLM application above, shown below, can refer to the parameters:
\begin{lstlisting}[numbers=none]
    D.extend(D.extend D.empty (spliced<1;6;string>, 
      spliced<11;12;'a>)) (spliced<15;20;string>, 
      spliced<25;26;'a>)
\end{lstlisting}
Validation checks that the proto-expansion is truly parametric, i.e. it must be valid for all modules \li{D : DICT} and types \li{'a}. It is only after validation that we substitute the actual parameters, here \li{HashDict} for \li{D} and \li{int} for \li{'a}, into the final expansion. Only the type annotations on references to spliced terms are subject to early parameter substitution (because they classify application site terms.)

If we will use the \li{HashDict} implementation ubiquitously, we can abbreviate the partial application of \li{$dict} to \li{HashDict}, resulting in a TLM that is parametric over only the type \li{'a}:
\begin{lstlisting}[numbers=none]
    syntax $dict' = $dict HashDict; 
\end{lstlisting}
TLM abbreviations can themselves be parameterized to support partial application of parameters other than the last.

In Sec. \ref{sec:context-dependence} we discussed the problem of the strict context independence discipline being too restrictive, in that it would seem to restrict expansions from referring to useful helpers bound at the TLM definition site. Module parameters address this problem -- the helper values, types and modules can be packaged into a module, passed in and partially applied to hide this detail from clients. Because this will be common in practice, we provide the following shorthand:
\begin{lstlisting}[numbers=none]
    syntax $a at t using X~${}_1$~=M~${}_1$~,...,X~${}_n$~=M~${}_n$~ by ...
\end{lstlisting} 
% This is the same as a definition that specifies $n$ type, expression or module parameters and immediately applies the given arguments, binding the result to the same TSM name. 

This explicit parameter passing discipline is reminiscent of work on explicit capture for distributed functions \cite{DBLP:conf/ecoop/MillerHO14}. By not implicitly giving expansions access to all definition-site bindings, we need not examine parse functions to reason about, e.g., renaming. Consequently, encodings of proto-terms can be values of standard datatypes (e.g. \li{proto_expr}) with variables represented simply as, e.g., strings, and quasiquotation notation can be expressing using TLMs (see supplement). A central design goal of Reason is to leave the OCaml semantics unchanged, so building quasiquotation in primitively, to integrate the free variables in term encodings into the overall binding discipline \cite{Bawd99a}, was in any case infeasible.


\newcommand{\moreExamplesSec}{Additional Examples}
\section{\protect\moreExamplesSec}
\label{sec:more-examples}

\subsection{HTML}
We start in this section by describing the TLM mechanism in detail by way of a substantial case study: literal notation, like that built in to Ur/Web (see Figure \ref{fig:urweb}) and also, presently, Reason, for expressions of the type  \li{html} in Figure \ref{fig:html-type-def}.

\begin{figure*}[t]

\begin{subfigure}[t]{0.48\textwidth}
\begin{lstlisting}
  type html_attrs = list(string, string)
  type html = BodyElement(html_attrs, list(html))
            | H1Element(html_attrs, list(html))
            | TextNode(string) 
            | /* ... */;
\end{lstlisting}
\caption{The \li{html} type, which classifies encodings of HTML data.}
\label{fig:html-type-def}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
\begin{lstlisting}[mathescape=|]
  syntax $html at html by static {
    fun(b : body) : parse_result(proto_expr) => 
      try (parse_html b |> html_to_ast |> Success) {
      | InvalidHTML msg loc => ParseError msg loc }
  };
\end{lstlisting}
\caption{The \li{$html} TLM definition. Figure \ref{fig:candidate-exp-Reason} defines   TLM-related types.}
\label{fig:html-tlm-def}
\end{subfigure}

\begin{subfigure}[t]{\textwidth}
\begin{lstlisting}
  let heading first_name = $html [|SURL<h1>Hello, {[EURLfirst_nameSURL]}!</h1>EURL|]
  let body = $html [|SURL<body>{EURLheading "World!"SURL} ...</body>EURL|]
\end{lstlisting}
\caption{Two examples of the \li{$html} TLM being applied. Compare to Ur/Web's built in HTML literal notation from Figure \ref{fig:urweb}.  %In each case, literal body, between backticks, is initially left unparsed according to the language's context-free syntax. %The applied TLM determines a segmentation and expansion during the typed expansion phase, which generalizes the usual typing phase.
}
\label{fig:first-tsm-example}
\end{subfigure}
\caption{Case Study: HTML literals}
\end{figure*}

\subsection{Quotation}

\subsection{More Examples, Briefly}
% 

\begin{figure}[t!]
\begin{minipage}{\textwidth}
\small
$\arraycolsep=2pt\begin{array}{llcl}
\mathsf{UTyp} & \utau & ::= & 
\ut ~\vert~ 
\parr{\utau}{\utau} ~\vert~
\forallt{\ut}{\utau} ~\vert~
\rect{\ut}{\utau}\\
& & \vert & 
\prodt{\mapschema{\utau}{i}{\labelset}} ~\vert~
\sumt{\mapschema{\utau}{i}{\labelset}}\\
\mathsf{UExp} & \ue & ::= & 
\ux ~\vert~
% \asc{\ue}{\utau} ~\vert~
% \letsyn{\ux}{\ue}{\ue} & \text{value binding}\\
\lam{\ux}{\utau}{\ue} ~\vert~
\ap{\ue}{\ue} ~\vert~
\Lam{\ut}{\ue} ~\vert~
\App{\ue}{\utau} ~\vert~
\fold{\ue} \\
% \unfold{\ue} ~\vert~
&& \vert & \tpl{\mapschema{\ue}{i}{\labelset}} ~\vert~
\inj{\ell}{\ue} ~\vert~
\matchwith{\ue}{\seqschemaX{\urv}}
 \\
& & \vert & \uesyntaxq{\tsmv}{\utau}{e}{\ue} \\   
& & \vert & \usyntaxup{\tsmv}{\utau}{e}{\ue} \\
&&\vert&  \utsmap{\tsmv}{b} \\
\mathsf{URule} & \urv & ::= & 
%& \aumatchrule{\upv}{\ue} 
\matchrule{\upv}{\ue} \\
\mathsf{UPat} & \upv & ::= & 
%& \ux 
\ux ~\vert~
\wildp ~\vert~ 
\foldp{\upv} ~\vert~
\tplp{\mapschema{\upv}{i}{\labelset}} ~\vert~
\injp{\ell}{\upv} ~\vert~
\utsmap{\tsmv}{b}

% \LCC  &  & 
% %& \lightgray 
% & \color{Yellow} & \color{Yellow} \\
% &&
% %& \audefuetsm{\utau}{e}{\tsmv}{\ue} 
% & \uesyntax{\tsmv}{\utau}{e}{\ue} & \text{seTLM definition}\\ 
% &&
% %& \autsmap{b}{\tsmv} 
% & \utsmap{\tsmv}{b} & \text{seTLM application}\ECC
\end{array}$
\end{minipage}
\caption[Syntax of the $\miniVersePat$ unexpanded language (UL)]{Syntax of the $\miniVersePat$ unexpanded language (UL). Metavariable $\ut$ ranges over type identifiers, $\ux$ over expression identifiers, $\ell$ over labels, $\labelset$ over finite sets of labels, $\tsmv$ over TLM names and $b$ over literal bodies. We write $\mapschema{\utau}{i}{\labelset}$ for a finite mapping of each label $i$ in $\labelset$ to some unexpanded type $\utau_i$, and similarly for other sorts. We write $\seqschemaX{\urv}$ for a finite sequence of $n$ unexpanded rules.
}
\label{fig:U-unexpanded-terms}
\end{figure}
\begin{comment}
\newcommand{\staticEvalSec}{Static Evaluation}
\section{\protect\staticEvalSec}
\label{sec:static-eval}

% It is important to note that module parameters are accessible by expansions, but not by parse functions directly, because the applied module parameters will not have been dynamically instantiated when typed expansion occurs. We will discuss a distinct mechanism for providing helper functions to parse functions in Sec. \ref{sec:static-eval}. 

There is one more major impracticality from the prior work on TSLs \cite{TSLs} that we will now briefly address: in the prior work, parse functions had no access to any libraries, nor any ability to themselves apply TLMs. To address this problem, we introduce a \emph{static environment}. 

\subsection{The Static Environment}
Figure \ref{fig:static-module-example} shows an example of a module, \li{ParserCombos}, that defines a number of parser combinators following \citet{Hutton1992d}. The \li{static} qualifier indicates that this module is bound for use only within similarly qualified values, including in particular parse functions of subsequent TLM definitions.
\begin{figure}[h]
\begin{lstlisting}[numbers=none]
  static module ParserCombos = { 
    type p('c, 't) = list('c) -> list('t, list('c));
    let alt : p('c, 't) -> p('c, 't) -> p('c, 't);
    /* ... */
  };
  syntax $a at t by static { fun(b) => 
    ParserCombos.alt /* ... OK */ };
  static let y = ParserCombos.alt /* ... OK */;
  let z = /* ... ParserCombos nor y bound here */;
\end{lstlisting}
\caption{Binding static modules and values for use within parse functions.}
\label{fig:static-module-example}
\end{figure}

The values that arise when the static phase runs do not persist from ``compile-time'' to ``run-time'', so we do not need a full staged computation system, e.g. as described by \citet{Taha99multi-stageprogramming:}. Instead, a sequence of static bindings operates like a read-evaluate-print loop (REPL) scoped according to the program structure, in that each static expression is evaluated immediately and the evaluated values are tracked by a \emph{static environment}, which is discarded after expansion finishes. We do not restrict the language features available to the static phase, though this is a worth considering in languages where that is more straightforward (e.g. to ensure deterministic builds.)

% A language designer might choose to restrict the external effects available to static terms in some way, e.g. to ensure deterministic builds. It might also be helpful to restrict mutable state shared between TLMs to prevent undesirable TLM application order dependencies. On the other hand, these features, if used for the purposes of caching, might speed up typed expansion. These are orthogonal design decisions.

\subsection{Applying TLMs Within TLM Definitions}\label{sec:tsms-for-tsms}
TLMs and TLM abbreviations can also be qualified with the \li{static} keyword, which marks them for use within subsequent static expressions and patterns. Let us consider some examples of relevance to TLM providers.

\subsubsection{Quasiquotation}\label{sec:quasiquotation}
TLMs must construct values of type \li{proto_expr} or \li{proto_pat}. Constructing values of these types explicitly can have high syntactic cost. To decrease this cost, we can define TLMs that provide support for \emph{quasiquotation syntax} similar to that built in to languages like Lisp \cite{Bawd99a} and Scala \cite{shabalin2013quasiquotes}. The following TLM defines quasi-quotation for encodings of proto-expressions:
\begin{lstlisting}[numbers=none]
  static syntax $proto_expr at proto_expr by static { /* ... */ };
\end{lstlisting}
For example, \li{$proto_expr `SQTg x ^EQTx`} might have expansion \li{App(App(Var "g", Var "x"), x)}. Notice that prefixing a variable (or parenthesized expression) with \li{^} serves to splice in its value, which here must be of type \li{proto_expr} (though in other syntactic positions, it might be \li{proto_typ}.) This is also known as \emph{anti-quotation}.

% A similar approach can be taken for working with encodings of terms of other languages (e.g. when writing an interpretter or compiler in Reason.)

\subsubsection{Parser Generators}
Abstractly, a grammar-based parser generator is a module matching the signature \li{PARSEGEN} defined below:
\begin{lstlisting}
  module type PARSEGEN = { 
    type grammar('a);
    /* ... operations on grammars ... */
    val generate : grammar('a) -> (body -> parse_result('a));
  };
\end{lstlisting}

Rather than constructing a grammar equipped with semantic actions using the associated operations (whose specifications are elided in \li{PARSEGEN}), we wish to use a syntax for context-free grammars that follows standard conventions. We can do so by defining a static parametric TLM:
\begin{lstlisting}[numbers=none]
  static syntax $grammar(P : PARSEGEN)(type 'a) at P.grammar('a) /*...*/
\end{lstlisting}
To support splicing, we need non-terminals that recognize unexpanded terms and produce the corresponding splice references, rather than the AST itself. This requires that the parser generator keep track of location information (as most production-grade parser generators already do for error reporting.) A more detailed example of this mechanism being used to define a TLM for a modular encoding of regular expressions is given in the supplement.% For spliced expressions, this non-terminal would need to be a family of non-terminals indexed by a value of type \li{proto_typ}. 

A grammar containing such non-terminals can serve as a \emph{summary specification} -- a human can simply be take this grammar as a specification of what the segmentation will be for every recognized string, rather than relying on an editor to communicate this information. The associated semantic actions can be held abstract as long as the system performs a simple check to ensure that the proto-expansion does mention each spliced expression from the corresponding production.

% \begin{figure}[h!]
% \begin{lstlisting}[deletekeywords={as}]
% syntax $rx(R : RX) at R.t by static 
%   P.generate ($grammar P proto_expr {|SHTML #\label{line:rx_parse_fn_start}#
%     start <- ""
%       EHTMLfn () => $proto_expr `SCSSR.EmptyECSS`SHTML
%     start <- "(" start ")"
%       EHTMLfn e => eSHTML
%     token str_tok #\label{line:str_tok_start}#
%       EHTMLRU.parse "SSTR[^(@$]+ESTR" /* cannot use $rx within its own def */SHTML #\label{line:str_tok_end}#
%     start <- str_tok
%       EHTMLfn s => $proto_expr `SCSSR.Str %(ECSSstr_to_proto_lit sSCSS)ECSS`SHTML
%     start <- start start
%       EHTMLfn e1 e2 => $proto_expr `SCSSR.Seq (%ECSSe1SCSS, %ECSSe2SCSS)ECSS`SHTML
%     start <- start "|" start 
%       EHTMLfn e1 e2 => $proto_expr `SCSSR.Or (%ECSSe1SCSS, %ECSSe2SCSS)ECSS`SHTML
%     start <- start "*"
%       EHTMLfn e => $proto_expr `SCSSR.Star %ECSSe`SHTML

%     using EHTMLspliced_uexp ($proto_typ `SCSSR.tECSS`) SHTML as spliced_rx #\label{line:splicede_using}#
%     start <- "${" spliced_rx "}" #\label{line:splicing-start}#
%       EHTMLfn e => eSHTML

%     using EHTMLspliced_uexp ($proto_typ `SCSSstringECSS`) SHTML as spliced_str
%     start <- "@{" spliced_str "}"
%       EHTMLfn e => $proto_expr `SCSSR.Str %(ECSSeSCSS)ECSS`SHTML #\label{line:splicing-end}#
%   EHTML|})
% end #\label{line:rx_parse_fn_end}#
% \end{lstlisting}
% \caption{A grammar-based definition of \texttt{\$rx}.}
% \label{fig:rx-grammar-based}
% \end{figure}

\subsection{Static Evaluation, Formally}
It is not difficult to extend $\miniVerseParam$ to account for static evaluation. Static environments, $\Sigma$, take the form $\staticenv{\omega}{\uOmega}{\uPsi}{\uPhi}$, where $\omega$ is a substitution. Each binding form is annotated with a \emph{phase}, $\phi$, either $\staticphase$ or $\standardphase$. The rules for binding forms annotated with $\standardphase$ are essentially unchanged, differing only in that $\Sigma$ passes through opaquely. The rules for binding forms annotated with $\staticphase$ are based on the corresponding $\standardphase$ phase rules, differing only in that 1) they operate on $\Sigma$ and 2) evaluation occurs immediately. Finally, the forms for TLM definition are modified so that the parse function is now an unexpanded, rather than an expanded expression. The substitution $\omega$ is applied to the parse function after it is expanded. The full details are defined as a small patch of $\miniVerseParam$ called $\miniVersePH$ in the supplement.

\subsection{Library Management}
In the examples above, and in our formal treatment, we explicitly qualified various definitions with the \li{static} keyword to make them available within static values. In practice, we would like to be able to use libraries within both static values and standard values as needed without duplicating code. This can be accomplished either by the package manager (e.g. SML/NJ's CM \cite{blume:smlnj-cm}, extended with phase annotations) or by allowing one to explicitly lower an instance of a module define in the standard phase for use also in  the static phase, as in a recent proposal for modular staging macros in OCaml \cite{Ocaml/macros}.

TLMs definitions can be exported from the top level of packages, but they cannot be exported from within ML-style modules because that would require that they also appear in signatures, and that, in turn, would complicate reasoning about signature equivalence, since TLM definitions contain arbitrary parse functions. 
%It would also bring in confusion about whether the generated expansions can use private knowledge about type identity. 
That said, it should be possible to export TLM \emph{abbreviations} from modules, since they refer to TLM definitions only through symbolic names. We have not yet formalized this intuition, but the work of \citet{culpepper2005syntactic,culpepper2007advanced} considered a closely related question: how should Typed Scheme's macros interact with its unit (i.e. package) system.



% For example, a language-external library manager for Reason similar to SML/NJ's CM \cite{blume:smlnj-cm} could support a \li{static} qualifier on imported libraries, which would place the definitions exported by the imported library into the static phase of the library being defined. In particular, a library definition in such a compilation manager might look like this:
% \begin{lstlisting}[numbers=none,morekeywords={Library,is}]
% Library 
%   /* ... exported module, signature and TLM names ... */
% is 
%   /* ... files defining those exports ... */

%   /* imports: */
%   static parsegen.cm 
% \end{lstlisting}

% A similar approach could be taken for languages the incorporate library management directly into the syntax of programs, e.g. Scala \cite{odersky2008programming}:
% \begin{lstlisting}[numbers=none]
% static import edu.cmu.comar.parsegen
% \end{lstlisting}


% \begin{equation}\label{rule:mExpandsPH-syntaxpe-standard}
% \inferrule{
%   \tsmtyExpands{\uOmega}{\urho}{\rho}\\
%   \Sigma = \staticenv{\omega}{\uOmega_S}{\uPsi_S}{\uPhi_S}\\\\
%   \expandsP{\uOmega_S}{\uPsi_S}{\uPhi_S}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultPCEExp}}\\\\
%   \evalU{[\omega]\eparse}{\eparse'}\\
%   \mExpandsPH{\uOmega}{\uAS{\uA \uplus \mapitem{\tsmv}{\adefref{a}}}{\Psi, \petsmdefn{a}{\rho}{\eparse'}}}{\uPhi}{\uM}{M}{\sigma}{\Sigma}
% }{
%   \mExpandsPH{\uOmega}{\uAS{\uA}{\Psi}}{\uPhi}{\defpetsmH{\standardphase}{\tsmv}{\urho}{\ueparse}{\uM}}{M}{\sigma}{\Sigma}
% }
% \end{equation}
% \begin{equation}\label{rule:mExpandsPH-syntaxpe-static}
% \inferrule{
%   \tsmtyExpands{\uOmega}{\urho}{\rho}\\
%   \Sigma = \staticenv{\omega}{\uOmega_S}{\uPsi_S}{\uPhi_S}\\
%   \uPsi_S = \uAS{\uA_S}{\Psi_S}\\\\
%   \expandsP{\uOmega_S}{\uPsi_S}{\uPhi_S}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultPCEExp}}\\\\
%   \evalU{[\omega]\eparse}{\eparse'}\\
%   \mExpandsPH{\uOmega}{\uPsi}{\uPhi}{\uM}{M}{\sigma}{\staticenv{\omega}{\uOmega_S}{\uAS{\uA_S \uplus \mapitem{\tsmv}{\adefref{a}}}{\Psi_S, \petsmdefn{a}{\rho}{\eparse'}}}{\uPhi_S}}
% }{
%   \mExpandsPH{\uOmega}{\uPsi}{\uPhi}{\defpetsmH{\staticphase}{\tsmv}{\urho}{\ueparse}{\uM}}{M}{\sigma}{\Sigma}
% }
% \end{equation}
\end{comment}

\newcommand{\seTLMsFormallySec}{Simple TLMs, Formally}
\section{\protect\seTLMsFormallySec}
\label{sec:setsms-formally}

This section will present a calculus of simple expression and pattern TLMs called $\miniVersePat$. By the end of this section, we will have a theorem that encodes the six reasoning principles that were outlined informally in the previous sections. 

Because our focus is on these reasoning principles, and for reasons of space, we leave our full calculus of parametric TLMs to the supplement. The full calculus extends the simple calculus of this section with an ML module calculus based closely on the system defined by \citet{pfple1}, which in turn is based on early work by \citet{MacQueen:1984:MSM:800055.802036,DBLP:conf/popl/MacQueen86}, subsequent work on the phase splitting interpretation of modules \cite{harper1989higher} and on using dependent singleton kinds to track type identity \cite{stone2006extensional,DBLP:conf/lfmtp/Crary09}, and finally on formal developments by \citet{dreyer2005understanding} and \citet{conf/popl/LeeCH07}. These additional mechanisms are necessary only to formalize the advanced features of Sec. \ref{sec:ptsms}. Proofs are in the supplement for both the simple and full calculus.


Both the simple and full calculus consist of an \emph{unexpanded language}, or \emph{UL}, defined by typed expansion to an \emph{expanded language}, or \emph{XL}. Figs. \ref{fig:U-unexpanded-terms}  and \ref{fig:U-expanded-terms} summarize the syntax of the UL and  the XL, respectively. % Programs are written as unexpanded expressions but evaluate as well-typed expanded expressions. We will start with a brief overview of our XL before turning in the remainder of the section on the UL.

\subsection{Expanded Language (XL)}\label{sec:s-XL}
The {XL} of $\miniVersePat$ forms a standard pure functional language with partial function types, quantification over types, recursive types, labeled product types and labeled sum types and support for pattern matching. 
 The reader is directed to \emph{PFPL} \cite{pfple1} for a detailed introductory account of these constructs. We will only tersely summarize the statics and dynamics of the XL below because the particularities are not critical.

% \subsubsection{Statics of the Expanded Language}
The \emph{statics of the XL} is organized around the type formation judgement, $\istypeU{\Delta}{\tau}$, the expression typing judgement, $\hastypeU{\Delta}{\Gamma}{e}{\tau}$, and the pattern typing judgement, $\patType{\pctx}{p}{\tau}$. In the latter, $\pctx$ is a typing context that tracks the typing hypotheses generated by $p$. These judgements are inductively defined in the supplemental material along with necessary auxiliary structures and  standard lemmas. %\emph{Type formation contexts}, $\Delta$, and \emph{typing contexts}, $\Gamma$ are defined in the supplement in the standard way \cite{pfpl}, 
% \[\begin{array}{ll}
% % \textbf{Judgement Form} & \textbf{Description}\\
% \istypeU{\Delta}{\tau} & \text{$\tau$ is a well-formed type}\\
% %\isctxU{\Delta}{\Gamma} & \text{$\Gamma$ is a well-formed typing context assuming $\Delta$}\\
% \hastypeU{\Delta}{\Gamma}{e}{\tau} & \text{$e$ is assigned type $\tau$}\\
% \ruleType{\Delta}{\Gamma}{r}{\tau}{\tau'} & \text{$r$ takes values of type $\tau$ to $\tau'$}\\
% \patType{\pctx}{p}{\tau} & \text{$p$ matches values of type $\tau$}\\
% & \text{and generates hypotheses $\pctx$} 
% \end{array}\]
% \emph{Type formation contexts}, $\Delta$, are finite sets of hypotheses of the form $\Dhyp{t}$. %Empty finite sets are written $\emptyset$, or omitted entirely within judgements, and non-empty finite sets are written as comma-separated finite sequences identified up to exchange and contraction. We write $\Delta, \Dhyp{t}$ when $\Dhyp{t} \notin \Delta$ for $\Delta$ extended with the hypothesis $\Dhyp{t}$. %Finite sets are written as finite sequences identified up to exchange.% We write $\Dcons{\Delta}{\Delta'}$ for the union of $\Delta$ and $\Delta'$.
% \emph{Typing contexts}, $\Gamma$, are finite functions that map each variable $x \in \domof{\Gamma}$, where $\domof{\Gamma}$ is a finite set of variables, to the hypothesis $\Ghyp{x}{\tau}$, for some $\tau$. The judgements above are inductively defined in the supplemental material and validate standard lemmas, also given in the supplement.
%Empty typing contexts are written $\emptyset$, or omitted entirely within judgements, and non-empty typing contexts are written as finite sequences of hypotheses identified up to exchange and contraction. We write $\Gamma, \Ghyp{x}{\tau}$, when $x \notin \domof{\Gamma}$, for the extension of $\Gamma$ with a mapping from $x$ to $\Ghyp{x}{\tau}$, and $\Gcons{\Gamma}{\Gamma'}$ when $\domof{\Gamma} \cap \domof{\Gamma'} = \emptyset$ for the typing context mapping each $x \in \domof{\Gamma} \cup \domof{\Gamma'}$ to $x : \tau$ if $x : \tau \in \Gamma$ or $x : \tau \in \Gamma'$. % We write $\isctxU{\Delta}{\Gamma}$ if every type in $\Gamma$ is well-formed relative to $\Delta$.
% \begin{definition}[Typing Context Formation] \label{def:isctxU}
% $\isctxU{\Delta}{\Gamma}$ iff for each hypothesis $x : \tau \in \Gamma$, we have $\istypeU{\Delta}{\tau}$.
% \end{definition}

% \begin{subequations}\label{rules:istypeU}
% \begin{equation*}\label{rule:istypeU-var}
% \inferrule{ }{\istypeU{\Delta, \Dhyp{t}}{t}}
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-parr}
% \inferrule{
%   \istypeU{\Delta}{\tau_1}\\
%   \istypeU{\Delta}{\tau_2}
% }{\istypeU{\Delta}{\aparr{\tau_1}{\tau_2}}}
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-all}
%   \inferrule{
%     \istypeU{\Delta, \Dhyp{t}}{\tau}
%   }{
%     \istypeU{\Delta}{\aall{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-rec}
%   \inferrule{
%     \istypeU{\Delta, \Dhyp{t}}{\tau}
%   }{
%     \istypeU{\Delta}{\arec{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-prod}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}
%   }{
%     \istypeU{\Delta}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-sum}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}
%   }{
%     \istypeU{\Delta}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \end{subequations}
% Premises of the form $\{{J}_i\}_{i \in \labelset}$ mean that for each $i \in \labelset$, the judgement ${J}_i$ must hold. 


% \begin{subequations}\label{rules:hastypeU}
% \begin{equation*}\label{rule:hastypeU-var}
%   \inferrule{ }{
%     \hastypeU{\Delta}{\Gamma, \Ghyp{x}{\tau}}{x}{\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-lam}
%   \inferrule{
%     \istypeU{\Delta}{\tau}\\
%     \hastypeU{\Delta}{\Gamma, \Ghyp{x}{\tau}}{e}{\tau'}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-ap}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e_1}{\aparr{\tau}{\tau'}}\\
%     \hastypeU{\Delta}{\Gamma}{e_2}{\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aeap{e_1}{e_2}}{\tau'}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-tlam}
%   \inferrule{
%     \hastypeU{\Delta, \Dhyp{t}}{\Gamma}{e}{\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aetlam{t}{e}}{\aall{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-tap}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\aall{t}{\tau}}\\
%     \istypeU{\Delta}{\tau'}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aetap{e}{\tau'}}{[\tau'/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-fold}
%   \inferrule{\
%     \istypeU{\Delta, \Dhyp{t}}{\tau}\\
%     \hastypeU{\Delta}{\Gamma}{e}{[\arec{t}{\tau}/t]\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aefold{e}}{\arec{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-unfold}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\arec{t}{\tau}}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aeunfold{e}}{[\arec{t}{\tau}/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-tpl}
%   \inferrule{
%     \{\hastypeU{\Delta}{\Gamma}{e_i}{\tau_i}\}_{i \in \labelset}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aetpl{\labelset}{\mapschema{e}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-pr}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\aprod{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \ell \hookrightarrow \tau}}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aepr{\ell}{e}}{\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-in}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}\\
%     \istypeU{\Delta}{\tau}\\
%     \hastypeU{\Delta}{\Gamma}{e}{\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aein{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \ell \hookrightarrow \tau}{e}}{\asum{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \ell \hookrightarrow \tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-case}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}\\
%     \istypeU{\Delta}{\tau}\\
%     \{\hastypeU{\Delta}{\Gamma, x_i : \tau_i}{e_i}{\tau}\}_{i \in \labelset}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aecase{\labelset}{e}{\mapschemab{x}{e}{i}{\labelset}}}{\tau}
%   }
% \end{equation*}
% \end{subequations}

%Rules (\ref{rules:istypeU}) and (\ref{rules:hastypeU}) are syntax-directed, so we assume an inversion lemma for each rule as needed without stating it separately. 
% The following standard lemmas also hold. 

% \subsubsection{Evaluation Semantics}\label{sec:dynamics-U}
The \emph{evaluation semantics} of $\miniVersePat$ is organized around the judgements $\isvalU{e}$, which says that $e$ is a value, and $\evalU{e}{e'}$, which says that $e$ evaluates to the value $e'$.% Additional  judgements, not shown, are needed to define the dynamics of pattern matching, but they do not appear directly in our subsequent developments, so we omit them. % We assume an eager dynamics. 

\subsection{Syntax of the Unexpanded Language}\label{sec:syntax-U}\label{sec:s-UL}
Unexpanded types and expressions are simple inductive structures. Unlike expanded types and expressions, they are \textbf{not} abstract binding trees -- we do \textbf{not} define the standard notions of renaming, alpha-equivalence or substitution for unexpanded terms. This is because unexpanded expressions remain ``partially parsed'' due to the presence of literal bodies, $b$, from which spliced terms might be extracted during typed expansion. In fact, unexpanded types and expressions do not involve variables at all, but rather \emph{type identifiers}, $\ut$, and \emph{expression identifiers}, $\ux$. Identifiers are given meaning by expansion to variables during typed expansion, as we will see. This distinction between identifiers and variables is technically crucial to our developments. %We \textbf{cannot} adopt the usual definitions of $\alpha$-renaming of identifiers, because unexpanded types and expressions are still in a ``partially parsed'' state -- the literal bodies, $b$, within an unexpanded expression might contain spliced subterms that are ``surfaced'' by a TLM only during typed expansion, as we will detail below. %identifiers are given meaning by expansion to variables. %In other words, unexpanded expressions are not abstract binding trees, nor sequences of characters, but a ``transitional'' structure with some characteristics of each of these. 
%For this reason, we will need to handle generating fresh variables explicitly at binding sites in our semantics. %To do so, we distinguish \emph{type identifiers}, $\ut$, and \emph{expression identifiers}, $\ux$, from type variables, $t$, and expression variables, $x$. identifiers will be given meaning by expansion to variables (which, in turn, are given meaning by substitution, as described above). 

% There are only two unexpanded expression forms, highlighted in gray in Figure \ref{fig:U-unexpanded-terms}, that do not correspond to expanded expression forms -- the seTLM definition form and the seTLM application form. %These are the ``interesting'' forms. % These are the ``interesting'' forms. % Let us define this correspondence by the metafunction $\Uof{e}$:
%\[
%\begin{split}
%\Uof{x} & = x\\
%\Uof{\aelam{\tau}{x}{e}} & = \aulam{\tau}{x}{\Uof{e}}\\
%\Uof{\aeap{e_1}{e_2}} & = \auap{\Uof{e_1}}{\Uof{e_2}}
%\end{split}
%\] and so on for the remaining expanded expression forms.
Most of the unexpanded forms in Figure \ref{fig:U-unexpanded-terms}  mirror the expanded forms. We refer to these as the \emph{common forms}. % The mapping from expanded forms to common unexpanded forms is defined explicitly in the supplement. 

There is also a corresponding context-free textual syntax for the UL. 
Giving a complete definition of the context-free textual syntax as, e.g., a context-free grammar, is not critical to our purposes here. 
%Our paper on Wyvern defines a textual syntax for a similar system \cite{TSLs}. 
Instead, we only posit partial metafunctions $\parseUTypF{b}$, $\parseUExpF{b}$ and $\parseUPatF{b}$  that go from character sequences, $b$, to unexpanded types, expressions and patterns (the supplement states the full condition.) 
% \begingroup
% \def\thetheorem{\ref{condition:textual-representability-SES}}
% \begin{condition}[Textual Representability] ~
% \begin{enumerate}[nolistsep]
% \item For each $\utau$, there exists $b$ such that $\parseUTyp{b}{\utau}$. 
% \item For each $\ue$, there exists $b$ such that $\parseUExp{b}{\ue}$.
% \item For each $\upv$, there exists $b$ such that $\parseUPat{b}{\upv}$.
% \end{enumerate}
% \end{condition}
% \endgroup



\begin{figure}
%\hspace{-5px}
\begin{minipage}{\textwidth}
\small
$\arraycolsep=2pt\begin{array}{llcl}
\mathsf{Typ} & \tau & ::= & t ~\vert~ \aparr{\tau}{\tau} ~\vert~ \aall{t}{\tau} ~\vert~ \arec{t}{\tau} \\
& & \vert &  \aprod{\labelset}{\mapschema{\tau}{i}{\labelset}} ~\vert~ \asum{\labelset}{\mapschema{\tau}{i}{\labelset}}\\
\mathsf{Exp} & e & ::= & x ~\vert~ \aelam{\tau}{x}{e} ~\vert~ \aeap{e}{e} ~\vert~ \aetlam{t}{e} ~\vert~ \aetap{e}{\tau} \\
& & \vert & \aefold{e} ~\vert~ \aetpl{\labelset}{\mapschema{e}{i}{\labelset}} ~\vert~  \aein{\ell}{e} \\
& & \vert & \aematchwith{n}{e}{\seqschemaX{r}}\\
\mathsf{Rule} & r & ::= & \aematchrule{p}{e}\\
\mathsf{Pat} & p & ::= & x  ~\vert~ \aewildp ~\vert~ \aefoldp{p} ~\vert~ \aetplp{\labelset}{\mapschema{p}{i}{\labelset}}\\
& & \vert & \aeinjp{\ell}{p}
\end{array}$
\end{minipage}
\caption[Syntax of the XL of $\miniVersePat$]{Syntax of the expanded language (XL). XL terms are \emph{abstract binding trees} (ABTs) identified up to alpha-equivalence, so we follow the syntactic conventions of \citet{pfple1}. Metavariable $x$ and $t$ ranges over variables.  %When using stylized forms, the label set is omitted when it can be inferred, e.g. the labeled product type $\prodt{\finmap{\mapitem{\ell_1}{e_1}, \mapitem{\ell_2}{e_2}}}$ leaves the label set $\{\ell_1, \ell_2\}$ implicit. 
% When we use the stylized forms, we assume that the reader can infer suppressed indices and arguments from the surrounding context.
}
\label{fig:U-expanded-terms}
\end{figure}


\begin{figure*}
{\small\begin{mathpar}
\inferrule{
  \inferrule{ }{\expandsTU{\uDelta}{\utau}{\tau}}\\
  \inferrule{
    \inferrule{ }{\expandsTU{\uDelta}{\utau}{\tau}}\\
    \inferrule{ }{
      \expandsUP{\uDelta}{\uGG{\vExpands{\ux}{x_2}}{x_1 : \tau, x_2 : \tau}}{\uPsi}{\uPhi}{\ux}{x_2}{\tau}
    }~\textsc{ee-id}
  }{
    \expandsUP{\uDelta}{\uGG{\vExpands{\ux}{x_1}}{x_1 : \tau}}{\uPsi}{\uPhi}{\lam{\ux}{\utau}{\ux}}{\aelam{\tau}{x_2}{x_2}}{\aparr{\tau}{\tau}}
    % \expandsU{\uDelta}{\uGG}
  }~\textsc{ee-lam}
}{
  \expandsUP{\uDelta}{\uGG{\emptyset}{\emptyset}}{\uPsi}{\uPhi}{\lam{\ux}{\utau}{\lam{\ux}{\utau}{\ux}}}{\aelam{\tau}{x_1}{\aelam{\tau}{x_2}{x_2}}}{\aparr{\tau}{\aparr{\tau}{\tau}}}
}~\textsc{ee-lam}
\end{mathpar}}
\caption{An example expansion derviation demonstrating how identifiers and variables are separately tracked.}
\label{fig:expansion-exmpl}
\end{figure*}


\subsection{Typed Expansion}\label{sec:typed-expansion-U}\label{sec:s-TE}
Unexpanded terms are checked and expanded simultaneously according to the central \emph{typed expansion judgements}:
\[\begin{array}{ll}
% \textbf{Judgement Form} & \textbf{Description}\\
\expandsTU{\uDelta}{\utau}{\tau} & \text{$\utau$ has well-formed expansion $\tau$}\\
\expandsUPX{\ue}{e}{\tau} & \text{$\ue$ has expansion $e$ of type $\tau$}\\
% \ruleExpands{\uDelta}{\uGamma}{\uPsi}{\uPhi}{\urv}{r}{\tau}{\tau'} & \text{$\urv$ has expansion $r$ taking values of type $\tau$ to values of type $\tau'$}\\
\patExpands{\upctx}{\uPhi}{\upv}{p}{\tau} & \text{$\upv$ has expansion $p$ matching $\tau$}
\end{array}\]
%\newcommand{\gray}[1]{{\color{gray} #1}}



% These judgements are inductively defined in the supplement. 
% \begingroup 
% \def\thetheorem{\ref{thm:typed-expansion-short-U}}

%These rules validate the following theorem, which establishes that typed expansion produces an expansion of the assigned type. 
%\begin{theorem}[Typed Expression Expansion] If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uPsi}{\ue}{e}{\tau}$ and $\uetsmenv{\Delta}{\uPsi}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.\end{theorem}
%\begin{proof} This is the first part of Theorem \ref{thm:typed-expansion-U}, defined and proven below.\end{proof}


The typed expansion rules that handle common forms mirror the corresponding typing rules. The \emph{expression TLM context}, $\uPsi$, and the \emph{pattern TLM context}, $\uPhi$, pass through these rules opaquely. For example, the rules for variables and lambdas are shown being applied in Fig. \ref{fig:expansion-exmpl}, discussed below. 
%Each of these rules is based on the corresponding typing rule, i.e. Rules (\ref{rule:hastypeU-var}) through (\ref{rule:hastypeU-case}), respectively. For example, the following typed expansion rules are based on the typing rules (\ref{rule:hastypeU-var}), (\ref{rule:hastypeU-lam}) and (\ref{rule:hastypeU-ap}), respectively:% for unexpanded expressions of variable, function and application form, respectively: 
% {\small\begin{mathpar}
%   \inferrule[ee-id]{ }{\expandsUP{\uDelta}{\uGamma, \uGhyp{\ux}{x}{\tau}}{\uPsi}{\uPhi}{\ux}{x}{\tau}}

%   \inferrule[ee-lam]{
%     \expandsTU{\uDelta}{\utau}{\tau}\\
%     \expandsUP{\uDelta}{\uGamma, \uGhyp{\ux}{x}{\tau}}{\uPsi}{\uPhi}{\ue}{e}{\tau'}
%   }{\expandsUPX{\lam{\ux}{\utau}{\ue}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}}
% \end{mathpar}}




The only subtlety related to common forms has to do with the relationship between identifiers, $\ux$, in the UL and variables, $x$, in the XL. To understand this, we must first describe in detail how unexpanded contexts work. \emph{Unexpanded typing contexts}, $\uGamma$, are pairs of the form $\uGG{\uG}{\Gamma}$, where $\uG$ maps each expression identifier $\ux \in \domof{\uG}$ to the hypothesis $\vExpands{\ux}{x}$, for some expression variable, $x$, called its expansion. The standard typing context, $\Gamma$, then tracks the type of $x$. We write $\ctxUpdate{\uG}{\ux}{x}$ for the expression identifier expansion context that maps $\ux$ to $\vExpands{\ux}{x}$ and defers to $\uG$ for all other expression identifiers (i.e. the previous mapping is \textbf{updated}.) Note the distinction between update and extension (which requires that the new identifier is not already in the domain.) %We write $\uGammaOK{\uGamma}$ when $\uGamma=\uGG{\uG}{\Gamma}$ and each expression variable in $\uG$ is assigned a type by $\Gamma$.
%\begin{definition} $\uGammaOK{\uGG{\uG}{\Gamma}}$ iff for each $\vExpands{\ux}{x} \in \uG$, we have $\Ghyp{x}{\tau} \in \Gamma$ for some $\tau$.\end{definition}
%\noindent 
We define $\uGamma, \uGhyp{\ux}{x}{\tau}$ when $\uGamma = \uGG{\uG}{\Gamma}$ as an abbreviation of $\uGG{\ctxUpdate{\uG}{\ux}{x}}{\Gamma, \Ghyp{x}{\tau}}$. 

To develop an intuition for why the update operation is necessary, it is instructive to inspect in Fig. \ref{fig:expansion-exmpl} the derivation of the expansion of the unexpanded expression $\lam{\ux}{\utau}{\lam{\ux}{\utau}{\ux}}$ to $\aelam{\tau}{x_1}{\aelam{\tau}{x_2}{x_2}}$ assuming $\expandsTU{\uDelta}{\utau}{\tau}$. Notice that when Rule \textsc{ee-lam} is applied, the type identifier expansion context is updated but the typing context is extended with a (necessarily fresh) variable, first $x_1$ then $x_2$. Without this mechanism, expansions for unexpanded terms with shadowing, like this minimal example, would not exist, because we cannot implicitly alpha-vary the unexpanded term to sidestep this problem in the usual manner.

% \emph{Unexpanded type formation contexts}, $\uDelta$, consist of a \emph{type identifier expansion context}, $\uD$, paired with a standard type formation context, $\Delta$, and operate analagously (see supplement.)
% of the form $\uDD{\uD}{\Delta}$, i.e. they . We similarly define $\uDelta, \uDhyp{\ut}{t}$ when $\uDelta=\uDD{\uD}{\Delta}$ as an abbreviation of $\uDD{\ctxUpdate{\uD}{\ut}{t}}{\Delta, \Dhyp{t}}$.%type identifier expansion context is always extended/updated together with 


% % Before we continue, let us state an important invariant: that Typed expansion produces a well-typed expression. 
% \begin{theorem}[Typed Expression Expansion]\label{thm:typed-expansion-short-U} ~\\
% If $\expandsUP{\uDD{\uD}{\Delta}\hspace{-3px}}{\uGG{\uG}{\Gamma}\hspace{-3px}}{\uPsi}{\uPhi}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
% \end{theorem}
% For the typed expansion rules governing common forms, like the two example rules applied in Fig. \ref{fig:expansion-exmpl}, it is easy to see that this invariant holds. The  rules of particular interest are the rules governing TLM definitions and TLM application, which are the topic of the remainder of this section. 
% \endgroup
% \subsubsection{Type Expansion}
% \emph{unexpanded type formation contexts}, $\udelta$, are of the form $\udd{\ud}{\delta}$, i.e. they consist of a \emph{type identifier expansion context}, $\ud$, paired with a standard type formation context, $\delta$. 

% A \emph{type identifier expansion context}, $\uD$, is a finite function that maps each type identifier $\ut \in \domof{\uD}$ to the hypothesis $\vExpands{\ut}{t}$, for some type variable $t$. We write $\ctxUpdate{\uD}{\ut}{t}$ for the type identifier expansion context that maps $\ut$ to $\vExpands{\ut}{t}$ and defers to $\uD$ for all other type identifiers (i.e. the previous mapping is \emph{updated}.) We define $\uDelta, \uDhyp{\ut}{t}$ when $\uDelta=\uDD{\uD}{\Delta}$ as an abbreviation of $\uDD{\ctxUpdate{\uD}{\ut}{t}}{\Delta, \Dhyp{t}}$.%type identifier expansion context is always extended/updated together with 

% The \emph{type expansion judgement}, $\expandsTU{\uDelta}{\utau}{\tau}$, is inductively defined by the rules given in the supplement. The first three of these rules are reproduced below:
% % \begin{subequations}%\label{rules:expandsTU}
% \begin{mathpar}
% \inferrule[te-id]{ }{\expandsTU{\uDelta, \uDhyp{\ut}{t}}{\ut}{t}}

% \inferrule[te-parr]{
%   \expandsTU{\uDelta}{\utau_1}{\tau_1}\\
%   \expandsTU{\uDelta}{\utau_2}{\tau_2}
% }{\expandsTU{\uDelta}{\parr{\utau_1}{\utau_2}}{\aparr{\tau_1}{\tau_2}}}

% \inferrule[te-all]{
%     \expandsTU{\uDelta, \uDhyp{\ut}{t}}{\utau}{\tau}
%   }{
%     \expandsTU{\uDelta}{\forallt{\ut}{\utau}}{\aall{t}{\tau}}
% }
% \end{mathpar}
% % \begin{equation*}\label{rule:expandsTU-rec}
% %   \inferrule{
% %     \expandsTU{\uDelta, \uDhyp{\ut}{t}}{\utau}{\tau}
% %   }{
% %     \expandsTU{\uDelta}{\aurec{\ut}{\utau}}{\arec{t}{\tau}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:expandsTU-prod}
% %   \inferrule{
% %     \{\expandsTU{\uDelta}{\utau_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \expandsTU{\uDelta}{\auprod{\labelset}{\mapschema{\utau}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:expandsTU-sum}
% %   \inferrule{
% %     \{\expandsTU{\uDelta}{\utau_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \expandsTU{\uDelta}{\ausum{\labelset}{\mapschema{\utau}{i}{\labelset}}}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}
% % \end{subequations}
% %We write $\uDeltaOK{\uDelta}$ when $\uDelta=\uDD{\uD}{\Delta}$ and each type variable in $\uD$ also appears in $\Delta$.
% %\begin{definition}\label{def:uDeltaOK} $\uDeltaOK{\uDD{\uD}{\Delta}}$ iff for each $\vExpands{\ut}{t} \in \uD$, we have $\Dhyp{t} \in \Delta$.\end{definition}



% The Type Expansion Lemma establishes that the expansion of an unexpanded type is a well-formed type.

% % \begingroup
% % \def\thetheorem{\ref{lemma:type-expansion-U}}
% \begin{lemma}[Type Expansion] If $\expandsTU{\uDD{\uD}{\Delta}}{\utau}{\tau}$ then $\istypeU{\Delta}{\tau}$.\end{lemma}
% % \begin{proof} By rule induction over Rules (\ref{rules:expandsTU}). In each case, we apply the IH to or over each premise, then apply the corresponding type formation rule in Rules (\ref{rules:istypeU}). \end{proof}
% % \endgroup
% \begin{subequations}\label{rules:expandsU}




% The rules for the remaining expressions of common form are entirely straightforward, mirroring the corresponding typing rules, i.e. Rules (\ref{rules:hastypeU}). %In particular, observe that, in each of these rules, the unexpanded and expanded expression forms in the conclusion correspond, and each premise corresponds to a premise of the corresponding typing rule. %Type formation premises in the typing rule give rise to  type expansion premises in the corresponding typed expansion rule, and each typed expression expansion premise in each rule above corresponds to a typing premise in the corresponding typing rule. 
% The type assigned in the conclusion of each rule above is identical to the type assigned in the conclusion of the corresponding typing rule. The seTLM context, $\uPsi$, passes opaquely through these rules (we will define seTLM contexts below.) As such, the corresponding cases in the proof of Theorem \ref{thm:typed-expansion-short-U} are by application of the induction hypothesis and the  corresponding typing rule. %Rules (\ref{rules:expandsTU}) could similarly have been generated by mechanically transforming Rules (\ref{rules:istypeU}).

% We can express this scheme more precisely with the rule transformation given in Appendix \ref{appendix:SES-uexps}. For each rule in Rules (\ref{rules:istypeU}) and Rules (\ref{rules:hastypeU}),
% \begin{mathpar}
% \refstepcounter{equation}
% % \label{rule:expandsU-tlam}
% % \refstepcounter{equation}
% % \label{rule:expandsU-tap}
% % \refstepcounter{equation}
% \label{rule:expandsU-fold}
% \refstepcounter{equation}
% \label{rule:expandsU-unfold}
% \refstepcounter{equation}
% \label{rule:expandsU-tpl}
% \refstepcounter{equation}
% \label{rule:expandsU-pr}
% \refstepcounter{equation}
% \label{rule:expandsU-in}
% \refstepcounter{equation}
% \label{rule:expandsU-case}
% \inferrule{J_1\\ \cdots \\ J_k}{J}
% \end{mathpar}
% the corresponding typed expansion rule is 
% \begin{mathpar}
% \inferrule{
%   \Uof{J_1} \\
%   \cdots\\
%   \Uof{J_k}
% }{
%   \Uof{J}
% }
% \end{mathpar}
% where
% \[\begin{split}
% \Uof{\istypeU{\Delta}{\tau}} & = \expandsTU{\Uof{\Delta}}{\Uof{\tau}}{\tau} \\
% \Uof{\hastypeU{\Gamma}{\Delta}{e}{\tau}} & = \expandsU{\Uof{\Gamma}}{\Uof{\Delta}}{\uPsi}{\Uof{e}}{e}{\tau}\\
% \Uof{\{J_i\}_{i \in \labelset}} & = \{\Uof{J_i}\}_{i \in \labelset}
% \end{split}\]
% and where:
% \begin{itemize}
% \item $\Uof{\tau}$ is defined as follows:
%   \begin{itemize}
%   \item When $\tau$ is of definite form, $\Uof{\tau}$ is defined as in Sec. \ref{sec:syntax-U}.
%   \item When $\tau$ is of indefinite form, $\Uof{\tau}$ is a uniquely corresponding metavariable of sort $\mathsf{UTyp}$ also of indefinite form. For example, in Rule (\ref{rule:istypeU-parr}), $\tau_1$ and $\tau_2$ are of indefinite form, i.e. they match arbitrary types. The rule transformation simply ``hats'' them, i.e. $\Uof{\tau_1}=\utau_1$ and $\Uof{\tau_2}=\utau_2$.
%   \end{itemize}
% \item $\Uof{e}$ is defined as follows
% \begin{itemize}
% \item When $e$ is of definite form, $\Uof{e}$ is defined as in Sec. \ref{sec:syntax-U}. 
% \item When $e$ is of indefinite form, $\Uof{e}$ is a uniquely corresponding metavariable of sort $\mathsf{UExp}$ also of indefinite form. For example, $\Uof{e_1}=\ue_1$ and $\Uof{e_2}=\ue_2$.
% \end{itemize}
% \item $\Uof{\Delta}$ is defined as follows:
%   \begin{itemize} 
%   \item When $\Delta$ is of definite form, $\Uof{\Delta}$ is defined as above.
%   \item When $\Delta$ is of indefinite form, $\Uof{\Delta}$ is a uniquely corresponding metavariable ranging over unexpanded type formation contexts. For example, $\Uof{\Delta} = \uDelta$.
%   \end{itemize}
% \item $\Uof{\Gamma}$ is defined as follows:
%   \begin{itemize}
%   \item When $\Gamma$ is of definite form, $\Uof{\Gamma}$ produces the corresponding unexpanded typing context as follows:
% \begin{align*}
% \Uof{\emptyset} & = \uGG{\emptyset}{\emptyset}\\
% \Uof{\Gamma, \Ghyp{x}{\tau}} & = \Uof{\Gamma}, \uGhyp{\identifierof{x}}{x}{\tau}
% \end{align*}
%   \item When $\Gamma$ is of indefinite form, $\Uof{\Gamma}$ is a uniquely corresponding metavariable ranging over unexpanded typing contexts. For example, $\Uof{\Gamma} = \uGamma$.
% \end{itemize}
% \end{itemize}

% It is instructive to use this rule transformation to generate Rules (\ref{rules:expandsTU}) and Rules (\ref{rule:expandsU-var}) through (\ref{rule:expandsU-tap}) above. We omit the remaining rules, i.e. Rules (\ref*{rule:expandsU-fold}) through (\ref*{rule:expandsU-case}). By instead defining these rules solely by the rule transformation just described, we avoid having to write down a number of rules that are of limited marginal interest. Moreover, this demonstrates the general technique for generating typed expansion rules for unexpanded types and expressions of common form, so our exposition is somewhat ``robust'' to changes to the inner core. 
%o that when the inner core changes,  typed expansion rules  our exposition somewhat robust to changes to the inner core (though not to changes to the judgement forms in the statics of the inner core).% Even if changes to the judgement forms in the statics of the inner core are needed (e.g. the addition of a symbol context), it is easy to see would correspond to changes in the generic specification above.
% \begin{subequations}\label{rules:expandsU}
% \begin{equation*}\label{rule:expandsU-var}
%   \inferrule{ }{\expandsU{\Delta}{\Gamma, x : \tau}{\uPsi}{x}{x}{\tau}}
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-lam}
%   \inferrule{
%     \istypeU{\Delta}{\tau}\\
%     \expandsU{\Delta}{\Gamma, x : \tau}{\uPsi}{\ue}{e}{\tau'}
%   }{\expandsUX{\aulam{\tau}{x}{\ue}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}}
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-ap}
%   \inferrule{
%     \expandsUX{\ue_1}{e_1}{\aparr{\tau}{\tau'}}\\
%     \expandsUX{\ue_2}{e_2}{\tau}
%   }{
%     \expandsUX{\auap{\ue_1}{\ue_2}}{\aeap{e_1}{e_2}}{\tau'}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tlam}
%   \inferrule{
%     \expandsU{\Delta, \Dhyp{t}}{\Gamma}{\uPsi}{\ue}{e}{\tau}
%   }{
%     \expandsUX{\autlam{t}{\ue}}{\aetlam{t}{e}}{\aall{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tap}
%   \inferrule{
%     \expandsUX{\ue}{e}{\aall{t}{\tau}}\\
%     \istypeU{\Delta}{\tau'}
%   }{
%     \expandsUX{\autap{\ue}{\tau'}}{\aetap{e}{\tau'}}{[\tau'/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-fold}
%   \inferrule{
%     \istypeU{\Delta, \Dhyp{t}}{\tau}\\
%     \expandsUX{\ue}{e}{[\arec{t}{\tau}/t]\tau}
%   }{
%     \expandsUX{\aufold{t}{\tau}{\ue}}{\aefold{e}}{\arec{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-unfold}
%   \inferrule{
%     \expandsUX{\ue}{e}{\arec{t}{\tau}}
%   }{
%     \expandsUX{\auunfold{\ue}}{\aeunfold{e}}{[\arec{t}{\tau}/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tpl}
%   \inferrule{
%     \{\expandsUX{\ue_i}{e_i}{\tau_i}\}_{i \in \labelset}
%   }{
%     \expandsUX{\autpl{\labelset}{\mapschema{\ue}{i}{\labelset}}}{\aetpl{\labelset}{\mapschema{e}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-pr}
%   \inferrule{
%     \expandsUX{\ue}{e}{\aprod{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}}
%   }{
%     \expandsUX{\aupr{\ell}{\ue}}{\aepr{\ell}{e}}{\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-in}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}\\
%     \istypeU{\Delta}{\tau}\\
%     \expandsUX{\ue}{e}{\tau}
%   }{
%     \left\{\shortstack{$\Delta~\Gamma \vdash_\uPsi \auin{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}{\ue}$\\$\leadsto$\\$\aein{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}{e} : \asum{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}$\vspace{-1.2em}}\right\}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-case}
%   \inferrule{
%     \expandsUX{\ue}{e}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}\\
%     \{\expandsU{\Delta}{\Gamma, \Ghyp{x_i}{\tau_i}}{\uPsi}{\ue_i}{e_i}{\tau}\}_{i \in \labelset}
%   }{
%     \expandsUX{\aucase{\labelset}{\ue}{\mapschemab{x}{\ue}{i}{\labelset}}}{\aecase{\labelset}{e}{\mapschemab{x}{e}{i}{\labelset}}}{\tau}
%   }
% \end{equation*}
% \end{subequations}


% \begin{equation*}\label{rule:expandsU-syntax}
% \inferrule{
%   \istypeU{\Delta}{\tau}\\
%   \expandsU{\emptyset}{\emptyset}{\emptyset}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultExp}}\\\\
%   a \notin \domof{\uPsi}\\
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\ue}{e}{\tau'}
% }{
%   \expandsUX{\audefuetsm{\tau}{\ueparse}{\tsmv}{\ue}}{e}{\tau'}
% }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tsmap}
% \inferrule{
%   \encodeBody{b}{\ebody}\\
%   \evalU{\ap{\eparse}{\ebody}}{\inj{\lbltxt{SuccessE}}{\ecand}}\\
%   \decodeCondE{\ecand}{\ce}\\\\
%   \cvalidE{\emptyset}{\emptyset}{\esceneU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{b}}{\ce}{e}{\tau}
% }{
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\autsmap{b}{\tsmv}}{e}{\tau}
% }
% \end{equation*}
%\end{subequations}

%Notice that each form of expanded expression (Figure \ref{fig:U-expanded-terms}) corresponds to a form of unexpanded expression (Figure \ref{fig:U-unexpanded-terms}). For each typing rule in Rules (\ref{rules:hastypeU}), there is a corresponding typed expansion rule -- Rules (\ref{rule:expandsU-var}) through (\ref{rule:expandsU-case}) -- where the unexpanded and expanded forms correspond. The premises also correspond -- if a typing judgement appears as a premise of a typing rule, then the corresponding premise in the corresponding typed expansion rule is the corresponding typed expansion judgement. The seTLM context is not extended or inspected by these rules (it is only ``threaded through'' them opaquely).

%There are two unexpanded expression forms that do not correspond to an expanded expression form: the seTLM definition form, and the seTLM application form. The rules governing these two forms interact with the seTLM context, and are the topics of the next two subsections, respectively.

\subsection{TLM Definitions}\label{sec:U-uetsm-definition}\label{sec:s-TLM-def}
% An unexpanded expression of seTLM definition form, $\uesyntaxq{\tsmv}{\utau}{\eparse}{\ue}$, 
%The operational form corresponding to this stylized form is \[\audefuetsm{\utau}{\eparse}{\tsmv}{\ue}\]
 % defines an {seTLM} identified as $\tsmv$ with \emph{unexpanded type annotation} $\utau$ and \emph{parse function} $\eparse$ for use within $\ue$. 
Rule \textsc{ee-def-setsm} in Fig. \ref{fig:ee-def-setsm} governs simple expression TLM (seTLM) definitions. The first premise expands the unexpanded type annotation. The second premise checks that $\eparse$ is a closed expanded function of the given function type. (In the supplement, we add the machinery necessary for parse functions that are neither closed nor yet expanded.)

% \begin{subequations}[resume]
% \begin{equation*}\label{rule:expandsU-syntax}
% \inferrule{
%   \istypeU{\Delta}{\tau}\\
%   \expandsU{\emptyset}{\emptyset}{\emptyset}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultExp}}\\\\
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\ue}{e}{\tau'}
% }{
%   \expandsUX{\audefuetsm{\tau}{\ueparse}{\tsmv}{\ue}}{e}{\tau'}
% }
% \end{equation*}
\begin{figure*}
{\small\begin{mathpar}
\inferrule[ee-def-setsm]{
  \expandsTU{\uDelta}{\utau}{\tau}\\
  \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultExp}}\\\\
  \evalU{\eparse}{\eparse'}\\
  \expandsUP{\uDelta}{\uGamma}{\uPsi, \uShyp{\tsmv}{a}{\tau}{\eparse'}}{\uPhi}{\ue}{e}{\tau'}
}{
  \expandsUPX{\uesyntaxq{\tsmv}{\utau}{\eparse}{\ue}}{e}{\tau'}
}
~~~~~~~~~~~~\hspace{15px}
\inferrule[ee-ap-setsm]{
  \encodeBody{b}{\ebody}\\
  \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{\ecand}}\\\\
  \decodeCondE{\ecand}{\ce}\\
  \segOK{\segof{\ce}}{b}\\\\
  \cvalidE{\emptyset}{\emptyset}{\esceneUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{b}}{\ce}{e}{\tau}
}{
  \expandsUP{\uDelta}{\uGamma}{\uPsi', \uShyp{\tsmv}{a}{\tau}{\eparse}}{\uPhi}{\utsmap{\tsmv}{b}}{e}{\tau}
}
\end{mathpar}}
\caption{The typed expansion rules for expression TLM definition and application.}
\label{fig:ee-def-setsm}
\end{figure*}
% \end{subequations}


The type abbreviated $\tBody$ classifies encodings of literal bodies, $b$. Rather than defining $\tBody$ explicitly it suffices to take as a condition that there is an isomorphism between literal bodies and values of type $\tBody$ mediated in one direction by a judgement $\encodeBody{b}{\ebody}$ that will come up below.

The return type, $\tParseResultExp$, abbreviates a labeled sum type that distinguishes parse errors from successful parses:
{$
% L_\mathtt{SE} & \defeq \lbltxt{ParseError}, \lbltxt{SuccessE}\\ \asumNL{
  \mapitem{\lbltxt{ParseError}}{\prodt{}}, 
  \mapitem{\lbltxt{SuccessE}}{\tCEExp}
$}.
% \end{align*}} %[\mapitem{\lbltxt{ParseError}}{\prodt{}}, \mapitem{\lbltxt{SuccessE}}{\tCEExp}]
% \] 

The type abbreviated $\tCEExp$ classifies encodings of \emph{proto-expressions}, $\ce$ (pronounced ``grave $e$''.) The syntax of proto-expressions, defined in Figure \ref{fig:U-candidate-terms}, will be described when we describe proto-expansion validation in Sec. \ref{sec:ce-syntax-U}. The mapping from proto-expressions to values of type $\tCEExp$ is defined by the \emph{proto-expression encoding judgement}, $\encodeCondE{\ce}{e}$. An inverse mapping is defined by the \emph{proto-expression decoding judgement}, $\decodeCondE{e}{\ce}$. Again, we need only take as a condition that there is an isomorphism between values of type $\tCEExp$ and closed proto-expressions mediated by these judgements (see supplement.)

The third premise of Rule \textsc{ee-def-setsm} evaluates the parse function to a value. This is not necessary, but it is the choice one would expect to make in an eager language.

The final premise of Rule \textsc{ee-def-setsm} extends the expression TLM context, $\uPsi$, with the newly determined {seTLM definition}, and proceeds to assign a type, $\tau'$, and expansion, $e$, to $\ue$. The conclusion of the rule then assigns this type and expansion to the seTLM definition as a whole. % i.e. TLMs define behavior that is relevant during typed expansion, but not during evaluation. 

{Expression TLM contexts}, $\uPsi$, are of the form $\uAS{\uA}{\Psi}$, where $\uA$ is a \emph{TLM identifier expansion context} and $\Psi$ is an \emph{expression TLM definition context}. We distinguish TLM identifiers, $\tsmv$, from TLM names, $a$, for much the same reason that we distinguish type and expression identifiers from type and expression variables: in order to allow a TLM definition to shadow a previously defined TLM definition without relying on an implicit identification convention.

% A {TLM identifier expansion context}, $\uA$, maps each TLM identifier $\tsmv \in \domof{\uA}$ to the \emph{TLM identifier expansion}, $\vExpands{\tsmv}{a}$, for some \emph{TLM name}, $a$. 



An {expression TLM definition context}, $\Psi$, is a finite function mapping each TLM name $a \in \domof{\Psi}$ to an \emph{expanded seTLM definition}, $\xuetsmbnd{a}{\tau}{\eparse}$, where $\tau$ is the seTLM's type annotation, and $\eparse$ is its parse function. 
We define $\uPsi, \uShyp{\tsmv}{a}{\tau}{\eparse}$, when $\uPsi=\uAS{\uA}{\Psi}$, as an abbreviation of $\uAS{\ctxUpdate{\uA}{\tsmv}{a}}{\Psi, \xuetsmbnd{a}{\tau}{\eparse}}$.

%Moreover, this distinction will be crucial in the semantics of TLM abbreviations in Chapter \ref{chap:ptsms}. 

% \end{enumerate}


The simple pattern TLM (spTLM) definition form operates analagously (see supplement), with the spTLM context, $\uPhi$, rather than the $\uPsi$ updated. This allows expression and pattern TLMs to use the same identifiers.
% {\vspace{-3px}\small\begin{mathpar}
% \inferrule[ee-def-sptsm]{
%   \expandsTU{\uDelta}{\utau}{\tau}\\
%   \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultPat}}\\\\
%   \evalU{\eparse}{\eparse'}\\
%   \expandsUP{\uDelta}{\uGamma}{\uPsi}{\uPhi, \uPhyp{\tsmv}{a}{\tau}{\eparse'}}{\ue}{e}{\tau'}
% }{
%   \expandsUPX{\usyntaxup{\tsmv}{\utau}{\eparse}{\ue}}{e}{\tau'}
% }
% \end{mathpar}}
% \vspace{-3px}

% \[\begin{array}{ll}
% \textbf{Judgement Form} & \textbf{Description}\\
% \uetsmenv{\Delta}{\uPsi} & \text{$\uPsi$ is well-formed assuming $\Delta$}\end{array}\]
% This judgement is inductively defined by the following rules:
% \begin{subequations}[intermezzo]\label{rules:uetsmenv-U}
% \begin{equation*}\label{rule:uetsmenv-empty}
% \inferrule{ }{\uetsmenv{\Delta}{\emptyset}}
% \end{equation*}
% \begin{equation*}\label{rule:uetsmenv-ext}
% \inferrule{
%   \uetsmenv{\Delta}{\uPsi}\\
%   \istypeU{\Delta}{\tau}\\
%   \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultExp}}
% }{
%   \uetsmenv{\Delta}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}
% }
% \end{equation*}
% \end{subequations}

\subsection{TLM Application}\label{sec:U-uetsm-application}\label{sec:s-TLM-ap}
The unexpanded expression form for applying an seTLM named $\tsmv$ to a literal form with literal body $b$ is $\utsmap{\tsmv}{b}$. 
% This form uses forward slashes\todo{use backticks} to delimit the literal body, but other generalized literal forms could also be included as derived forms in the textual syntax. % (we omit them for simplicity).
%The corresponding operational form is $\autsmap{b}{\tsmv}$. %i.e. for each literal body $b$, the operator $\texttt{uapuetsm}[b]$ is indexed by the TLM name $\tsmv$ and takes no arguments. %\footnote{This is in following the conventions in \emph{PFPL} \cite{pfpl}, where operators parameters allow for the use of metatheoretic objects that are not syntax trees or binding trees, e.g. $\mathsf{str}[s]$ and $\mathsf{num}[n]$.} This operator is indexed by the TLM name $\tsmv$ and takes no arguments. 
Rule \textsc{ee-ap-setsm} governing this form is shown in Fig. \ref{fig:ee-def-setsm}. 

% \begin{subequations}[resume]
% \begin{equation*}\label{rule:expandsU-tsmap}
% \inferrule{
%   \encodeBody{b}{\ebody}\\
%   \evalU{\ap{\eparse}{\ebody}}{\inj{\lbltxt{SuccessE}}{\ecand}}\\
%   \decodeCondE{\ecand}{\ce}\\\\
%   \cvalidE{\emptyset}{\emptyset}{\esceneU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{b}}{\ce}{e}{\tau}
% }{
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\autsmap{b}{\tsmv}}{e}{\tau}
% }
% \end{equation*}
% {\small\begin{mathpar}
% \inferrule[ee-ap-setsm]{
%   \encodeBody{b}{\ebody}\\
%   \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{\ecand}}\\
%   \decodeCondE{\ecand}{\ce}\\\\
%   \segOK{\segof{\ce}}{b}\\
%   \cvalidE{\emptyset}{\emptyset}{\esceneUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{b}}{\ce}{e}{\tau}
% }{
%   \expandsUP{\uDelta}{\uGamma}{\uPsi', \uShyp{\tsmv}{a}{\tau}{\eparse}}{\uPhi}{\utsmap{\tsmv}{b}}{e}{\tau}
% }
% \end{mathpar}}

The first premise encodes the literal body, $\ebody$, which, as described above, is a value of type $\tBody$.

The second premise applies the parse function $\eparse$ to the encoding of the literal body. If parsing succeeds, i.e. a value of the form $\aein{\mathtt{SuccessE}}{\ecand}$ results from evaluation, then $\ecand$ will be a value of type $\tCEExp$ (assuming a well-formed expression TLM context, by application of Type Safety.) We call $\ecand$ the \emph{encoding of the proto-expansion}. If the parse function produces a value labeled $\lbltxt{ParseError}$, then typed expansion fails and formally, no rule is necessary.

The third premise decodes the encoding of the proto-expansion using the judgement described in Sec. \ref{sec:U-uetsm-definition}. 

The fourth premise of Rule \textsc{ee-ap-setsm} determines the segmentation of the proto-expansion, $\segof{\ce}$, and ensures that it is valid with respect to $b$ via the predicate $\segOK{\psi}{b}$, which  checks that each segment in the finite set of segments $\psi$ has non-negative length and is within bounds of $b$, and that the segments in $\psi$ do not overlap.

The final premise \emph{validates} the proto-expansion and simultaneously generates the \emph{final expansion}, $e$, which appears in the conclusion of the rule. The proto-expression validation judgement is defined in the next subsection.

The typed pattern expansion rule governing pattern TLM application is analagous (see supplement).
% {\small\begin{mathpar}
% \inferrule[pe-ap-sptsm]{
%   \encodeBody{b}{\ebody}\\
%   \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessP}}{\ecand}}\\
%   \decodeCEPat{\ecand}{\cpv}\\\\
%   \segOK{\segof{\cpv}}{b}\\
%   \cvalidP{\upctx}{\pscene{\uDelta}{\uPhi}{b}}{\cpv}{p}{\tau}
% }{
%   \patExpands{\upctx}{\uPhi', \uPhyp{\tsmv}{a}{\tau}{\eparse}}{\utsmap{\tsmv}{b}}{p}{\tau}
% }
% \end{mathpar}}

% \subsection{Syntax of Proto-Expansions}\label{sec:ce-syntax-U}

\subsection{Proto-Expansion Validation}\label{sec:ce-validation-U}\label{sec:ce-syntax-U}\label{sec:s-PEV}


\begin{figure}
\begin{minipage}{\textwidth}
\small
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{PrTyp} & \ctau & ::= & t ~\vert~
\aceparr{\ctau}{\ctau} ~\vert~ \cdots ~\vert~
% \aceall{t}{\ctau} ~\vert~
% \acerec{t}{\ctau} ~\vert~
% \aceprod{\labelset}{\mapschema{\ctau}{i}{\labelset}} \\
% & & \vert & 
% \acesum{\labelset}{\mapschema{\ctau}{i}{\labelset}} ~\vert~ 
\acesplicedt{m}{n}\\
\mathsf{PrExp} & \ce & ::= & x ~\vert~
% \aceasc{\ctau}{\ce} ~\vert~
% \aceletsyn{x}{\ce}{\ce} ~\vert~
\acelam{\ctau}{x}{\ce} ~\vert~
\aceap{\ce}{\ce} ~\vert~
\cdots \\
% \acetlam{t}{\ce} ~\vert~
% \acetap{\ce}{\ctau} ~\vert~
% \acefold{\ce} \\
% & & \vert & \acetpl{\labelset}{\mapschema{\ce}{i}{\labelset}} ~\vert~
% \acein{\ell}{\ce} ~\vert~
& & \vert & 
\acematchwith{n}{\ce}{\seqschemaX{\crv}} ~\vert~
\acesplicede{m}{n}{\ctau}\\
\mathsf{PrRule} & \crv & ::= & \acematchrule{p}{\ce}\\
\mathsf{PrPat} & \cpv & ::= & \acewildp ~\vert~
\cdots ~\vert~
% \acefoldp{p} ~\vert~
% \acetplp{\labelset}{\mapschema{\cpv}{i}{\labelset}} ~\vert~
% \aceinjp{\ell}{\cpv} ~\vert~
\acesplicedp{m}{n}{\ctau} 
\end{array}$
\end{minipage}
\caption[Syntax of proto-types and proto-expressions]{Syntax of proto-expansions. Proto-expansion terms are ABTs identified up to alpha-equivalence.}
\label{fig:U-candidate-terms}
\end{figure}

Finally, we arrive at the crucial \emph{proto-expansion validation judgements}, which validate the proto-expansions generated by TLMs and simultaneously generate their final expansions:% are types and expanded expressions, respectively.
\[\begin{array}{ll}
\cvalidT{\Delta}{\tscenev}{\ctau}{\tau} & \text{$\ctau$ has well-formed expansion $\tau$}\\
\cvalidE{\Delta}{\Gamma}{\escenev}{\ce}{e}{\tau} & \text{$\ce$ has expansion $e$ of type $\tau$}\\
% \cvalidR{\Delta}{\Gamma}{\escenev}{\crv}{r}{\tau}{\tau'} & \text{$\crv$ has expansion $r$ taking values of type $\tau$ to values of type $\tau'$}\\
\cvalidP{\upctx}{\pscenev}{\cpv}{p}{\tau} & \text{$\cpv$ has expansion $p$ matching $\tau$}
\end{array}\]



The purpose of the \emph{splicing scenes} $\tscenev$, $\escenev$ and $\pscenev$ is to ``remember'' the contexts and literal body from the TLM application site (cf. the final premise of Rule \textsc{ee-ap-setsm} in Fig. \ref{fig:ee-def-setsm}) for when validation encounters spliced terms. For example, \emph{expression splicing scenes}, $\escenev$, are of the form $\esceneUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{b}$.

\paragraph{Common Forms} Most of the proto-expansion forms, including all of those elided in Fig. \ref{fig:U-candidate-terms} mirror corresponding expanded forms. The rules governing proto-expansion validation for these common forms  in the supplement correspondingly mirror the typing rules. Splicing scenes---$\escenev$, $\tscenev$ and $\pscenev$--pass opaquely through these rules, i.e. none of these rules can access the application site contexts. This maintains context independence (defined formally below.)

Notice that proto-rules, $\crv$, involve expanded patterns, $p$, not proto-patterns, $\cpv$. The reason is that proto-rules appear in proto-expressions, which are generated by expression TLMs. Proto-patterns, in contrast, arise only from pattern TLMs. There is not a variable proto-pattern form, for the reasons described in Sec. \ref{sec:sptsms}.

\paragraph{References to Spliced Terms} The only interesting forms are the references to spliced unexpanded types, expressions and patterns. Let us consider the rule for references to spliced unexpanded expressions:
{\small\begin{mathpar}
\inferrule[pev-spliced]{
  \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\
  \cvalidT{\emptyset}{\tsceneUP{\uDelta}{b}}{\ctau}{\tau}\\
  \expandsUP{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{\uPhi}{\ue}{e}{\tau}\\\\
  \Delta \cap \Delta_\text{app} = \emptyset\\
  \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset
}{
  \cvalidE{\Delta}{\Gamma}{\esceneUP{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{\uPhi}{b}}{\acesplicede{m}{n}{\ctau}}{e}{\tau}
}
\end{mathpar}}

% {\small\begin{mathpar}
%   \inferrule[ptv-spliced]{
%     \parseUTyp{\bsubseq{b}{m}{n}}{\utau}\\
%     \expandsTU{\uDD{\uD}{\Delta_\text{app}}}{\utau}{\tau}\\
%     \Delta \cap \Delta_\text{app} = \emptyset
%   }{
%     \cvalidT{\Delta}{\tsceneU{\uDD{\uD}{\Delta_\text{app}}}{b}}{\acesplicedt{m}{n}}{\tau}
%   }
% \end{mathpar}}

\noindent
This first premise of this rule parses out the requested segment of the literal body, $b$, to produce an unexpanded expression, $\ue$. The second premise performs proto-type expansion on the given type annotation, $\ctau$, producing a type, $\tau$. The third premise then invokes type expansion on $\ue$ \emph{under the application site contexts}, $\uDD{\uD}{\Delta_\text{app}}$ and $\uGG{\uG}{\Gamma_\text{app}}$, but \emph{not} the expansion-local contexts, $\Delta$ and $\Gamma$.  The final premise requires that the application site contexts are disjoint from the expansion-local type formation context. Because proto-expansions are ABTs identified up to alpha-equivalence, we can always discharge the final premise by alpha-varying the proto-expansion. This serves to enforce capture avoidance. %(we will formally state this property in the next section.) 

The rule for references to spliced unexpanded types and patterns are fundamentally analagous (see supplement). 
% \noindent
% We first splice out the requested segment. The second premise expands the type annotation under an empty context, because the type annotation must be meaningful at the application site (so, independent of $\Delta$ and $\Gamma$) and not itself make any assumptions about the application site context. Spliced types can appear in the annotation. The third premise performs typed expansion of the spliced unexpanded expression under the application site contexts, but not the expansion-local contexts. The final two premises ensure that these contexts are disjoint, again to force capture avoidance.



% The rule for references to spliced unexpanded patterns is entirely analagous (see supplement).
% {\small\begin{mathpar}
% \inferrule[ppv-spliced]{
%   \parseUPat{\bsubseq{b}{m}{n}}{\upv}\\
%   \cvalidT{\emptyset}{\tsceneUP{\uDelta}{b}}{\ctau}{\tau}\\
%   \patExpands{\upctx}{\uPhi}{\upv}{p}{\tau}
% }{
%   \cvalidP{\upctx}{\pscene{\uDelta}{\uPhi}{b}}{\acesplicedp{m}{n}{\ctau}}{p}{\tau}
% }
% \end{mathpar}}


\subsection{Metatheory}\label{sec:s-metatheory}

\subsubsection{Typed Expansion} The first property that we are interested in is simple: that typed expansion produces a well-typed expansion. As it turns out, in order to prove this theorem, we must  prove the following stronger theorem, because the proto-expression validation judgement is defined mutually inductively with the typed expansion judgement (due to splicing).

% \begingroup
% \def\thetheorem{\ref{thm:typed-expansion-full-U}}
\begin{theorem}[Typed Expression Expansion (Strong)] ~
\begin{enumerate}[nolistsep]
\item If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uAS{\uA}{\Psi}}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
\item If $\cvalidE{\Delta}{\Gamma}{\esceneU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uAS{\uA}{\Psi}}{b}}{\ce}{e}{\tau}$ and $\Delta \cap \Delta_\text{app} = \emptyset$ and $\domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset$ then $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$.
\end{enumerate}
\end{theorem}

% \endgroup
The additional second clause simply states that the final expansion produced by proto-expression validation is well-typed under the combined application site and expansion-internal context (because spliced terms are distinguished only in the proto-expansion, not in the final expansion.) The combined context can only be formed if these are disjoint.

The proof proceeds by mutual rule induction and appeal to simple lemmas about type expansion and proto-type validation (see supplement). The proof is straightforward but for one issue: it is not immediately clear that the mutual induction is well-founded, because the case in the proof of part 2 for Rule \textsc{pev-spliced} invokes part 1 of the induction hypothesis on a term that is not a sub-term of the conclusion, but rather parsed out of the literal body, $b$. To establish that the mutual induction is well-founded, then, we need to explicitly establish a decreasing metric. The intuition is that parsing a term of out a literal body cannot produce a bigger term than the term that contained that very literal body. 
% More specifically, {the sum of the lengths of the literal bodies that appear in the term strictly decreases each time you perform a nested TLM application} because some portion of the term has to be consumed by the TLM name and the delimiters. 
The details are given in the supplemental material.

%  A similar argument is needed to prove Typed Pattern Expansion:
% \begin{theorem}[Typed Pattern Expansion (Strong)] ~
% \begin{enumerate}[nolistsep]
%   \item If $\pExpandsSP{\uDD{\uD}{\Delta}}{\uAS{\uA}{\Phi}}{\upv}{p}{\tau}{\uGG{\uG}{\pctx}}$ then $\patType{\pctx}{p}{\tau}$.
%   \item If $\cvalidP{\uGG{\uG}{\pctx}}{\pscene{\uDD{\uD}{\Delta}}{\uAP{\uA}{\Phi}}{b}}{\cpv}{p}{\tau}$ then $\patType{\pctx}{p}{\tau}$.
% \end{enumerate}
% \end{theorem}


\subsubsection{seTLM Reasoning Principles} 

The following theorem summarizes the abstract reasoning principles that programmers can rely on when applying a simple expression TLM. Informal descriptions of the labeled clauses are given inline, in gray boxes. 
% \begingroup
% \def\thetheorem{\ref{thm:tsc-SES}}

\begin{theorem}[seTLM Reasoning Principles]\label{thm:setlm-reasoning} ~\\
If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uPsi}{\utsmap{\tsmv}{b}}{e}{\tau}$ then:
\begin{enumerate}[nolistsep]
\item (\textbf{Typing 1}) $\uPsi = \uPsi', \uShyp{\tsmv}{a}{\tau}{\eparse}$ and $\hastypeU{\Delta}{\Gamma}{e}{\tau}$
  \begin{quote}
  \begin{grayparbox}
     The type of the expansion is consistent with the type annotation on the applied seTLM definition.
  \end{grayparbox}
  \end{quote}
\item $\encodeBody{b}{\ebody}$
\item (\textbf{Responsibility}) $\evalU{\ap{\eparse}{\ebody}}{\aein{\lbltxt{SuccessE}}{\ecand}}$
  \begin{quote}
  \begin{grayparbox}
  The parse function of the invoked TLM is responsible for the expansion.
  \end{grayparbox}
  \end{quote}
\item $\decodeCondE{\ecand}{\ce}$
\item (\textbf{Segmentation}) $\segOK{\segof{\ce}}{b}$
        \begin{quote}
          \begin{grayparbox}
        The segmentation determined by the proto-expansion actually segments the literal body (i.e. each segment is in-bounds and the segments are non-overlapping.)
          \end{grayparbox}
\end{quote}
\item $\segof{\ce} = \sseq{\acesplicedt{m'_i}{n'_i}}{\nty} \cup$\\ $\quad\sseq{\acesplicede{m_i}{n_i}{\ctau_i}}{\nexp}$
\item \textbf{(Typing 2)} $\sseq{
      \expandsTU{\uDD{\uD}{\Delta}}
      {
        \parseUTypF{\bsubseq{b}{m'_i}{n'_i}}
      }{\tau'_i}
    }{\nty}$ and $\sseq{\istypeU{\Delta}{\tau'_i}}{\nty}$
      \begin{quote}
        \begin{grayparbox}

        Each spliced type has a well-formed expansion.
        \end{grayparbox}
\end{quote}
\item \textbf{(Typing 3)} $\sseq{
  \cvalidT{\emptyset}{
    \tsceneUP
      {\uDD
        {\uD}{\Delta}
      }{b}
  }{
    \ctau_i
  }{\tau_i}
}{\nexp}$ and $\sseq{\istypeU{\Delta}{\tau_i}}{\nexp}$
    \begin{quote}
      \begin{grayparbox}
      Each type annotation on a reference to a spliced expression has a well-formed expansion.
      \end{grayparbox}
\end{quote}
\item \textbf{(Typing 4)} ~\\ $\sseq{
  \expandsU
    {\uDD{\uD}{\Delta}}
    {\uGG{\uG}{\Gamma}}
    {\uPsi}
    {\parseUExpF{\bsubseq{b}{m_i}{n_i}}}
    {e_i}
    {\tau_i}
}{\nexp}$ and $\sseq{\hastypeU{\Delta}{\Gamma}{e_i}{\tau_i}}{\nexp}$
    \begin{quote}
      \begin{grayparbox}
      Each spliced expression has a well-typed expansion consistent with the type annotation in the segmentation.
      \end{grayparbox}
\end{quote}
\item (\textbf{Capture Avoidance}) ~\\$e = [\sseq{\tau'_i/t_i}{\nty}, \sseq{e_i/x_i}{\nexp}]e'$ for some variables $\sseq{t_i}{\nty}$ and $\sseq{x_i}{\nexp}$, and $e'$
    \begin{quote}
    \begin{grayparbox}
      The final expansion can be decomposed into a  term with variables in place of each spliced type or expression. The expansions of these spliced types and expressions can be substituted into this term in the standard capture avoiding manner.
    \end{grayparbox}
    \end{quote}
\item (\textbf{Context Independence}) ~\\$\mathsf{fv}(e') \subset \sseq{t_i}{\nty} \cup \sseq{x_i}{\nexp}$
    \begin{quote}
      \begin{grayparbox}

      The decomposed term makes no mention of bindings in the application site context, i.e. the only free variables are those standing for spliced terms.
      \end{grayparbox}
\end{quote}
  % $\hastypeU
  % {\sseq{\Dhyp{t_i}}{\nty}}
  % {\sseq{x_i : \tau_i}{\nexp}}
  % {e'}{\tau}$
\end{enumerate}
\end{theorem}


% The proof, which involves auxiliary lemmas about the decomposition of proto-types and proto-expressions, is given in the supplement.

Notice that we were able to state the hygiene properties (\textbf{Capture Avoidance} and \textbf{Context Independence}) without needing a notion of alpha-equivalence of source terms, as in typical formal accounts of hygiene \cite{Kohlbecker86a,DBLP:conf/popl/Adams15,DBLP:conf/popl/ClingerR91,DBLP:journals/lisp/DybvigHB92,DBLP:conf/esop/HermanW08,Herman10:Theory}. Instead, we used standard notions of capture avoiding substitution and free variables combined with the context disjointness conditions in the rules above. This is possible only because we keep track of spliced terms explicitly in the proto-expansion, rather than going straight to the final expansion. %In fact, doing so is critical for TLMs -- there is no notion of alpha-conversion for partially parsed terms, so any notion of hygiene that relies on this notion would be inapplicable.

The reasoning principles theorem for pattern TLMs is in the supplement. The key clause establishes that the hypotheses generated by the TLM application form are exactly the union of the hypothesis generated by the spliced patterns. % No additional hypotheses can be produced by the TLM.

In the full calculus, which supports parametric TLMs, the context independence clause allows reference to the variables standing for parameters.

\newcommand{\pTLMsFormallySec}{Parametric TLMs, Formally}
\section{\protect\pTLMsFormallySec}
\label{sec:ptlms-formally}

% \begin{figure}[p]
% \[\begin{array}{llcl}
% \mathsf{UMType} & \urho & ::= 
% %& \autype{\utau} 
% & \utau & \text{type annotation}\\
% % &&
% %& \aualltypes{\ut}{\urho} 
% % & \alltypes{\ut}{\urho} & \text{type parameterization}\\
% &&
% %& \auallmods{\usigma}{\uX}{\urho} 
% & \allmods{\uX}{\usigma}{\urho} & \text{module parameterization}\\
% \mathsf{UMExp} & \uepsilon & ::= 
% %& \abindref{\tsmv} 
% & \tsmv & \text{TLM identifier reference}\\
% % &&
% %& \auabstype{\ut}{\uepsilon} 
% % & \abstype{\ut}{\uepsilon} & \text{type abstraction}\\
% &&
% %& \auabsmod{\usigma}{\uX}{\uepsilon} 
% & \absmod{\uX}{\usigma}{\uepsilon} & \text{module abstraction}\\
% % &&
% %& \auaptype{\utau}{\uepsilon} 
% % & \aptype{\uepsilon}{\utau} & \text{type application}\\
% &&
% %& \auapmod{\uM}{\uepsilon} 
% & \apmod{\uepsilon}{\uX} & \text{module application}\ECC
% \end{array}
% \]
% \caption{Syntax of unexpanded TLM types and expressions.}
% \label{fig:P-macro-expressions-types-u}
% \end{figure}

We will now outline $\miniVerseParam$, a calculus that extends $\miniVersePat$ with parametric TLMs. This calculus is organized, like $\miniVersePat$, as an unexpanded language (UL) defined by typed expansion to an expanded language (XL). There is not enough space to describe $\miniVerseParam$ with the same level of detail as in Sec. \ref{sec:setsms-formally}, so we highlight only the most important concepts below. The details are in the supplement.

The XL consists of 1) module expressions, $M$, classified by signatures, $\sigma$; 2) constructions, $c$, classified by kinds, $\kappa$; and 3) expressions classified by types, which are constructions of kind $\akty$ (we use metavariables $\tau$ instead of $c$ for types by convention.) Metavariables $X$ ranges over module variables and $u$ or $t$ over construction variables. The module and construction languages are based closely on those defined by \citet{pfple1}, which in turn are based on early work by \citet{MacQueen:1984:MSM:800055.802036,DBLP:conf/popl/MacQueen86}, subsequent work on the phase splitting interpretation of modules \cite{harper1989higher} and on using dependent singleton kinds to track type identity \cite{stone2006extensional,DBLP:conf/lfmtp/Crary09}, and finally on formal developments by \citet{dreyer2005understanding} and \citet{conf/popl/LeeCH07}. A complete account of these developments is unfortunately beyond the scope of this paper. The expression language extends the language of $\miniVersePat$ only to allow projection out of modules.

The main conceptual difference between $\miniVersePat$ and $\miniVerseParam$ is that $\miniVerseParam$ introduces the notion of unexpanded and expanded TLM expressions and types, as shown in Fig. \ref{fig:P-macro-expressions-types}. 

\begin{figure}[h]
\small
\begin{minipage}{0.38\textwidth}
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{UMType} & \urho & ::= & \utau ~\vert~ \allmods{\uX}{\usigma}{\urho}\\
\mathsf{UMExp} & \uepsilon & ::= & \tsmv ~\vert~ \absmod{\uX}{\usigma}{\uepsilon}~\vert~ \apmod{\uepsilon}{\uX}
\end{array}$
\end{minipage}
\begin{minipage}{0.6\textwidth}
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{MType} & \rho & ::= & \aetype{\tau} ~\vert~ \aeallmods{\sigma}{X}{\rho} \\
\mathsf{MExp} & \epsilon & ::= & \adefref{a} ~\vert~ \aeabsmod{\sigma}{X}{\epsilon} ~\vert~\aeapmod{X}{\epsilon} 
\end{array}$
\end{minipage}
\caption{Syntax of unexpanded and expanded TLM types and expressions in $\miniVerseParam$}
\label{fig:P-macro-expressions-types}
\end{figure}
The TLM type $\aeallmods{\sigma}{X}{\rho}$ classifies TLM expressions that have one module parameter matching $\sigma$. For simplicity, we formalize only module parameters. Type parameters can be expressed as module parameters having exactly one abstract type member.

The rule governing expression TLM application, reproduced below, touches all of the main ideas in $\miniVerseParam$, so we will refer to it throughout the remainder of this section.%It might be useful to compare this rule to Rule \textsc{ee-ap-sptsm}.
{\small\begin{mathpar}
\inferrule[ee-ap-petsm]{
  \uOmega = \uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}\\
  \uPsi=\uAS{\uA}{\Psi}\\\\
  \tsmexpExpandsExp{\uOmega}{\uPsi}{\uepsilon}{\epsilon}{\aetype{\tau_\text{final}}}\\
  \tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}\\\\
  \tsmdefof{\epsilon_\text{normal}}=a\\
  \Psi = \Psi', \petsmdefn{a}{\rho}{\eparse}\\\\
  \encodeBody{b}{\ebody}\\
  \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{e_\text{pproto}}}\\
  \decodePCEExp{e_\text{pproto}}{\pce}\\\\
  \prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon_\text{normal}}{\aetype{\tau_\text{proto}}}{\omega}{\Omega_\text{params}}\\\\
  \segOK{\segof{\ce}}{b}\\
  \cvalidEP{\Omega_\text{params}}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\ce}{e}{\tau_\text{proto}}
}{
  \expandsP{\uOmega}{\uPsi}{\uPhi}{\utsmap{\uepsilon}{b}}{[\omega]e}{[\omega]\tau_\text{proto}}
}
\end{mathpar}}

The first two premises simply deconstruct the (unified) unexpanded context $\uOmega$ (which tracks the expansion of expression, constructor and module identifiers, as $\uDelta$ and $\uGamma$ did in $\miniVersePat$) and peTLM context, $\uPsi$. Next, we expand $\uepsilon$ according to straightforward unexpanded peTLM expression expansion rules. The resulting TLM expression, $\epsilon$, must be defined at a type (i.e. no quantification over modules must remain once the literal body is encountered.)

The fourth premise performs \emph{peTLM expression normalization}, $\tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}$. This is defined in terms of a structural operational semantics \cite{DBLP:journals/jlp/Plotkin04a} with two stepping rules:
% \begin{equation*}\tag{\ref{rule:tsmexpEvalsExp}}
% \inferrule{
%   \tsmexpMultistepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}\\
%   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon'}
% }{
%   \tsmexpEvalsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
% }
% \end{equation*}
% where the multistep judgement, $\tsmexpMultistepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}$, is defined as the reflexive, transitive closure of the stepping judgement defined by the following rules:
% \begin{equation*}\tag{\ref{rule:tsmexpStepsExp-aptype-1}}
% \inferrule{
%   \tsmexpStepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
% }{
%   \tsmexpStepsExp{\Omega}{\Psi}{\aeaptype{\tau}{\epsilon}}{\aeaptype{\tau}{\epsilon'}}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpStepsExp-aptype-2}}
% \inferrule{ }{
%   \tsmexpStepsExp{\Omega}{\Psi}{\aeaptype{\tau}{\aeabstype{t}{\epsilon}}}{[\tau/t]\epsilon}
% }
% \end{equation*}
{\small\begin{mathpar}
\inferrule[eps-dyn-apmod-subst-e]{ }{
  \tsmexpStepsExp{\Omega}{\Psi}{\aeapmod{X}{\aeabsmod{\sigma}{X'}{\epsilon}}}{[X/X']\epsilon}
}

\inferrule[eps-dyn-apmod-steps-e]{
  \tsmexpStepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
}{
  \tsmexpStepsExp{\Omega}{\Psi}{\aeapmod{X}{\epsilon}}{\aeapmod{X}{\epsilon'}}
}
\end{mathpar}}
% The peTLM expression normal forms are defined as follows:
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-defref}}
% \inferrule{ }{
%   \tsmexpNormalExp{\Omega}{\Psi, \petsmdefn{a}{\rho}{\eparse}}{\adefref{a}}
% }
% \end{equation*}
% % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-abstype}}
% % \inferrule{ }{
% %   \tsmexpNormalExp{\Omega}{\Psi}{\aeabstype{t}{\epsilon}}
% % }
% % \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-absmod}}
% \inferrule{ }{
%   \tsmexpNormalExp{\Omega}{\Psi}{\aeabsmod{\sigma}{X}{\epsilon}}
% }
% \end{equation*}
% % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-aptype}}
% % \inferrule{
% %   \epsilon \neq \aeabstype{t}{\epsilon'}\\
% %   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon}
% % }{
% %   \tsmexpNormalExp{\Omega}{\Psi}{\aeaptype{\tau}{\epsilon}}
% % }
% % \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-apmod}}
% \inferrule{
%   \epsilon \neq \aeabsmod{\sigma}{X'}{\epsilon'}\\
%   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon}
% }{
%   \tsmexpNormalExp{\Omega}{\Psi}{\aeapmod{X}{\epsilon}}
% }
% \end{equation*}
\vspace{-5px}

Normalization eliminates parameters introduced in higher-order abbreviations, leaving only those parameter applications specified by the original TLM definition. Normal forms and progress and preservation theorems are established in the supplement.

The third row of premises looks up the applied TLM's definition by invoking a simple metafunction to extract its name, $a$, then looking up $a$ within the peTLM definition context, $\Psi$.
% \begin{align}
% \tsmdefof{\adefref{a}} & = a \tag{\ref{eqn:tsmdefof-adefref}}\\
% % \tsmdefof{\aeabstype{t}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-abstype}}\\
% \tsmdefof{\aeabsmod{\sigma}{X}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-absmod}}\\
% % \tsmdefof{\aeaptype{\tau}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-aptype}}\\
% \tsmdefof{\aeapmod{X}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-apmod}}
% \end{align}


The fourth row of premises 1) encodes the body as a value of the type $\tBody$; 2) applies the parse function; and 3) decodes the result, producing a \emph{parameterized proto-expression}, $\pce$. Parameterized proto-expressions, $\pce$, are ABTs that serve simply to introduce the parameter bindings into an underlying proto-expression, $\ce$. The syntax of parameterized proto-expressions is given below.

\vspace{-4px}{\small\[\begin{array}{llcl}
% \textbf{Sort} & & & \textbf{Operational Form} & \textbf{Stylized Form} & \textbf{Description}\\
% \LCC \color{Yellow}&\color{Yellow}&\color{Yellow}& \color{Yellow} & \color{Yellow} & \color{Yellow}\\
\mathsf{PPrExp} & \pce & ::= & \apceexp{\ce} ~\vert~ \apcebindmod{X}{\pce}
\end{array}\]}%
\vspace{-6px}%

There must be one binder in $\pce$ for each TLM parameter specified by $a$. (In Reason, we can insert these binders automatically as a convenience.) 

The judgement on the fifth row of Rule \textsc{ee-ap-petsm} then \emph{deparameterizes} $\pce$ by peeling away these binders to produce 1) the underlying proto-expression, $\ce$, with the variables that stand for the parameters free; 2) a corresponding deparameterized type, $\tau_\text{proto}$, that uses the same free variables to stand for the parameters; 3) a \emph{substitution}, $\omega$, that pairs the applied parameters from $\epsilon_\text{normal}$ with the corresponding variables generated when peeling away the binders in $\pce$; and 4) a corresponding \emph{parameter context}, $\Omega_\text{params}$, that tracks the signatures of these variables. The two rules governing the proto-expression deparameterization judgement are below:
{\small\begin{mathpar}
\inferrule{ }{
  \prepce{\Omega_\text{app}}{\Psi, \petsmdefn{a}{\rho}{\eparse}}{\apceexp{\ce}}{\ce}{\adefref{a}}{\rho}{\emptyset}{\emptyset}
}

\vspace{-4px}\inferrule{
  \prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon}{\aeallmods{\sigma}{X}{\rho}}{\omega}{\Omega}\\
  X \notin \domof{\Omega_\text{app}}
}{
  \prepce{\Omega_\text{app}}{\Psi}{\apcebindmod{X}{\pce}}{\ce}{\aeapmod{X'}{\epsilon}}{\rho}{(\omega, X'/X)}{(\Omega, X : \sigma)}
}
\end{mathpar}}%
This judgement can be pronounced ``when applying peTLM $\epsilon$, $\pce$ has deparameterization $\ce$ leaving $\rho$ with parameter substitution $\omega$''. 
Notice based on the second rule that every module binding in $\pce$ must pair with a corresponding module parameter application. Moreover, the variables standing for parameters must not appear in $\Omega_\text{app}$, i.e. $\domof{\Omega_\text{params}}$ must be disjoint from $\domof{\Omega_\text{app}}$ (this requirement can always be discharged by alpha-variation.)

The final row of premises checks that the segmentation of $\ce$ is valid and  performs proto-expansion validation under the parameter context, $\Omega_\text{param}$ (rather than the empty context, as was the case in $\miniVersePat$.) The conclusion of the rule applies the parameter substitution, $\omega$, to the resulting expression and the deparameterized type.

Proto-expansion validation operates conceptually as in $\miniVersePat$. The only subtlety has to do with the type annotations on references to spliced terms. As described at the end of Sec. \ref{sec:ptsms-by-example}, these annotations might refer to the parameters, so the parameter substitution, $\omega$, which is tracked by the splicing scene, must be applied to the type annotation before proceeding recursively to expand the referenced unexpanded term. However, the spliced term itself must treat parameters parametrically, so the substitution is not applied in the conclusion of the following rule:
\begin{mathpar}\label{rule:cvalidE-P-splicede}
\inferrule{
  \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\
    \cvalidC{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\ctau}{\tau}{\akty}\\
  \expandsP{\uOmega}{\uPsi}{\uPhi}{\ue}{e}{[\omega]\tau}\\\\
  \uOmega=\uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}\\
  \domof{\Omega} \cap \domof{\Omega_\text{app}} = \emptyset
}{
  \cvalidEP{\Omega}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\acesplicede{m}{n}{\ctau}}{e}{\tau}
}
\end{mathpar}
(This is only sensible because we maintain the invariant that $\Omega$ is always an extension of $\Omega_\text{params}$.)

The calculus enjoys metatheoretic properties analagous to those described in Sec. \ref{sec:s-metatheory}, modified to account for the presence of modules, kinds and parameterization. The following theorem establishes the abstract reasoning principles available when applying a parametric expression TLM. The clauses are directly analagous to those of Theorem \ref{thm:setlm-reasoning}, so for reasons of space we do not repeat the inline descriptions. The \textbf{Kinding} clauses can be understood by analogy to the \textbf{Typing} clauses. The details of parametric pattern TLMs (ppTLMs) are analagous (see supplement.)
\vspace{-3px}
\begin{theorem}[peTLM Reasoning Principles]
If $\expandsP{\uOmega}{\uPsi}{\uPhi}{\utsmap{\uepsilon}{b}}{e}{\tau}$ then:
\begin{enumerate}[nolistsep,noitemsep]
  \item $\uOmega=\uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}$
  \item $\uPsi=\uAS{\uA}{\Psi}$
  \item (\textbf{Typing 1}) $\tsmexpExpandsExp{\uOmega}{\uPsi}{\uepsilon}{\epsilon}{\aetype{\tau}}$ and $\hastypeP{\Omega_\text{app}}{e}{\tau}$
  \item $\tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}$
  \item $\tsmdefof{\epsilon_\text{normal}}=a$
  \item $\Psi = \Psi', \petsmdefn{a}{\rho}{\eparse}$
  \item $\encodeBody{b}{\ebody}$
    \item $\evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{e_\text{pproto}}}$
  \item $\decodePCEExp{e_\text{pproto}}{\pce}$
  \item $\prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon_\text{normal}}{\aetype{\tau_\text{proto}}}{\omega}{\Omega_\text{params}}$
  \item (\textbf{Segmentation}) $\segOK{\segof{\ce}}{b}$
  \item $\cvalidEP{\Omega_\text{params}}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\ce}{e'}{\tau_\text{proto}}$
  \item $e = [\omega]e'$
  \item $\tau = [\omega]\tau_\text{proto}$
  \item $
    \segof{\ce} = \sseq{\acesplicedk{m_i}{n_i}}{\nkind} \cup \sseq{\acesplicedc{m'_i}{n'_i}{\cekappa'_i}}{\ncon} \cup $ \\
     ~~~~$          \sseq{\acesplicede{m''_i}{n''_i}{\ctau_i}}{\nexp}
    $
  \item (\textbf{Kinding 1}) $\sseq{\kExpands{\uOmega}{\parseUKindF{\bsubseq{b}{m_i}{n_i}}}{\kappa_i}}{\nkind}$ and \\ ~~~~$\sseq{\iskind{\Omega_\text{app}}{\kappa_i}}{\nkind}$
  \item (\textbf{Kinding 2}) $\sseq{\cvalidK{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\cekappa'_i}{\kappa'_i}}{\ncon}$ and $\sseq{\iskind{\Omega_\text{app}}{[\omega]\kappa'_i}}{\ncon}$
  \item (\textbf{Kinding 3}) $\sseq{\cExpands{\uOmega}{\parseUConF{\bsubseq{b}{m'_i}{n'_i}}}{c_i}{[\omega]\kappa'_i}}{\ncon}$ and $\sseq{\haskind{\Omega_\text{app}}{c_i}{[\omega]\kappa'_i}}{\ncon}$
  \item (\textbf{Kinding 4}) $\sseq{\cvalidC{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\ctau_i}{\tau_i}{\akty}}{\nexp}$ and $\sseq{\haskind{\Omega_\text{app}}{[\omega]\tau_i}{\akty}}{\nexp}$
  \item (\textbf{Typing 2}) $\sseq{\expandsP{\uOmega}{\uPsi}{\uPhi}{\parseUExpF{\bsubseq{b}{m''_i}{n''_i}}}{e_i}{[\omega]\tau_i}}{\nexp}$ and $\sseq{\hastypeP{\Omega_\text{app}}{e_i}{[\omega]\tau_i}}{\nexp}$
  \item (\textbf{Capture Avoidance}) $e = [\sseq{\kappa_i/k_i}{\nkind}, \sseq{c_i/u_i}{\ncon}, \sseq{e_i/x_i}{\nexp}, \omega]e''$ for some $e''$ and fresh $\sseq{k_i}{\nkind}$ and fresh $\sseq{u_i}{\ncon}$ and fresh $\sseq{x_i}{\nexp}$
  \item (\textbf{Context Independence}) $\mathsf{fv}(e'') \subset \sseq{k_i}{\nkind} \cup \sseq{u_i}{\ncon} \cup \sseq{x_i}{\nexp} \cup \domof{\OParams}$
  % $\hastypeP{\sseq{\Khyp{k_i}}{\nkind} \cup \sseq{u_i :: [\omega]\kappa'_i}{\ncon} \cup \sseq{x_i : [\omega]\tau_i}{\nexp}}{[\omega]e''}{\tau}$\todo{maybe restate this in terms of free variables of e'' here and elsewhere, because context isn't technically well-formed here?}
\end{enumerate}
\end{theorem}

\vspace{-6px}

% The following theorem summarizes the abstract reasoning principles available to programmers when applying a pattern TLM. Most of the labeled clauses are analagous to those described above, so we omit their descriptions.
% % \begingroup
% % \def\thetheorem{\ref{thm:spTLM-Typing-Segmentation}}
% \begin{theorem}[spTLM Reasoning Principles]
% % \label{thm:spTLM-Typing-Segmentation}
% If $\patExpands{\upctx}{\uPhi}{\utsmap{\tsmv}{b}}{p}{\tau}$ where $\uDelta=\uDD{\uD}{\Delta}$ and $\uGamma=\uGG{\uG}{\Gamma}$ then all of the following hold:
% \begin{enumerate}[noitemsep,nolistsep]
%         \item (\textbf{Typing 1}) $\uPhi=\uPhi', \uPhyp{\tsmv}{a}{\tau}{\eparse}$ and $\patType{\pctx}{p}{\tau}$
%         \item $\encodeBody{b}{\ebody}$
%         \item $\evalU{\eparse(\ebody)}{\aein{\mathtt{SuccessP}}{\ecand}}$
%         \item $\decodeCEPat{\ecand}{\cpv}$
%         \item (\textbf{Segmentation}) $\segOK{\segof{\cpv}}{b}$
%         \item $\segof{\cpv} = \sseq{\acesplicedt{n'_i}{m'_i}}{\nty} \cup \sseq{\acesplicedp{m_i}{n_i}{\ctau_i}}{\npat}$
%         \item (\textbf{Typing 2}) $\sseq{
%               \expandsTU{\uDelta}
%               {
%                 \parseUTypF{\bsubseq{b}{m'_i}{n'_i}}
%               }{\tau'_i}
%             }{\nty}$ and $\sseq{\istypeU{\Delta}{\tau'_i}}{\nty}$
%         \item (\textbf{Typing 3}) $\sseq{
%           \cvalidT{\emptyset}{
%             \tsceneUP
%               {\uDelta}{b}
%           }{
%             \ctau_i
%           }{\tau_i}
%         }{\npat}$ and $\sseq{\istypeU{\Delta}{\tau_i}}{\npat}$
%         \item (\textbf{Typing 4}) $\sseq{
%           \patExpands
%             {\uGG{\uG_i}{\pctx_i}}
%             {\uPhi}
%             {\parseUPatF{\bsubseq{b}{m_i}{n_i}}}
%             {p_i}
%             {\tau_i}
%         }{\npat}$  and $\sseq{\patType{\pctx_i}{p_i}{\tau_i}}{\npat}$
%       \item (\textbf{Visibility}) $\uG = \biguplus_{0 \leq i < \npat} \uG_i$ and $\Gamma = \bigcup_{0 \leq i < \npat} \pctx_i$
%         \begin{quote}
%           \begin{grayparbox}
%           The hypotheses generated by the TLM application are exactly those generated by the spliced patterns.
%           \end{grayparbox}
%         \end{quote}
% \end{enumerate}
% \end{theorem}
% 
% \begin{proof} 
% The proof, in the supplement, relies on an auxiliary lemma about decomposing proto-patterns.

% \subsection{Retrospective} Let us step back and briefly review what we have accomplished so far. First, we defined an entirely standard expanded language (Sec. \ref{sec:s-XL}). Then we mirrored the forms in this language to form the common forms of the unexpanded language (Sec. \ref{sec:s-UL}), taking care to handle identifiers carefully (Sec. \ref{sec:s-TE}). We also added forms for defining TLMs (Sec. \ref{sec:s-TLM-def}). We then considered TLM application, which invokes a parse function to programmatically produce an encoding of a proto-expansion (Sec. \ref{sec:s-TLM-ap}). Each proto-expansion is then decoded, validated and inductively expanded  (Sec. \ref{sec:s-PEV}) to establish the abstract reasoning principles just described (Sec. \ref{sec:s-metatheory}). 
% We write $\tsfrom{\escenev}$ for the type splicing scene constructed by dropping unnecessary contexts from $\escenev$:
% \[\tsfrom{\esceneUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{b}} = \tsceneUP{\uDelta}{b}\]

% Figure \ref{fig:U-candidate-terms} defines the syntax of proto-types, $\ctau$, and proto-expressions, $\ce$. Proto-types and -expressions 

% Each expanded form maps onto a proto-expansion form. We refer to these as the \emph{common proto-expansion forms}. The mapping is given explicitly in Appendix \ref{appendix:proto-expansions-SES}.

% There are two ``interesting'' proto-expansion forms, highlighted in yellow in Figure \ref{fig:U-candidate-terms}: a proto-type form for \emph{references to spliced unexpanded types}, $\acesplicedt{m}{n}$, and a proto-expression form for \emph{references to spliced unexpanded expressions}, $\acesplicede{m}{n}{\ctau}$, where $m$ and $n$ are natural numbers.%TLM utilize these to splice types and unexpanded expressions out of literal bodies.


% \subsubsection{Proto-Type Validation}\label{sec:SE-proto-type-validation}
% The \emph{proto-type validation judgement}, $\cvalidT{\Delta}{\tscenev}{\ctau}{\tau}$, is inductively defined by Rules (\ref{rules:cvalidT-U}).

% \paragraph{Common Forms} Rules (\ref{rule:cvalidT-U-tvar}) through (\ref{rule:cvalidT-U-sum}) validate proto-types of common form. The first three of these are reproduced below.
% %Each of these rules is defined based on the corresponding type formation rule, i.e. Rules (\ref{rule:istypeU-var}) through (\ref{rule:istypeU-sum}), respectively. For example, the following proto-types validation rules are based on type formation rules (\ref{rule:istypeU-var}), (\ref{rule:istypeU-parr}) and (\ref{rule:istypeU-all}), respectively: 
% % \begin{subequations}%\label{rules:cvalidT-U}
% \begin{equation*}\tag{\ref{rule:cvalidT-U-tvar}}
% \inferrule{ }{
%   \cvalidT{\Delta, \Dhyp{t}}{\tscenev}{t}{t}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidT-U-parr}}
%   \inferrule{
%     \cvalidT{\Delta}{\tscenev}{\ctau_1}{\tau_1}\\
%     \cvalidT{\Delta}{\tscenev}{\ctau_2}{\tau_2}
%   }{
%     \cvalidT{\Delta}{\tscenev}{\aceparr{\ctau_1}{\ctau_2}}{\aparr{\tau_1}{\tau_2}}
%   }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidT-U-all}}
%   \inferrule {
%     \cvalidT{\Delta, \Dhyp{t}}{\tscenev}{\ctau}{\tau}
%   }{
%     \cvalidT{\Delta}{\tscenev}{\aceall{t}{\ctau}}{\aall{t}{\tau}}
%   }
% \end{equation*}
% % \begin{equation*}\label{rule:cvalidT-U-rec}
% %   \inferrule{
% %     \cvalidT{\Delta, \Dhyp{t}}{\tscenev}{\ctau}{\tau}
% %   }{
% %     \cvalidT{\Delta}{\tscenev}{\acerec{t}{\ctau}}{\arec{t}{\tau}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidT-U-prod}
% %   \inferrule{
% %     \{\cvalidT{\Delta}{\tscenev}{\ctau_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \cvalidT{\Delta}{\tscenev}{\aceprod{\labelset}{\mapschema{\ctau}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidT-U-sum}
% %   \inferrule{
% %     \{\cvalidT{\Delta}{\tscenev}{\ctau_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \cvalidT{\Delta}{\tscenev}{\acesum{\labelset}{\mapschema{\ctau}{i}{\labelset}}}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}

% These rules, like the rules for common unexpanded type forms,  mirror the corresponding type formation rules, i.e. Rules (\ref{rules:istypeU}). The type splicing scene, $\tscenev$, passes opaquely through these rules. 
% % We can express this scheme more precisely with the following rule transformation. For each rule in Rules (\ref{rules:istypeU}), 
% % \begin{mathpar}
% % % \refstepcounter{equation}
% % % \label{rule:cvalidT-U-rec}
% % % \refstepcounter{equation}
% % % \label{rule:cvalidT-U-prod}
% % % \refstepcounter{equation}
% % % \label{rule:cvalidT-U-sum}
% % % \inferrule{J_1\\\cdots\\J_k}{J}
% % \end{mathpar}
% % the corresponding proto-types validation rule is
% % \begin{mathpar}
% % \inferrule{
% %   \VTypof{J_1}\\
% %   \cdots\\
% %   \VTypof{J_k}
% % }{
% %   \VTypof{J}
% % }
% % \end{mathpar}
% % where 
% % \[\begin{split}
% % \VTypof{\istypeU{\Delta}{\tau}} & = \cvalidT{\Delta}{\tscenev}{\VTypof{\tau}}{\tau}\\
% % \VTypof{\{J_i\}_{i \in \labelset}} & = \{\VTypof{J_i}\}_{i \in \labelset}
% % \end{split}\]
% % and where $\VTypof{\tau}$, when $\tau$ is a metapattern of sort $\mathsf{Typ}$, is a metapattern of sort $\mathsf{CETyp}$ defined as follows:
% % \begin{itemize}
% % \item When $\tau$ is of definite form, $\VTypof{\tau}$ is defined as follows:
% % \begin{align*}
% % \VTypof{t} & = t\\
% % \VTypof{\aparr{\tau_1}{\tau_2}} & = \aceparr{\VTypof{\tau_1}}{\VTypof{\tau_2}}\\
% % \VTypof{\aall{t}{\tau}} & = \aceall{t}{\VTypof{\tau}}\\
% % \VTypof{\arec{t}{\tau}} & = \acerec{t}{\VTypof{\tau}}\\
% % \VTypof{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}} & = \aceprod{\labelset}{\mapschemax{\VTypofv}{\tau}{i}{\labelset}}\\
% % \VTypof{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}} & = \acesum{\labelset}{\mapschemax{\VTypofv}{\tau}{i}{\labelset}}
% % \end{align*}
% % \item When $\tau$ is of indefinite form, $\VTypof{\tau}$ is a uniquely corresponding metapattern also of indefinite form. For example, $\VTypof{\tau_1}=\ctau_1$ and $\VTypof{\tau_2}=\ctau_2$.
% % \end{itemize}

% % It is instructive to use this rule transformation to generate Rules (\ref{rule:cvalidT-U-tvar}) through (\ref{rule:cvalidT-U-all}) above. We omit the remaining rules, i.e. Rules (\ref*{rule:cvalidT-U-rec}) through (\ref*{rule:cvalidT-U-sum}). 

% Notice that in Rule (\ref{rule:cvalidT-U-tvar}), only type variables tracked by $\Delta$, the expansion's local type validation context, are well-formed. Type variables tracked by the application site unexpanded type formation context, which is a component of the type splicing scene, $\tscenev$, are not validated. %Indeed, $\tscenev$ passes opaquely through the rules above. %This achieves \emph{context-independent expansion} as described in Sec. \ref{sec:splicing-and-hygiene} for type variables -- seTLMs cannot impose ``hidden constraints'' on the application site unexpanded type formation context, because the type variables bound at the application site are simply not directly available to proto-types.

% \paragraph{References to Spliced Types} The only proto-type form that does not correspond to a type form is $\acesplicedt{m}{n}$, which is a \emph{reference to a spliced unexpanded type}, i.e. it indicates that an unexpanded type should be parsed out from the literal body, which appears in the type splicing scene $\tscenev$, beginning at position $m$ and ending at position $n$, where $m$ and $n$ are natural numbers. Rule (\ref{rule:cvalidT-U-splicedt}) governs this form:
% \begin{equation*}\tag{\ref{rule:cvalidT-U-splicedt}}
%   \inferrule{
%     \parseUTyp{\bsubseq{b}{m}{n}}{\utau}\\
%     \expandsTU{\uDD{\uD}{\Delta_\text{app}}}{\utau}{\tau}\\
%     \Delta \cap \Delta_\text{app} = \emptyset
%   }{
%     \cvalidT{\Delta}{\tsceneU{\uDD{\uD}{\Delta_\text{app}}}{b}}{\acesplicedt{m}{n}}{\tau}
%   }
% \end{equation*}
% The first premise of this rule extracts the indicated subsequence of $b$ using the partial metafunction $\bsubseq{b}{m}{n}$ and parses it using the partial metafunction $\mathsf{parseUTyp}(b)$, characterized in Sec. \ref{sec:syntax-U}, to produce the spliced unexpanded type itself, $\utau$.

% The second premise of Rule (\ref{rule:cvalidT-U-splicedt}) performs type expansion of $\utau$ under the application site unexpanded type formation context, $\uDD{\uD}{\Delta_\text{app}}$, which is a component of the type splicing scene. The hypotheses in the expansion's local type formation context, $\Delta$, are not made available to $\tau$. %This enforces the injunction on shadowing as described in Sec. \ref{sec:splicing-and-hygiene} for type variables that appear in proto-types. 

% The third premise of Rule (\ref{rule:cvalidT-U-splicedt}) imposes the constraint that the proto-expansion's type formation context, $\Delta$, be disjoint from the application site type formation context, $\Delta_\text{app}$. This premise can always be discharged by $\alpha$-varying the proto-expansion that the reference to the spliced type appears within. 

% Together, these two premises enforce the injunction on type variable capture as described in Sec. \ref{sec:uetsms-validation} -- the TLM provider can choose type variable names freely within a proto-expansion. We will consider this formally in Sec. \ref{sec:SE-metatheory} below. %, because the language prevents them from shadowing type variables at the application site (by $\alpha$-varying the proto-expansion as needed.)%Such a change in bound variable names is possible again because variables bound by the seTLM provider in a proto-expansion cannot ``leak into'' spliced terms because the hypotheses in $\Delta$ are not made available to the spliced type, $\tau$. 

% Rules (\ref{rules:cvalidT-U}) validate the following lemma, which establishes that the final expansion of a valid proto-type is a well-formed type under the combined type formation context.
% \begingroup
% \def\thetheorem{\ref{lemma:candidate-expansion-type-validation}}
% \begin{lemma}[Proto-Expansion Type Validation]
% If $\cvalidT{\Delta}{\tsceneU{\uDD{\uD}{\Delta_\text{app}}}{b}}{\ctau}{\tau}$ and $\Delta \cap \Delta_\text{app}=\emptyset$ then $\istypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\tau}$.
% \end{lemma}
% \endgroup

% \subsubsection{Proto-Expression Validation}
% The \emph{proto-expression validation judgement}, $\cvalidE{\Delta}{\Gamma}{\escenev}{\ce}{e}{\tau}$, is defined mutually inductively with the typed expansion judgement by Rules (\ref{rules:cvalidE-U}) as follows.% This is necessary because a typed expansion judgement appears as a premise in Rule (\ref{rule:cvalidE-U-splicede}) below, and a proto-expression validation judgement appears as a premise in Rule (\ref{rule:expandsU-tsmap}) above.

% \paragraph{Common Forms} Rules (\ref{rule:cvalidE-U-var}) through (\ref{rule:cvalidE-U-case}) validate proto-expressions of common form, as well as ascriptions and let binding. The first five of these rules are reproduced below:
% %For each expanded expression form defined in Figure \ref{fig:U-expanded-terms}, Figure \ref{fig:U-candidate-terms} defines a corresponding proto-expression form. The validation rules for proto-expressions of these forms are each based on the corresponding typing rule in Rules (\ref{rules:hastypeU}). For example, the validation rules for proto-expressions of variable, function and function application form  are based on Rules (\ref{rule:hastypeU-var}) through (\ref{rule:hastypeU-ap}), respectively:
% %\begin{subequations}%\label{rules:cvalidE-U}
% \begin{equation*}\tag{\ref{rule:cvalidE-U-var}}
% \inferrule{ }{
%   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{x}{x}{\tau}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidE-U-asc}}
% \inferrule{
%   \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
%   \cvalidE{\Delta}{\Gamma}{\escenev}{\ce}{e}{\tau}
% }{
%   \cvalidE{\Delta}{\Gamma}{\escenev}{\aceasc{\ctau}{\ce}}{e}{\tau}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidE-U-letsyn}}
%   \inferrule{
%     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_1}{e_1}{\tau_1}\\
%     \cvalidE{\Delta}{\Gamma, x : \tau_1}{\ce_2}{e_2}{\tau_2}
%   }{
%     \cvalidE{\Delta}{\Gamma}{\escenev}{\aceletsyn{x}{\ce_1}{\ce_2}}{
%       \aeap{\aelam{\tau_1}{x}{e_2}}{e_1}
%     }{\tau_2}
%   }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidE-U-lam}}
% \inferrule{
%   \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
%   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{\ce}{e}{\tau'}
% }{
%   \cvalidE{\Delta}{\Gamma}{\escenev}{\acelam{\ctau}{x}{\ce}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidE-U-ap}}
%   \inferrule{
%     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_1}{e_1}{\aparr{\tau}{\tau'}}\\
%     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_2}{e_2}{\tau}
%   }{
%     \cvalidE{\Delta}{\Gamma}{\escenev}{\aceap{\ce_1}{\ce_2}}{\aeap{e_1}{e_2}}{\tau'}
%   }
% \end{equation*}
% Once again, the rules for common forms mirror the typing rules, i.e. Rules (\ref{rules:hastypeU}). The expression splicing scene, $\escenev$, passes opaquely through these rules.


% Notice that in Rule (\ref{rule:cvalidE-U-var}), only variables tracked by the proto-expansion typing context, $\Gamma$, are validated. Variables  in the application site unexpanded typing context, which appears within the expression splicing scene $\escenev$, are not validated. This achieves \emph{context independence} as described in Sec. \ref{sec:uetsms-validation} -- seTLMs cannot impose ``hidden constraints'' on the application site unexpanded typing context, because the variable bindings at the application site are not directly available to proto-expansions. We will consider this formally in Sec. \ref{sec:SE-metatheory} below.

% \paragraph{References to Spliced Unexpanded Expressions} The only proto-expression form that does not correspond to an expanded expression form is $\acesplicede{m}{n}{\ctau}$, which is a \emph{reference to a spliced unexpanded expression}, i.e. it indicates that an unexpanded expression should be parsed out from the literal body beginning at position $m$ and ending at position $n$. Rule (\ref{rule:cvalidE-U-splicede}) governs this form:
% \begin{equation*}\tag{\ref{rule:cvalidE-U-splicede}}
% \inferrule{
%   \cvalidT{\emptyset}{\tsfrom{\escenev}}{\ctau}{\tau}\\
%   \escenev=\esceneU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{b}\\
%   \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\
%   \expandsU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{\ue}{e}{\tau}\\\\
%   \Delta \cap \Delta_\text{app} = \emptyset\\
%   \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset
% }{
%   \cvalidE{\Delta}{\Gamma}{\escenev}{\acesplicede{m}{n}{\ctau}}{e}{\tau}
% }
% \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-splicede}
% % \inferrule{
% %   \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\\\
% %   \expandsU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{\ue}{e}{\tau}\\
% %   \Delta \cap \Delta_\text{app} = \emptyset\\
% %   \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset
% % }{
% %   \cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\splicede{m}{n}}{e}{\tau}
% % }
% % \end{equation*}

% The first premise of this rule validates and expands the type annotation. This type must be context independent.

% The second premise of this rule serves simply to reveal the components of the expression splicing scene.

% The third premise of this rule extracts the indicated subsequence of $b$ using the partial metafunction $\bsubseq{b}{m}{n}$ and parses it using the partial metafunction $\mathsf{parseUExp}(b)$, characterized in Sec. \ref{sec:syntax-U}, to produce the referenced spliced unexpanded expression, $\ue$.

% The fourth premise of Rule (\ref{rule:cvalidE-U-splicede}) performs typed expansion of $\ue$ assuming the application site contexts that appear in the expression splicing scene. Notice that the hypotheses in $\Delta$ and $\Gamma$ are not made available to $\ue$. 

% The fifth premise of Rule (\ref{rule:cvalidE-U-splicede}) imposes the constraint that the proto-expansion's type formation context, $\Delta$, be disjoint from the application site type formation context, $\Delta_\text{app}$. Similarly, the sixth premise requires that the proto-expansion's typing context, $\Gamma$, be disjoint from the application site typing context, $\Gamma_\text{app}$. These two premises can always be discharged by $\alpha$-varying the proto-expression that the reference to the spliced unexpanded expression appears within. 
% Together, these premises enforce the prohibition on capture as described in Sec. \ref{sec:uetsms-validation} -- the TLM provider can choose variable names freely within a proto-expansion, because the language prevents them from shadowing those at the application site.
% %\end{subequations}
% % \begin{subequations}\label{rules:cvalidE-U}
% % \begin{equation*}\label{rule:cvalidE-U-var}
% % \inferrule{ }{
% %   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{x}{x}{\tau}
% % }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-lam}
% % \inferrule{
% %   \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
% %   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{\ce}{e}{\tau'}
% % }{
% %   \cvalidE{\Delta}{\Gamma}{\escenev}{\acelam{\ctau}{x}{\ce}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}
% % }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-ap}
% %   \inferrule{
% %     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_1}{e_1}{\aparr{\tau}{\tau'}}\\
% %     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_2}{e_2}{\tau}
% %   }{
% %     \cvalidE{\Delta}{\Gamma}{\escenev}{\aceap{\ce_1}{\ce_2}}{\aeap{e_1}{e_2}}{\tau'}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-tlam}
% %   \inferrule{
% %     \cvalidE{\Delta, \Dhyp{t}}{\Gamma}{\escenev}{\ce}{e}{\tau}
% %   }{
% %     \cvalidEX{\acetlam{t}{\ce}}{\aetlam{t}{e}}{\aall{t}{\tau}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-tap}
% %   \inferrule{
% %     \cvalidEX{\ce}{e}{\aall{t}{\tau}}\\
% %     \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau'}{\tau'}
% %   }{
% %     \cvalidEX{\acetap{\ce}{\ctau'}}{\aetap{e}{\tau'}}{[\tau'/t]\tau}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-fold}
% %   \inferrule{
% %     \cvalidT{\Delta, \Dhyp{t}}{\escenev}{\ctau}{\tau}\\
% %     \cvalidEX{\ce}{e}{[\arec{t}{\tau}/t]\tau}
% %   }{
% %     \cvalidEX{\acefold{t}{\ctau}{\ce}}{\aefold{e}}{\arec{t}{\tau}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-unfold}
% %   \inferrule{
% %     \cvalidEX{\ce}{e}{\arec{t}{\tau}}
% %   }{
% %     \cvalidEX{\aceunfold{\ce}}{\aeunfold{e}}{[\arec{t}{\tau}/t]\tau}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-tpl}
% %   \inferrule{
% %     \{\cvalidEX{\ce_i}{e_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \cvalidEX{\acetpl{\labelset}{\mapschema{\ce}{i}{\labelset}}}{\aetpl{\labelset}{\mapschema{e}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-pr}
% %   \inferrule{
% %     \cvalidEX{\ce}{e}{\aprod{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}}
% %   }{
% %     \cvalidEX{\acepr{\ell}{\ce}}{\aepr{\ell}{e}}{\tau}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-in}
% %   \inferrule{
% %     \{\cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau_i}{\tau_i}\}_{i \in \labelset}\\
% %     \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
% %     \cvalidEX{\ce}{e}{\tau}
% %   }{
% %     \left\{\shortstack{$\Delta~\Gamma \vdash_\uPsi \acein{\labelset, \ell}{\ell}{\mapschema{\ctau}{i}{\labelset}; \mapitem{\ell}{\ctau}}{\ce}$\\$\leadsto$\\$\aein{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}{e} : \asum{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}$\vspace{-1.2em}}\right\}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-case}
% %   \inferrule{
% %     \cvalidEX{\ce}{e}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}\\
% %     \{\cvalidE{\Delta}{\Gamma, \Ghyp{x_i}{\tau_i}}{\escenev}{\ue_i}{e_i}{\tau}\}_{i \in \labelset}
% %   }{
% %     \cvalidEX{\acecase{\labelset}{\ce}{\mapschemab{x}{\ce}{i}{\labelset}}}{\aecase{\labelset}{e}{\mapschemab{x}{e}{i}{\labelset}}}{\tau}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-splicede}
% % \inferrule{
% %   \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\\\
% %   \Delta \cap \Delta_\text{app} = \emptyset\\
% %   \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset\\
% %   \expandsU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{\ue}{e}{\tau}
% % }{
% %   \cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\acesplicede{m}{n}}{e}{\tau}
% % }
% % \end{equation*}
% % \end{subequations}

% % Each form of expanded expression, $e$, corresponds to a form of proto-expression, $\ce$ (compare Figure \ref{fig:U-expanded-terms} and Figure \ref{fig:U-candidate-terms}). For each typing rule in Rules \ref{rules:hastypeU}, there is a corresponding proto-expression validation rule -- Rules (\ref{rule:cvalidE-U-var}) to (\ref{rule:cvalidE-U-case}) -- where the proto-expression and expanded expression correspond. The premises also correspond.


% %Candidate expansions cannot themselves define or apply TLMs. This simplifies our metatheory, though it can be inconvenient at times for TLM providers. We discuss adding the ability to use TLMs within proto-expansions in Sec. \ref{sec:tsms-in-expansions}.


% \subsection{Metatheory}\label{sec:SE-metatheory}
% \subsubsection{Typed Expansion}
% Let us now consider Theorem \ref{thm:typed-expansion-short-U}, which was mentioned at the beginnning of Sec. \ref{sec:typed-expansion-U} and is reproduced below:
% \begingroup
% \def\thetheorem{\ref{thm:typed-expansion-short-U}}
% \begin{theorem}[Typed Expression Expansion] \hspace{-3px}If $\expandsU{\uDD{\uD}{\Delta}\hspace{-3px}}{\uGG{\uG}{\Gamma}\hspace{-3px}}{\uPsi}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
% \end{theorem}
% \endgroup

%  To prove this theorem, we must  prove the following stronger theorem, because the proto-expression validation judgement is defined mutually inductively with the typed expansion judgement:

% \begingroup
% \def\thetheorem{\ref{thm:typed-expansion-full-U}}
% \begin{theorem}[Typed Expansion (Full)] ~
% \begin{enumerate}
% \item If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uAS{\uA}{\Psi}}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
% \item If $\cvalidE{\Delta}{\Gamma}{\esceneU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uAS{\uA}{\Psi}}{b}}{\ce}{e}{\tau}$ and $\Delta \cap \Delta_\text{app} = \emptyset$ and $\domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset$ then $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$.
% \end{enumerate}
% \end{theorem}
% \endgroup
% \begin{proof}
% By mutual rule induction over Rules (\ref{rules:expandsU}) and Rules (\ref{rules:cvalidE-U}). The full proof is given in Appendix \ref{appendix:SES-typed-expression-expansion-metatheory}. We will reproduce the interesting cases below. 

% The proof of part 1 proceeds by inducting over the typed expansion assumption. The only interesting cases are those related to seTLM definition and application, reproduced below. In the following cases, let $\uDelta=\uDD{\uD}{\Delta}$ and $\uGamma=\uGG{\uG}{\Gamma}$ and $\uPsi=\uAS{\uA}{\Psi}$.

% \begin{byCases}
% \item[\text{(\ref{rule:expandsU-syntax})}] We have 
% \begin{pfsteps}
%   \item \ue=\uesyntax{\tsmv}{\utau'}{\eparse}{\ue'} \BY{assumption}
%   \item \expandsTU{\uDelta}{\utau'}{\tau'} \BY{assumption} \pflabel{expandsTU}
%  \item \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultExp}} \BY{assumption}\pflabel{eparse}
%   \item \expandsU{\uDelta}{\uGamma}{\uPsi, \uShyp{\tsmv}{a}{\tau'}{\eparse}}{\ue'}{e}{\tau} \BY{assumption}\pflabel{expandsU}
% %  \item \uetsmenv{\Delta}{\Psi} \BY{assumption}\pflabel{uetsmenv1}
%  \item \istypeU{\Delta}{\tau'} \BY{Lemma \ref{lemma:type-expansion-U} to \pfref{expandsTU}} \pflabel{istype}
% %  \item \uetsmenv{\Delta}{\Psi, \xuetsmbnd{\tsmv}{\tau'}{\eparse}} \BY{Definition \ref{def:seTLM-def-ctx-formation} on \pfref{uetsmenv1}, \pfref{istype} and \pfref{eparse}}\pflabel{uetsmenv3}
%   \item \hastypeU{\Delta}{\Gamma}{e}{\tau} \BY{IH, part 1(a) on \pfref{expandsU}}
% \end{pfsteps}
% \resetpfcounter 

% \item[\text{(\ref{rule:expandsU-tsmap})}] We have 
% \begin{pfsteps}
%   \item \ue=\utsmap{\tsmv}{b} \BY{assumption}
%   \item \uA = \uA', \vExpands{\tsmv}{a} \BY{assumption}
%   \item \Psi=\Psi', \xuetsmbnd{a}{\tau}{\eparse} \BY{assumption}
%   \item \encodeBody{b}{\ebody} \BY{assumption}
%   \item \evalU{\eparse(\ebody)}{\aein{\lbltxt{SuccessE}}{\ecand}} \BY{assumption}
%   \item \decodeCondE{\ecand}{\ce} \BY{assumption}
%   \item \cvalidE{\emptyset}{\emptyset}{\esceneU{\uDelta}{\uGamma}{\uPsi}{b}}{\ce}{e}{\tau} \BY{assumption}\pflabel{cvalidE}
% %  \item \uetsmenv{\Delta}{\Psi} \BY{assumption} \pflabel{uetsmenv}
%   \item \emptyset \cap \Delta = \emptyset \BY{finite set intersection} \pflabel{delta-cap}
%   \item {\emptyset} \cap \domof{\Gamma} = \emptyset \BY{finite set intersection} \pflabel{gamma-cap}
%   \item \hastypeU{\emptyset \cup \Delta}{\emptyset \cup \Gamma}{e}{\tau} \BY{IH, part 2 on \pfref{cvalidE}, \pfref{delta-cap}, and \pfref{gamma-cap}} \pflabel{penultimate}
%   \item \hastypeU{\Delta}{\Gamma}{e}{\tau} \BY{finite set and finite function identity over \pfref{penultimate}}
% \end{pfsteps}
% \resetpfcounter
% \end{byCases}

% The proof of part 2 proceeds by induction over the proto-expression validation assumption. The only interesting case governs references to spliced expressions. In the following cases, let $\uDelta_\text{app}=\uDD{\uD}{\Delta_\text{app}}$ and $\uGamma_\text{app}=\uGG{\uG}{\Gamma_\text{app}}$ and $\uPsi = \uAS{\uA}{\Psi}$.
% \begin{byCases}
% % \item[\text{(\ref{rule:cvalidE-U-var})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=x$ \BY{assumption}
% %   \item $e=x$ \BY{assumption}
% %   \item $\Gamma=\Gamma', \Ghyp{x}{\tau}$ \BY{assumption}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gamma', \Ghyp{x}{\tau}}{x}{\tau}$ \BY{Rule (\ref{rule:hastypeU-var})} \pflabel{hastypeU}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma', \Ghyp{x}{\tau}}{\Gamma_\text{app}}}{x}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} over $\Gamma_\text{app}$ to \pfref{hastypeU}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\text{(\ref{rule:cvalidE-U-lam})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=\acelam{\ctau_1}{x}{\ce'}$ \BY{assumption}
% %   \item $e=\aelam{\tau_1}{x}{e'}$ \BY{assumption}
% %   \item $\tau=\aparr{\tau_1}{\tau_2}$ \BY{assumption}
% %   \item $\cvalidT{\Delta}{\tsceneU{\uDelta_\text{app}}{b}}{\ctau_1}{\tau_1}$ \BY{assumption} \pflabel{cvalidT}
% %   \item $\cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau_1}}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce'}{e'}{\tau_2}$ \BY{assumption} \pflabel{cvalidE}
% % %  \item $\uetsmenv{\Delta_\text{app}}{\Psi}$ \BY{assumption} \pflabel{uetsmenv}
% %   \item $\Delta \cap \Delta_\text{app}=\emptyset$ \BY{assumption} \pflabel{delta-disjoint}
% %   \item $\domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{assumption} \pflabel{gamma-disjoint}
% %   \item $x \notin \domof{\Gamma_\text{app}}$ \BY{identification convention} \pflabel{x-fresh}
% %   \item $\domof{\Gamma, x : \tau_1} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{\pfref{gamma-disjoint} and \pfref{x-fresh}} \pflabel{gamma-disjoint2}
% %   \item $\istypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\tau_1}$ \BY{Lemma \ref{lemma:candidate-expansion-type-validation} on \pfref{cvalidT} and \pfref{delta-disjoint}} \pflabel{istype}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma, \Ghyp{x}{\tau_1}}{\Gamma_\text{app}}}{e'}{\tau_2}$ \BY{IH, part 2 on \pfref{cvalidE}, \pfref{delta-disjoint} and \pfref{gamma-disjoint2}} \pflabel{hastype1}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}, \Ghyp{x}{\tau_1}}{e'}{\tau_2}$ \BY{exchange over $\Gamma_\text{app}$ on \pfref{hastype1}} \pflabel{hastype2}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aelam{\tau_1}{x}{e'}}{\aparr{\tau_1}{\tau_2}}$ \BY{Rule (\ref{rule:hastypeU-lam}) on \pfref{istype} and \pfref{hastype2}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\text{(\ref{rule:cvalidE-U-ap})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=\aceap{\ce_1}{\ce_2}$ \BY{assumption}
% %   \item $e=\aeap{e_1}{e_2}$ \BY{assumption}
% %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce_1}{e_1}{\aparr{\tau_2}{\tau}}$ \BY{assumption} \pflabel{cvalidE1}
% %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce_2}{e_2}{\tau_2}$ \BY{assumption} \pflabel{cvalidE2}
% % %  \item $\uetsmenv{\Delta_\text{app}}{\Psi}$ \BY{assumption} \pflabel{uetsmenv}
% %   \item $\Delta \cap \Delta_\text{app}=\emptyset$ \BY{assumption} \pflabel{delta-disjoint}
% %   \item $\domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{assumption} \pflabel{gamma-disjoint}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_1}{\aparr{\tau_2}{\tau}}$ \BY{IH, part 2 on \pfref{cvalidE1}, \pfref{delta-disjoint} and \pfref{gamma-disjoint}} \pflabel{hastypeU1}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_2}{\tau_2}$ \BY{IH, part 2 on \pfref{cvalidE2}, \pfref{delta-disjoint} and \pfref{gamma-disjoint}} \pflabel{hastypeU2}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aeap{e_1}{e_2}}{\tau}$ \BY{Rule (\ref{rule:hastypeU-ap}) on \pfref{hastypeU1} and \pfref{hastypeU2}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\text{(\ref{rule:cvalidE-U-tlam})}] ~
% % \begin{pfsteps}
% %   \item \ce=\acetlam{t}{\ce'} \BY{assumption}
% %   \item e = \aetlam{t}{e'} \BY{assumption}
% %   \item \tau = \aall{t}{\tau'}\BY{assumption}
% %   \item \cvalidE{\Delta, \Dhyp{t}}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce'}{e'}{\tau'} \BY{assumption} \pflabel{cvalidE}
% % %  \item \uetsmenv{\Delta_\text{app}}{\Psi} \BY{assumption} \pflabel{uetsmenv}
% %   \item \Delta \cap \Delta_\text{app}=\emptyset \BY{assumption} \pflabel{delta-disjoint}
% %   \item \domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset \BY{assumption} \pflabel{gamma-disjoint}
% %   \item \Dhyp{t} \notin \Delta_\text{app} \BY{identification convention}\pflabel{t-fresh}
% %   \item \Delta, \Dhyp{t} \cap \Delta_\text{app} = \emptyset \BY{\pfref{delta-disjoint} and \pfref{t-fresh}}\pflabel{delta-disjoint2}
% %   \item \hastypeU{\Dcons{\Delta, \Dhyp{t}}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e'}{\tau'} \BY{IH, part 2 on \pfref{cvalidE}, \pfref{delta-disjoint2} and \pfref{gamma-disjoint}}\pflabel{hastype1}
% %   \item \hastypeU{\Dcons{\Delta}{\Delta_\text{app}, \Dhyp{t}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e'}{\tau'} \BY{exchange over $\Delta_\text{app}$ on \pfref{hastype1}}\pflabel{hastype2}
% %   \item \hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aetlam{t}{e'}}{\aall{t}{\tau'}} \BY{Rule (\ref{rule:hastypeU-tlam}) on \pfref{hastype2}}
% % \end{pfsteps}
% % \resetpfcounter

% % \item[{\text{(\ref{rule:cvalidE-U-tap})}}~\textbf{through}~{\text{(\ref{rule:cvalidE-U-case})}}] These cases follow analagously, i.e. we apply the IH, part 2 to all proto-expression validation judgements, Lemma \ref{lemma:candidate-expansion-type-validation} to all proto-type validation judgements, the identification convention to ensure that extended contexts remain disjoint, weakening and exchange as needed, and the corresponding typing rule in Rules (\ref{rule:hastypeU-tap}) through (\ref{rule:hastypeU-case}).
% % \\

% \item[\text{(\ref{rule:cvalidE-U-splicede})}] ~
% \begin{pfsteps*}
%   \item $\ce=\acesplicede{m}{n}{\ctau}$ \BY{assumption}
%   \item $  \escenev=\esceneU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{b}$ \BY{assumption}
%   \item   $\cvalidT{\emptyset}{\tsfrom{\escenev}}{\ctau}{\tau}$ \BY{assumption}
%   \item $\parseUExp{\bsubseq{b}{m}{n}}{\ue}$ \BY{assumption}
%   \item $\expandsU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{\ue}{e}{\tau}$ \BY{assumption} \pflabel{expands}
% %  \item $\uetsmenv{\Delta_\text{app}}{\Psi}$ \BY{assumption} \pflabel{uetsmenv}
%   \item $\Delta \cap \Delta_\text{app}=\emptyset$ \BY{assumption} \pflabel{delta-disjoint}
%   \item $\domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{assumption} \pflabel{gamma-disjoint}
%   \item $\hastypeU{\Delta_\text{app}}{\Gamma_\text{app}}{e}{\tau}$ \BY{IH, part 1 on \pfref{expands}} \pflabel{hastype}
%   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} over $\Delta$ and $\Gamma$ and exchange on \pfref{hastype}}
% \end{pfsteps*}
% \resetpfcounter
% \end{byCases}

% The mutual induction can be shown to be well-founded by showing that the following numeric metric on the judgements that we induct over is decreasing:
% \begin{align*}
% \sizeof{\expandsU{\uDelta}{\uGamma}{\uPsi}{\ue}{e}{\tau}} & = \sizeof{\ue}\\
% \sizeof{\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce}{e}{\tau}} & = \sizeof{b}
% \end{align*}
% where $\sizeof{b}$ is the length of $b$ and $\sizeof{\ue}$ is the sum of the lengths of the literal bodies in $\ue$ (see Appendix \ref{appendix:SES-body-lengths}.)

% The only case in the proof of part 1 that invokes part 2 is Case (\ref{rule:expandsU-tsmap}). There, we have that the metric remains stable: \begin{align*}
%  & \sizeof{\expandsU{\uDelta}{\uGamma}{\uPsi}{\utsmap{\tsmv}{b}}{e}{\tau}}\\
% =& \sizeof{\cvalidE{\emptyset}{\emptyset}{\esceneU{\uDelta}{\uGamma}{\uPsi}{b}}{\ce}{e}{\tau}}\\
% =&\sizeof{b}\end{align*}

% The only case in the proof of part 2 that invokes part 1 is Case (\ref{rule:cvalidE-U-splicede}). There, we have that $\parseUExp{\bsubseq{b}{m}{n}}{\ue}$ and the IH is applied to the judgement $\expandsU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{\ue}{e}{\tau}$ where $\uDelta_\text{app}=\uDD{\uD}{\Delta_\text{app}}$ and $\uGamma_\text{app}=\uGG{\uG}{\Gamma_\text{app}}$ and $\uPsi=\uAS{\uA}{\Psi}$. Because the metric is stable when passing from part 1 to part 2, we must have that it is strictly decreasing in the other direction:
% \[\sizeof{\expandsU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{\ue}{e}{\tau}} < \sizeof{\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\acesplicede{m}{n}{\ctau}}{e}{\tau}}\]
% i.e. by the definitions above, 
% \[\sizeof{\ue} < \sizeof{b}\]

% This is established by appeal to the following two conditions. The first condition states that an unexpanded expression constructed by parsing a textual sequence $b$ is strictly smaller, as measured by the metric defined above, than the length of $b$, because some characters must necessarily be used to invoke a TLM and delimit each literal body.
% \begingroup
% \def\thetheorem{\ref{condition:body-parsing}}
% \begin{condition}[Expression Parsing Monotonicity] If $\parseUExp{b}{\ue}$ then $\sizeof{\ue} < \sizeof{b}$.\end{condition}
% \endgroup
% The second condition simply states that subsequences of $b$ are no longer than $b$.
% \begingroup
% \def\thetheorem{\ref{condition:body-subsequences}}
% \begin{condition}[Body Subsequencing] If $\bsubseq{b}{m}{n}=b'$ then $\sizeof{b'} \leq \sizeof{b}$. \end{condition}
% \endgroup

% Combining these two conditions, we have that $\sizeof{\ue} < \sizeof{b}$ as needed.
% \end{proof}

% % We need to define the following theorem about proto-expression validation mutually with Theorem \ref{thm:typed-expansion-U}. 
% % \begin{theorem}[Proto-Expansion Expression Validation]\label{thm:candidate-expansion-validation-U}
% % If $\cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce}{e}{\tau}$ and $\uetsmenv{\Delta_\text{app}}{\uPsi}$ then $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$.
% % \end{theorem}
% % \begin{proof} By rule induction over Rules (\ref{rules:cvalidE-U}).
% % \begin{byCases}
% % \item[\text{(\ref{rule:cvalidE-U-var})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=x$ \BY{assumption}
% %   \item $e=x$ \BY{assumption}
% %   \item $\Gamma=\Gamma', \Ghyp{x}{\tau}$ \BY{assumption}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gamma', \Ghyp{x}{\tau}}{x}{\tau}$ \BY{Rule (\ref{rule:hastypeU-var})} \pflabel{hastypeU}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma', \Ghyp{x}{\tau}}{\Gamma_\text{app}}}{x}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} over $\Gamma_\text{app}$ to \pfref{hastypeU}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\text{(\ref{rule:cvalidE-U-lam})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=\acelam{\ctau_1}{x}{\ce'}$ \BY{assumption}
% %   \item $e=\aelam{\tau_1}{x}{e'}$ \BY{assumption}
% %   \item $\tau=\aparr{\tau_1}{\tau_2}$ \BY{assumption}
% %   \item $\cvalidT{\Delta}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ctau_1}{\tau_1}$ \BY{assumption} \pflabel{cvalidT}
% %   \item $\cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau_1}}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce'}{e'}{\tau_2}$ \BY{assumption} \pflabel{cvalidE}
% %   \item $\uetsmenv{\Delta_\text{app}}{\uPsi}$ \BY{assumption} \pflabel{uetsmenv}
% %   \item $\istypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\tau_1}$ \BY{Lemma \ref{lemma:candidate-expansion-type-validation} on \pfref{cvalidT}} \pflabel{istype}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma, \Ghyp{x}{\tau_1}}{\Gamma_\text{app}}}{e'}{\tau_2}$ \BY{IH on \pfref{cvalidE} and \pfref{uetsmenv}} \pflabel{hastype1}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}, \Ghyp{x}{\tau_1}}{e'}{\tau_2}$ \BY{exchange over $\Gamma_\text{app}$ on \pfref{hastype1}} \pflabel{hastype2}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aelam{\tau_1}{x}{e'}}{\aparr{\tau_1}{\tau_2}}$ \BY{Rule (\ref{rule:hastypeU-lam}) on \pfref{istype} and \pfref{hastype2}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\text{(\ref{rule:cvalidE-U-ap})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=\aceap{\ce_1}{\ce_2}$ \BY{assumption}
% %   \item $e=\aeap{e_1}{e_2}$ \BY{assumption}
% %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce_1}{e_1}{\aparr{\tau_1}{\tau}}$ \BY{assumption} \pflabel{cvalidE1}
% %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce_2}{e_2}{\tau_1}$ \BY{assumption} \pflabel{cvalidE2}
% %   \item $\uetsmenv{\Delta_\text{app}}{\uPsi}$ \BY{assumption} \pflabel{uetsmenv}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_1}{\aparr{\tau_1}{\tau}}$ \BY{IH on \pfref{cvalidE1} and \pfref{uetsmenv}} \pflabel{hastypeU1}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_2}{\tau_1}$ \BY{IH on \pfref{cvalidE2} and \pfref{uetsmenv}} \pflabel{hastypeU2}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aeap{e_1}{e_2}}{\tau}$ \BY{Rule (\ref{rule:hastypeU-ap}) on \pfref{hastypeU1} and \pfref{hastypeU2}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\VExpof{\text{\ref{rule:hastypeU-tlam}}}~\text{through}~\VExpof{\text{\ref{rule:hastypeU-case}}}] These cases follow analagously, i.e. we apply the IH to all proto-expression validation premises, Lemma \ref{lemma:candidate-expansion-type-validation} to all proto-types validation premises, weakening and exchange as needed, and then apply the corresponding typing rule.
% % \\

% % \item[\text{(\ref{rule:cvalidE-U-splicede})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=\acesplicede{m}{n}$ \BY{assumption}
% %   \item $\parseUExp{\bsubseq{b}{m}{n}}{\ue}$ \BY{assumption}
% %   \item $\expandsU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{\ue}{e}{\tau}$ \BY{assumption} \pflabel{expands}
% %   \item $\uetsmenv{\Delta_\text{app}}{\uPsi}$ \BY{assumption} \pflabel{uetsmenv}
% %   \item $\hastypeU{\Delta_\text{app}}{\Gamma_\text{app}}{e}{\tau}$ \BY{Theorem \ref{thm:typed-expansion-U} on \pfref{expands} and \pfref{uetsmenv}} \pflabel{hastype}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} on \pfref{hastype}}
% % \end{pfsteps*}
% % \resetpfcounter
% % \end{byCases}
% % \end{proof}


% %\qed


% \subsubsection{Abstract Reasoning Principles}\label{sec:uetsms-reasoning-principles}
% The following theorem summarizes the abstract reasoning principles that programmers can rely on when applying an seTLM. In particular, the programmer can be sure that:
% \begin{enumerate} 
% \item \textbf{Segmentation}: The segmentation determined by the proto-expansion actually segments the literal body (i.e. each segment is in-bounds and the segments are non-overlapping.)
% \item \textbf{Typing 1}: The type of the expansion is consistent with the type annotation on the seTLM definition.
% \item \textbf{Typing 2}: Each spliced type has a well-formed expansion at the application site.
% \item \textbf{Typing 3}: Each type annotation on a reference to a spliced expression has a well-formed expansion at the application site.
% \item \textbf{Typing 4}: Each spliced expression has a well-typed expansion consistent with its type annotation.
% \item \textbf{Capture Avoidance}: The final expansion can be decomposed into a  term with variables in place of each spliced type or expression. The expansions of these spliced types and expressions can be substituted into this term in the standard capture avoiding manner.
% \item \textbf{Context Independence}: The decomposed term is indeed well-typed independent of the application site contexts.

% \end{enumerate}

% \begingroup
% \def\thetheorem{\ref{thm:tsc-SES}}
% \begin{theorem}[seTLM Abstract Reasoning Principles]
% If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uPsi}{\utsmap{\tsmv}{b}}{e}{\tau}$ then:
% \begin{enumerate}
% \item (\textbf{Typing 1}) $\uPsi = \uPsi', \uShyp{\tsmv}{a}{\tau}{\eparse}$ and $\hastypeU{\Delta}{\Gamma}{e}{\tau}$
% \item $\encodeBody{b}{\ebody}$
% \item $\evalU{\ap{\eparse}{\ebody}}{\aein{\lbltxt{SuccessE}}{\ecand}}$
% \item $\decodeCondE{\ecand}{\ce}$
% \item (\textbf{Segmentation}) $\segOK{\segof{\ce}}{b}$
% \item $\segof{\ce} = \sseq{\acesplicedt{m'_i}{n'_i}}{\nty} \cup \sseq{\acesplicede{m_i}{n_i}{\ctau_i}}{\nexp}$
% \item \textbf{(Typing 2)} $\sseq{
%       \expandsTU{\uDD{\uD}{\Delta}}
%       {
%         \parseUTypF{\bsubseq{b}{m'_i}{n'_i}}
%       }{\tau'_i}
%     }{\nty}$ and $\sseq{\istypeU{\Delta}{\tau'_i}}{\nty}$
% \item \textbf{(Typing 3)} $\sseq{
%   \cvalidT{\emptyset}{
%     \tsceneUP
%       {\uDD
%         {\uD}{\Delta}
%       }{b}
%   }{
%     \ctau_i
%   }{\tau_i}
% }{\nexp}$ and $\sseq{\istypeU{\Delta}{\tau_i}}{\nexp}$
% \item \textbf{(Typing 4)} $\sseq{
%   \expandsU
%     {\uDD{\uD}{\Delta}}
%     {\uGG{\uG}{\Gamma}}
%     {\uPsi}
%     {\parseUExpF{\bsubseq{b}{m_i}{n_i}}}
%     {e_i}
%     {\tau_i}
% }{\nexp}$ and $\sseq{\hastypeU{\Delta}{\Gamma}{e_i}{\tau_i}}{\nexp}$
% \item (\textbf{Capture Avoidance}) $e = [\sseq{\tau'_i/t_i}{\nty}, \sseq{e_i/x_i}{\nexp}]e'$ for some $\sseq{t_i}{\nty}$ and $\sseq{x_i}{\nexp}$ and $e'$
% \item (\textbf{Context Independence}) $\mathsf{fv}(e') \subset \sseq{t_i}{\nty} \cup \sseq{x_i}{\nexp}$
%   % $\hastypeU
%   % {\sseq{\Dhyp{t_i}}{\nty}}
%   % {\sseq{x_i : \tau_i}{\nexp}}
%   % {e'}{\tau}$
% \end{enumerate}
% \end{theorem}
% \begin{proof} The proof, which involves auxiliary lemmas about the decomposition of proto-types and proto-expressions, is given in Appendix \ref{appendix:SES-reasoning-principles}.
% \end{proof}
% \endgroup


\newcommand{\rwSec}{Related Work}
\section{\protect\rwSec}
\label{sec:existing-approaches}


Unhygienic term rewriting systems like OCaml's PPX system and Template Haskell can be used to rewrite string literals, but these systems also do no adequately address any of these reasoning criteria. Hygienic macro systems, like those available in various Lisp-family languages and in Scala, are also unsuitable because the hygiene constraints prevent the macros from surfacing spliced expressions from literal bodies. The system of \emph{type-specific languages} (TSLs) developed in prior work by \citet{TSLs} has made perhaps the most headway toward these ideals, but it too does not adequately maintain these reasoning criteria. Moreover, it is not clear how to integrate this approach, which was developed for a simple monomorphic language with nominal types and local type inference, into ML, which supports structural types, parameterized types, abstract types, modules, pattern matching and non-local type inference. We will say more about these existing approaches in Sec. \ref{sec:existing-approaches}. For now, it suffices to say that after evaluating existing approaches, we found that none of them were suitable for integration into Reason.

An approach that has made some headway toward this ideal is Omar et al.'s \emph{type-specific languages} (TSLs) \cite{TSLs}. A language that supports TSLs delegates control over the parsing and expansion of certain \emph{generalized literal forms} to a parser associated with the type that the form is being checked against. So if we add TSLs to our language, our example then requires a type annotation on \li{y}:
\begin{lstlisting}[numbers=none,basicstyle=\ttfamily\fontsize{9pt}{1em}\selectfont]
  let y : KQuery.t = `((!R)@&{&/x!/:2_!x}'!R)`
\end{lstlisting}
The parser associated with the type \li{KQuery.t} is statically invoked to lex, parse and expand the literal body, i.e. the sequence of characters between the outer delimiters, here \li{`(} and \li{)`}. The literal body is constrained by the context-free grammar of the language only in that nested delimiters must be matched (like comments in OCaml). TSLs are closely related to the mechanism that we will propose, so let us analyze TSLs from the perspective of the six reasoning criteria just outlined.
\begin{enumerate}[leftmargin=12px]
  \item[\LEFTcircle] \textbf{Responsibility}: The type-directed dispatch mechanism allows the client to easily determine which parser is responsible for each literal form: the parser that was associated with the type when it was defined. Clients do not need to worry about different TSLs conflicting syntactically because the context-free grammar of the language remains fixed and composition is mediated by splicing. The main limitation here is that it is impossible to define literal notation after a type has been defined, and it is also impossible to define multiple notations at a single type.
  \item[\LEFTcircle] \textbf{Segmentation}: The parser can splice expressions out of the literal body, but there is no clear way to associate each spliced term with some particular segment (i.e. subsequence) of the literal body, nor is there a guarantee that spliced terms are non-overlapping. As such, there is no way to indicate to the programmer where base language expressions are located within a literal form.
  \item[\LEFTcircle] \textbf{Capture}: The TSL mechanism as described enforces complete capture avoidance (though the formal specification given in the paper did not adequately enforce this constraint).
  \item[\LEFTcircle] \textbf{Context Dependence}: The TSL mechanism as described requires that the expansion be completely closed, so it is trivially context independent. However, this comes at a significant expressive cost: the generated expansions cannot make use of any libraries whatsoever.
  \item[\LEFTcircle] \textbf{Typing}: The expansion must necessarily be of the associated type, so abstract reasoning about the type of the expansion is straightforward. However, there is no easy way to determine the type expected for each spliced expression. In addition, the prior work considered only a monomorphic, nominally-typed language with local type inference, leaving open a number of problems that came up as we considered integrating TSLs into Reason/OCaml:
\begin{enumerate}[leftmargin=15px,nolistsep,noitemsep]
  \item \textbf{Structural Types}: There is no way to define literal notation at structural types, e.g. tuple and arrow types, because there is no ``definition site'' for such types.
  \item \textbf{Parameterized \& Abstract Types}: There is no way to define literal notation over a type-parameterized family of types, e.g. at all types \li{list('a)}. Similarly, there is no way to define literal notation over a module-parameterized family of abstract types, e.g. at every abstract type defined by a module implementing the \li{QUEUE} signature. Parameterized and abstract type families are ubiquitous in ML-family languages.% Programmers in ML-family languages commonly define various implementations of abstract data types in this way.
  \item \textbf{Pattern Literals}: Pattern matching is ubiquitous in ML-family languages but the prior work on TSLs considered only expression literals. %Pattern literals are dual to expression literals in that expression literals support value construction, while pattern literals support value deconstruction.
  \item \textbf{ML-Style Type Inference}: It is not clear that a type-directed dispatch scheme can be cleanly  reconciled with ML-style type inference.
\end{enumerate}
\end{enumerate}

\subsection{Syntax Definition Systems}\label{sec:syntax-dialects-intro}
One approach available to library providers seeking to introduce new literal forms is to use a syntax definition system to construct a library-specific \emph{syntax dialect}: a new syntax definition that extends the syntax definition given in the language definition with new forms, including literal forms.% For example, Ur/Web's syntax (Figure 1) is a library-specific dialect of Ur's syntax \cite{conf/pldi/Chlipala10,conf/popl/Chlipala15}.

 % Such dialects are sometimes qualitatively taxonomized as amongst the ``domain-specific language'' for this reason \cite{fowler2010domain}. %Syntactic cost is often assessed qualitatively \cite{green1996usability}, though quantitative metrics can be defined. 
% Syntax definition systems , which we will discuss in Sec. \ref{sec:syntax-dialects}, have simplified the task of defining ``library-specific'' (a.k.a. ``domain-specific'') syntax dialects like Ur/Web, and have thereby contributed to their ongoing proliferation.


% Many have argued that a proliferation of syntax dialects constructed using these tools is harmless or even desirable, because programmers can simply choose the right syntax dialect for each job at hand \cite{journals/stp/Ward94}. However, we argue that in fact 
There are hundreds of syntax definition systems of various design. Notable examples include grammar-oriented systems like Camlp4 \cite{ocaml-manual}, Copper \cite{conf/gpce/WykS07,conf/pldi/SchwerdfegerW09} and Sugar*/SoundExt \cite{erdweg2011sugarj,erdweg2013framework,conf/icfp/LorenzenE13,conf/popl/LorenzenE16}, as well as parser combinator systems \cite{Hutton1992d}. The parsers generated by these systems can be invoked to preprocess program text in various ways, e.g. by invoking them from within a build script, by using a preprocessor-aware build system (e.g. \li{ocamlbuild}), or via language-integrated preprocessing directives, e.g. Racket's \li{#lang} directive or its reader macros \cite{Flatt:2012:CLR:2063176.2063195}, or the import mechanism of SugarJ \cite{erdweg2011sugarj}.% the directives of language-integrated ``mixfix'' systems \cite{wieland2009parsing,missura1997higher,5134} like Coq \cite{Coq:manual}.)
%library-specific (a.k.a. ``domain-specific'') 

The problem was described by example in Sec. \ref{sec:intro}: these systems make it difficult to reason abstractly. Let us reitierate more generally, before discussing a few systems that are exceptional along some strict subset of these dimensions:
\begin{enumerate}[noitemsep,nolistsep,leftmargin=10pt]
\item \textbf{Responsibility}: Clients using a combined syntax dialect cannot easily determine which constituent extension is responsible for a given form, whereas TLMs have explicit names which follow the usual scoping conventions. 

Moreover, there can be syntactic conflicts because multiple extensions can claim responsibility for the same form. TLMs sidestep these complexities entirely because the context-free syntax of the language is fixed, and composition is via splicing rather than direct combination.
\item \textbf{Segmentation}: Clients of a syntax dialect cannot accurately determine which segments of the program text appear directly in the desugaring. In contrast, TLM clients can inspect the inferred segmentation (communicated via secondary notation.)
\item \textbf{Capture}: Unlike TLM clients, clients of a syntax dialect cannot be sure that spliced terms are capture avoiding. 
\item \textbf{Context Dependence}: Similarly, clients cannot be sure that the desugaring is context independent. Indeed, without a method to pass in parameters (Sec. \ref{sec:ptsms}), achieving strict context independence would be impractical.
\item \textbf{Typing}: Clients of a syntax dialect cannot reason abstractly about what type a desugaring has. In contrast, TLM clients can determine the type of any expansion by referring to the parameter and type declarations on the TLM definition, and nothing else. The inferred segmentation also gives types for each spliced expression or pattern. %This information can be communicated upon request by the type inspection service of a program editor.
\end{enumerate}


% The first problem with using a syntax definition systems to directly extend the context-free syntax of a language is that clients cannot always deterministically combine the resulting extended syntax dialects when they want to use the new forms that they define together, i.e. there can be syntactic conflicts. This is a significant problem because large programs use many separately developed libraries \cite{DBLP:conf/sac/LammelPS11}.

An extensible syntax definition system that has confronted the problem of \textbf{Responsibility} (but not the other problems) is Copper \cite{conf/pldi/SchwerdfegerW09}. Copper integrates a modular grammar analysis that guarantees that determinism is conserved when extensions of a certain restricted class are combined. The caveat is that the constituent extensions must prefix all newly introduced forms with marking tokens drawn from disjoint sets. To be confident that the marking tokens used are disjoint, providers must base them on the domain name system or some other coordinating entity. Because the mechanism operates at the level of the context-free grammar, it is difficult for the client to define scoped abbreviations for these verbose marking tokens. TLMs can be abbreviated (Sec. \ref{sec:ptsms}).

Some programming languages, notably including theorem provers like Coq \cite{Coq:manual} and Agda \cite{norell2007towards}, support ``mixfix'' notation directives \cite{wieland2009parsing,missura1997higher,5134}. Many of these systems enforce capture avoidance and application-site context independence \cite{5134,DBLP:conf/gpce/TahaJ03,Coq:manual,DBLP:conf/ifl/DanielssonN08}. The problem is that mixfix notation requires a fixed number of sub-trees, e.g. \li{if _ then _ else _}. Coq has some limited extensions for list-like literals \cite{Coq:manual}. These systems cannot express the example literal forms from this paper, because they can have any number of spliced terms.


The work of \citet{conf/icfp/LorenzenE13,conf/popl/LorenzenE16} introduces SoundExt, a grammar-based syntax extension system where extension providers can equip their new forms with derived typing rules. The system then attempts to automatically verify that the expansion logic (expressed using a rewrite system, rather than an arbitrary function) is sound relative to these derived rules. TLMs differ in several ways. First, as already discussed, we leave the context-free syntax fixed, so different TLMs cannot conflict. Second, SoundExt does not enforce hygiene, i.e. expansions might depend on the context and intentionally induce capture. Similarly, there is no abstract segmentation discipline. A client can only indirectly reason about binding (but not segmentation) by inspecting the derived typing rules. Unlike TLMs, SoundExt supports type-dependent expansions \cite{conf/popl/LorenzenE16}. The trade-off is that TLMs can generate expansions, and therefore segmentations, even when the program is ill-typed. Another important distinction is that TLMs rely on proto-expansion validation, rather than verification as in SoundExt. The trade-off is that TLMs do not require that the expansion logic be written using a restricted rewriting system, nor does the system require a fully mechanized language definition. Finally, there is no clear notion of ``partial application'' in SoundExt or other syntax definition systems.



%In other words, encountering an unfamiliar derived form has made it difficult for the programmer to maintain the usual \emph{type discipline} and \emph{binding discipline}. %Compelling the programmer to examine the desugaring directly defeat the purpose of defining the derived form -- decreasing cognitive cost. Indeed, it substantially increases cognitive cost.

% In contrast, when a programmer encounters, for example, a function call like the call to \li{read_data} on Line 3, the analagous questions can be answered by following clear protocols that become ``cognitive reflexes'' after sufficient experience with the language, even if the programmer has no experience with the library defining \li{read_data}:
% \begin{enumerate}
% \item The language's syntax definition determines that \li{read_data(a)} is an expression of function application form.
% \item Similarly, \li{read_data} and \li{a} are definitively expressions of variable form.
% \item The variable \li{a} can only refer to the binding of \li{a} on Line 1.
% \item The variable \li{w} can be renamed without knowing anything about the values that \li{read_data} and \li{a} stand for.
% \item The type of \li{x} can be determined to be \li{B} by determining that the type of \li{read_data} is \li{A -> B} for some \li{A} and \li{B}, and checking that \li{a} has type \li{A}. Nothing else needs to be known about the values that \li{read_data} and \li{a} stand for. In Reynolds' words \cite{B304}:
% \begin{quote}
% \emph{Type structure is a syntactic discipline for enforcing levels of abstraction.}
% \end{quote}
% \end{enumerate}

\subsection{Term Rewriting Systems}
Another approach -- and the approach that TLMs are rooted in -- is to leave the context-free syntax of the language fixed, and instead contextually rewrite existing literal forms.

OCaml's textual syntax now includes \emph{preprocessor extension (ppx) points} used to identify terms that some external term rewriting preprocessor must rewrite \cite{ocaml-manual}. We could  mark a string literal as follows:
\begin{lstlisting}[numbers=none]
    [%xml "SSTR<h1>Hello, {[first_name]}!</h1>ESTR"]
\end{lstlisting}
More than one applied preprocessor might recognize this annotation (there are, in practice, many XML/HTML libraries), so the problems of \textbf{Responsibility} comes up. It is also impossible to reason abstractly about \textbf{Segmentation}, \textbf{Capture}, \textbf{Context Dependence} and \textbf{Typing} because the code that the preprocessor generates is unconstrained.% For these reasons, users of this mechanism warn that they should be used sparingly.\todo{citation}

Term-rewriting macro systems are language-integrated local term rewriting systems that require that the client explicitly apply the intended rewriting, implemented by a macro, to the term that is to be rewritten. This addresses the issue of \textbf{Responsibility}. However, unhygienic, untyped macro systems, like the earliest variants of the Lisp macro system \cite{Hart63a}, Template Haskell \cite{SheardPeytonJones:Haskell-02} and GHC's quasiquotation system \cite{mainland2007s} (which is based on Template Haskell), do not allow clients to reason abstractly about the remaining issues, again because the expansion that they produce is unconstrained. (It is not enough that with Template Haskell / GHC quasiquotation, the generated expansion is typechecked -- to satisfy the \textbf{Typing} criterion, it must be possible to reason abstractly about \emph{what the type of the generated expansion is}.)  

\emph{Hygienic} macro systems prevent, or abstractly account for \cite{DBLP:conf/esop/HermanW08,Herman10:Theory}, \textbf{Capture}, and they enforce application-site \textbf{Context Independence} \cite{Kohlbecker86a,DBLP:conf/popl/Adams15,DBLP:conf/popl/ClingerR91,DBLP:journals/lisp/DybvigHB92}. The critical problem is that a standard hygiene discipline makes it impossible to repurpose string literal forms to introduce compositional literal forms at other types. Consider again our running XHTML example, which we might try to realize by applying a hygienic macro, \li{xml!}, to a string literal that the macro parses:
\begin{lstlisting}[numbers=none]
    (xml! "SSTR<h1>Hello, {[first_name]}!</h1>ESTR")
\end{lstlisting}
The expansion of this macro will fail the check for application-site context independence because \li{first_name} will appear as a free variable, in no way different from any other. The hygiene mechanism for TLMs addresses this problem by explicitly distinguishing spliced segments of the literal body in the proto-expansion. This also addresses the problem of \textbf{Segmentation} -- the segmentation abstractly communicates the fact that (only) \li{first_name} is a spliced \li{string} expression.

Much of the research on macro systems has been for languages in the LISP tradition \cite{mccarthy1978history} that do not have rich static type structure. The formal macro calculus studied by \citet{DBLP:conf/esop/HermanW08} (which is not capable of expressing new literal forms, for the reasons just discussed) uses types only to encode the binding structure of the generated expansion (as discussed in Sec. \ref{sec:capture}). Research on typed \emph{staging macro systems} like MetaML \cite{Sheard:1999:UMS}, MetaOCaml \cite{DBLP:conf/flops/Kiselyov14} and MacroML \cite{ganz2001macros} is also not directly applicable to the problem of defining new literal forms -- the syntax tree of the arguments cannot be inspected at all (staging macros are used mainly for partial evaluation and performance-related reasons.) 

The Scala macro system is a hygienic macro system. Its ``black box'' macros support reasoning abstractly about \textbf{Typing} because type annotations constrain the macro arguments and the generated expansions, though the precise reasoning principles available are unclear because the Scala macro system has not been formally specified. The full calculus we have defined is the first detailed type-theoretic accounts of a typed, hygienic macro system of any design for an ML-like language, i.e. one with a rich static type system, support for pattern matching, type functions and ML-like modules.

  Some languages, including Scala \cite{odersky2008programming}, build in \emph{string splicing} (a.k.a. \emph{string interpolation}) forms, or similar but more general \emph{fragmentary quotation forms} \cite{conf/icfp/Slind91}, e.g. SML/NJ. These designate a particular delimiter to escape out into the expression language. The problem with using these together with macros as vehicles to introduce literal forms at various other types is 1) there is no ``one-size-fits-all'' escape delimiter, and 2) typing is problematic because every escaped term is checked against the same type. In the HTML example, we have splicing at two different types using two different delimiters. These forms also cannot appear in patterns.%In general, e.g. when defining syntax for a programming language with many sorts of terms, the most appropriate choice of delimiters might depend on where each spliced term appears.


This brings us back to the most closely related work, that of \citet{TSLs} on \emph{type-specific languages} (TSLs). Like simple expression TLMs (Sec. \ref{sec:setsms}), TSLs allow library providers to programmatically control the parsing of expressions of generalized literal form. With TSLs, parse functions are associated directly with nominal types and invoked according to a bidirectionally typed protocol. In contrast, TLMs are separately defined and explicitly applied. Accordingly, different TLMs can operate at the same type, and can operate at any type, including structural types.  In a subsequent short paper, \citet{sac15} suggested explicit application of simple expression TLMs  also in a bidirectional typed setting \cite{Pierce:2000:LTI:345099.345100}, but this paper did not have any formal content. With TLMs, it is not necessary for the language to be bidirectionally typed (see Sec. \ref{sec:typing-e} on type inference).% It should be possible to implicitly invoke TLMs based on the expected type in situations where the inference engine had enough information, but we leave this as future work. 

Perhaps most importantly, the metatheory presented by \citet{TSLs} establishes only that generated expansions are of the expected type (i.e. a variant of the Typed Expression Expansion theorem from Sec. \ref{sec:s-metatheory}.) It does not establish the remaining abstract reasoning principles that have been the major focus of this paper. In particular, there is no formal hygiene theorem and indeed the formal system in the paper does not correctly handle substitution or capture avoidance, issues we emphasized because they were non-obvious in Sec. 5. Moreover, the TLM does not guarantee that a valid segmentation will exist, nor associate types with segments. 

Finally, the prior work did not consider pattern matching, type functions, ML-style modules, parameters or static evaluation. This paper addresses all of these.

% These existing forms normally have other meanings, so this can be confusing \cite{pane1996usability}.

\newcommand{\discussionSec}{Discussion}
\section{\protect\discussionSec}
\label{sec:discussion}
\label{sec:conclusion}

The importance of specialized notation as a ``tool for thought'' has long been recognized \cite{DBLP:journals/cacm/Iverson80}. According to Whitehead, a good notation ``relieves the brain of unnecessary work'' and ``sets it free to concentrate on more advanced problems'' \cite{cajori1928history}, and indeed, advances in mathematics, science and programming have often been accompanied by new notation. 

Of course, this desire to ``relieve the brain of unnecessary work'' has motivated not only the syntax but also the semantics of languages like ML and Scala -- these languages maintain a strong type and binding discipline so that programmers, and their tools, can hold certain implementation details abstract when reasoning about program behavior.  In the words of \citet{B304}, ``type structure is a syntactic discipline for enforcing levels of abstraction.''

Previously, these two relief mechanisms were in tension---mechanisms  that allowed programmers to express new notation would obscure the type and binding structure of the program text. TLMs resolve this tension for the broad class of literal forms that generalized literal forms subsume. This class includes all of the examples enumerated in Sec. \ref{sec:intro} (up to the choice of outermost delimiter), the case studies detailed in this paper and in the supplement, and the examples collected from the empirical study by \citet{TSLs}.

Of course, not all possible literal notation will prove to be in good taste. %TLMs leave the burden of establishing the value of any particular literal notation to individual library providers, rather than to the language designer. 
The reasoning principles that TLMs provide, which are the primary contributions of this paper, allow clients to "reason around" poor literal designs, using principles analagous to those already familiar to programmers in languages like ML and Scala. % We must leave a detailed empirical evaluation of the impact of particular TLMs on various quality attributes, like programmer productivity and program comprehensibility, as future work. 
% Our own implementation efforts have focused on an emerging alternative front-end for OCaml called Reason, but Scala supports analagous features \cite{conf/oopsla/AminRO14} and would also be a suitable host for the TLM mechanism. The intended audience for this paper is language designers seeking a reasonable solution to the problem of user-defined literal notation.% We also plan to implement TLMs into \li{typy}, a typed functional language embedded into Python as a library \cite{gpce/Omar16}.%We have evaluated this claim by stating the available abstract reasoning principles formally and proving that they hold for the calculi that we have described.

% The examples in this paper are written in an open source alternative syntactic front-end for OCaml called Reason because we are incorporating TLMs into Reason. However, the essential ideas are not Reason-, OCaml- or ML-specific -- it should be possible to adapt TLMs to other languages that take a similar approach to types and binding (e.g. Haskell, Scala and others.) Languages that have a disciplined binding structure but that lack a rich static type structure, e.g. standard Racket, would also benefit from TLMs -- they can apply these results by deploying the usual trick of viewing the language as statically ``{unityped}'' \cite{pfpl,scott1980lambda}.

% The intended audience for this paper is language designers who want to decentralize control over literal forms but seek a detailed understanding, rooted in the first principles of type theory, of a reasonable mechanism by which to achieve that goal. 


%(To summarize Sec. \ref{sec:existing-approaches}, the closest existing typed, hygienic macro system -- the Scala macro system \cite{ScalaMacros2013} -- is not formally specified, the work of \cite{DBLP:conf/esop/HermanW08} uses types only to reason abstractly about binding, and the specification of TSLs in the work of \citet{TSLs} did not formalize hygiene nor handle these advanced language features.)

% Haskell's \li{do}-notation at types with monadic structure cannot be expressed directly using TLMs because it needs to purposefully induce capture in spliced sub-terms, and TLMs prevent capture. This limitation could be resolved by allowing identifiers that appear in literal bodies to be explicitly designated as bound within spliced expressions in the segmentation. The prior work by \citet{DBLP:conf/esop/HermanW08} considered a similar approach in a setting where the locations of sub-terms is known ahead of time (building on the \emph{tree locations} of \citet{gorn1965explicit}, which are conceptually similar to our spliced segment locations.)  We leave as future work the details of this proposal and a consideration of whether this additional expressive power would justify the increased reasoning complexity for clients (it is presently the author's opinion that it would not.)  

A correct parse function never returns an encoding of a proto-expansion that fails validation given well-typed splices, but this invariant cannot be enforced by the ML type system.
% Using a proof assistant, it would be possible to verify that
% a parse function generates only encodings of valid proto-expansions. 
Under a
richer type system, the return type of the parse function itself could be refined so as to
enforce this invariant intrinsically. This problem -- of typed first-class typed term representations -- has been studied in a variety of settings, e.g. in MetaML \cite{Sheard:1999:UMS} and in the modal logic tradition \cite{DBLP:conf/popl/DaviesP96}. % We leave integration of these approaches into the TLM mechanism as future work.
 Our present efforts aim to leave the semantics of OCaml unchanged.

%Another topic of interest has to do with intentional capture. 
Another direction has to do with automated refactoring. The unexpanded language does not come with context-free notions of renaming and substitution. However, given a segmentation, it should be possible to ``backpatch'' refactorings into literal bodies. Recent work by \citet{wand2017inferring} on tracking bindings ``backwards'' from an expansion to the source program is likely relevant. The challenge is that the TLM's splicing logic might not be invariant to refactorings.

At several points in the paper, we allude to editor integration. However, several important questions having to do with TLM-specific syntax highlighting, incremental parsing and error recovery \cite{graham1979practical} remain to be considered, and indeed these are our biggest remaining implementation challenges. % Another interesting direction would be to generalize TLMs to support non-textual notation in the setting of a structure editor. The semantic foundations for structure editors proposed recently by \citet{DBLP:conf/popl/OmarVHAH17} may guide such an effort.

% Let us conclude by considering the oft-uttered phrase ``syntax doesn't matter'', which is generally taken to mean that syntactic concerns are orthogonal to various more important semantic concerns. Our hope is that the reader will leave this paper with an understanding that this is a rather simplistic proclamation, because semantic concerns are relevant to notation design as well! % We hope that this work will lead to more ``reasonable'' variants of other syntactic conveniences, e.g. the ubiquitious ``deriving'' clauses on datatype definitions.

% Other directions for future work are given in the supplement\todo{do this?}.

% Another interesting question is what this mechanism would look like in the setting of a projectional structure editor, i.e. one where the syntax is not textual but rather tree-shaped and projected in an interative manner to the programmer. This paper taken together with work by \citet{Omar:2012:ACC:2337223.2337324,DBLP:conf/popl/OmarVHAH17} can serve to guide such an effort.

% To conclude, TLMs give substantial syntactic control to library providers while leaving programmers with strong abstract reasoning principles. We believe TLMs therefore occupy a ``sweet spot'' in the  design space. %This paper serves to guide those who hope to incorporate TLMs into their future language designs.





%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}

\clearpage
%% Bibliography
\bibliography{../../../papers/research}

\begin{comment}\appendix
\subsection{Parametric TLMs, Formally}
\vspace{-2px}
% \begin{figure}[p]
% \[\begin{array}{llcl}
% \mathsf{UMType} & \urho & ::= 
% %& \autype{\utau} 
% & \utau & \text{type annotation}\\
% % &&
% %& \aualltypes{\ut}{\urho} 
% % & \alltypes{\ut}{\urho} & \text{type parameterization}\\
% &&
% %& \auallmods{\usigma}{\uX}{\urho} 
% & \allmods{\uX}{\usigma}{\urho} & \text{module parameterization}\\
% \mathsf{UMExp} & \uepsilon & ::= 
% %& \abindref{\tsmv} 
% & \tsmv & \text{TLM identifier reference}\\
% % &&
% %& \auabstype{\ut}{\uepsilon} 
% % & \abstype{\ut}{\uepsilon} & \text{type abstraction}\\
% &&
% %& \auabsmod{\usigma}{\uX}{\uepsilon} 
% & \absmod{\uX}{\usigma}{\uepsilon} & \text{module abstraction}\\
% % &&
% %& \auaptype{\utau}{\uepsilon} 
% % & \aptype{\uepsilon}{\utau} & \text{type application}\\
% &&
% %& \auapmod{\uM}{\uepsilon} 
% & \apmod{\uepsilon}{\uX} & \text{module application}\ECC
% \end{array}
% \]
% \caption{Syntax of unexpanded TLM types and expressions.}
% \label{fig:P-macro-expressions-types-u}
% \end{figure}

We will now outline $\miniVerseParam$, a calculus that extends $\miniVersePat$ with parametric TLMs. This calculus is organized, like $\miniVersePat$, as an unexpanded language (UL) defined by typed expansion to an expanded language (XL). There is not enough space to describe $\miniVerseParam$ with the same level of detail as in Sec. \ref{sec:setsms-formally}, so we highlight only the most important concepts below. The details are in the supplement.

The XL consists of 1) module expressions, $M$, classified by signatures, $\sigma$; 2) constructions, $c$, classified by kinds, $\kappa$; and 3) expressions classified by types, which are constructions of kind $\akty$ (we use metavariables $\tau$ instead of $c$ for types by convention.) Metavariables $X$ ranges over module variables and $u$ or $t$ over construction variables. The module and construction languages are based closely on those defined by \citet{pfple1}, which in turn are based on early work by \citet{MacQueen:1984:MSM:800055.802036,DBLP:conf/popl/MacQueen86}, subsequent work on the phase splitting interpretation of modules \cite{harper1989higher} and on using dependent singleton kinds to track type identity \cite{stone2006extensional,DBLP:conf/lfmtp/Crary09}, and finally on formal developments by \citet{dreyer2005understanding} and \citet{conf/popl/LeeCH07}. A complete account of these developments is unfortunately beyond the scope of this paper. The expression language extends the language of $\miniVersePat$ only to allow projection out of modules.

The main conceptual difference between $\miniVersePat$ and $\miniVerseParam$ is that $\miniVerseParam$ introduces the notion of unexpanded and expanded TLM expressions and types, as shown in Figure \ref{fig:P-macro-expressions-types}. 

\begin{figure}[h]
\vspace{-6px}
\small
\begin{minipage}{0.38\textwidth}
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{UMType} & \urho & ::= & \utau ~\vert~ \allmods{\uX}{\usigma}{\urho}\\
\mathsf{UMExp} & \uepsilon & ::= & \tsmv ~\vert~ \absmod{\uX}{\usigma}{\uepsilon}~\vert~ \apmod{\uepsilon}{\uX}
\end{array}$
\end{minipage}
\begin{minipage}{0.6\textwidth}
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{MType} & \rho & ::= & \aetype{\tau} ~\vert~ \aeallmods{\sigma}{X}{\rho} \\
\mathsf{MExp} & \epsilon & ::= & \adefref{a} ~\vert~ \aeabsmod{\sigma}{X}{\epsilon} ~\vert~\aeapmod{X}{\epsilon} 
\end{array}$
\end{minipage}
\vspace{-10px}
\caption{Syntax of unexpanded and expanded TLM types and expressions in $\miniVerseParam$}
\label{fig:P-macro-expressions-types}
\vspace{-10px}
\end{figure}
The TLM type $\aeallmods{\sigma}{X}{\rho}$ classifies TLM expressions that have one module parameter matching $\sigma$. For simplicity, we formalize only module parameters. Type parameters can be expressed as module parameters having exactly one abstract type member.

The rule governing expression TLM application, reproduced below, touches all of the main ideas in $\miniVerseParam$, so we will refer to it throughout the remainder of this section.%It might be useful to compare this rule to Rule \textsc{ee-ap-sptsm}.
{\small\begin{mathpar}
\inferrule[ee-ap-petsm]{
  \uOmega = \uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}\\
  \uPsi=\uAS{\uA}{\Psi}\\\\
  \tsmexpExpandsExp{\uOmega}{\uPsi}{\uepsilon}{\epsilon}{\aetype{\tau_\text{final}}}\\
  \tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}\\\\
  \tsmdefof{\epsilon_\text{normal}}=a\\
  \Psi = \Psi', \petsmdefn{a}{\rho}{\eparse}\\\\
  \encodeBody{b}{\ebody}\\
  \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{e_\text{pproto}}}\\
  \decodePCEExp{e_\text{pproto}}{\pce}\\\\
  \prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon_\text{normal}}{\aetype{\tau_\text{proto}}}{\omega}{\Omega_\text{params}}\\\\
  \segOK{\segof{\ce}}{b}\\
  \cvalidEP{\Omega_\text{params}}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\ce}{e}{\tau_\text{proto}}
}{
  \expandsP{\uOmega}{\uPsi}{\uPhi}{\utsmap{\uepsilon}{b}}{[\omega]e}{[\omega]\tau_\text{proto}}
}
\end{mathpar}}

The first two premises simply deconstruct the (unified) unexpanded context $\uOmega$ (which tracks the expansion of expression, constructor and module identifiers, as $\uDelta$ and $\uGamma$ did in $\miniVersePat$) and peTLM context, $\uPsi$. Next, we expand $\uepsilon$ according to straightforward unexpanded peTLM expression expansion rules. The resulting TLM expression, $\epsilon$, must be defined at a type (i.e. no quantification over modules must remain once the literal body is encountered.)

The fourth premise performs \emph{peTLM expression normalization}, $\tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}$. This is defined in terms of a structural operational semantics \cite{DBLP:journals/jlp/Plotkin04a} with two stepping rules:
% \begin{equation*}\tag{\ref{rule:tsmexpEvalsExp}}
% \inferrule{
%   \tsmexpMultistepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}\\
%   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon'}
% }{
%   \tsmexpEvalsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
% }
% \end{equation*}
% where the multistep judgement, $\tsmexpMultistepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}$, is defined as the reflexive, transitive closure of the stepping judgement defined by the following rules:
% \begin{equation*}\tag{\ref{rule:tsmexpStepsExp-aptype-1}}
% \inferrule{
%   \tsmexpStepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
% }{
%   \tsmexpStepsExp{\Omega}{\Psi}{\aeaptype{\tau}{\epsilon}}{\aeaptype{\tau}{\epsilon'}}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpStepsExp-aptype-2}}
% \inferrule{ }{
%   \tsmexpStepsExp{\Omega}{\Psi}{\aeaptype{\tau}{\aeabstype{t}{\epsilon}}}{[\tau/t]\epsilon}
% }
% \end{equation*}
{\small\begin{mathpar}
\inferrule[eps-dyn-apmod-subst-e]{ }{
  \tsmexpStepsExp{\Omega}{\Psi}{\aeapmod{X}{\aeabsmod{\sigma}{X'}{\epsilon}}}{[X/X']\epsilon}
}

\inferrule[eps-dyn-apmod-steps-e]{
  \tsmexpStepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
}{
  \tsmexpStepsExp{\Omega}{\Psi}{\aeapmod{X}{\epsilon}}{\aeapmod{X}{\epsilon'}}
}
\end{mathpar}}
% The peTLM expression normal forms are defined as follows:
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-defref}}
% \inferrule{ }{
%   \tsmexpNormalExp{\Omega}{\Psi, \petsmdefn{a}{\rho}{\eparse}}{\adefref{a}}
% }
% \end{equation*}
% % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-abstype}}
% % \inferrule{ }{
% %   \tsmexpNormalExp{\Omega}{\Psi}{\aeabstype{t}{\epsilon}}
% % }
% % \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-absmod}}
% \inferrule{ }{
%   \tsmexpNormalExp{\Omega}{\Psi}{\aeabsmod{\sigma}{X}{\epsilon}}
% }
% \end{equation*}
% % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-aptype}}
% % \inferrule{
% %   \epsilon \neq \aeabstype{t}{\epsilon'}\\
% %   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon}
% % }{
% %   \tsmexpNormalExp{\Omega}{\Psi}{\aeaptype{\tau}{\epsilon}}
% % }
% % \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-apmod}}
% \inferrule{
%   \epsilon \neq \aeabsmod{\sigma}{X'}{\epsilon'}\\
%   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon}
% }{
%   \tsmexpNormalExp{\Omega}{\Psi}{\aeapmod{X}{\epsilon}}
% }
% \end{equation*}
\vspace{-5px}

Normalization eliminates parameters introduced in higher-order abbreviations, leaving only those parameter applications specified by the original TLM definition. Normal forms and progress and preservation theorems are established in the supplement.

The third row of premises looks up the applied TLM's definition by invoking a simple metafunction to extract its name, $a$, then looking up $a$ within the peTLM definition context, $\Psi$.
% \begin{align}
% \tsmdefof{\adefref{a}} & = a \tag{\ref{eqn:tsmdefof-adefref}}\\
% % \tsmdefof{\aeabstype{t}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-abstype}}\\
% \tsmdefof{\aeabsmod{\sigma}{X}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-absmod}}\\
% % \tsmdefof{\aeaptype{\tau}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-aptype}}\\
% \tsmdefof{\aeapmod{X}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-apmod}}
% \end{align}


The fourth row of premises 1) encodes the body as a value of the type $\tBody$; 2) applies the parse function; and 3) decodes the result, producing a \emph{parameterized proto-expression}, $\pce$. Parameterized proto-expressions, $\pce$, are ABTs that serve simply to introduce the parameter bindings into an underlying proto-expression, $\ce$. The syntax of parameterized proto-expressions is given below.

\vspace{-4px}{\small\[\begin{array}{llcl}
% \textbf{Sort} & & & \textbf{Operational Form} & \textbf{Stylized Form} & \textbf{Description}\\
% \LCC \color{Yellow}&\color{Yellow}&\color{Yellow}& \color{Yellow} & \color{Yellow} & \color{Yellow}\\
\mathsf{PPrExp} & \pce & ::= & \apceexp{\ce} ~\vert~ \apcebindmod{X}{\pce}
\end{array}\]}%
\vspace{-6px}%

There must be one binder in $\pce$ for each TLM parameter specified by $a$. (In Reason, we can insert these binders automatically as a convenience.) 

The judgement on the fifth row of Rule \textsc{ee-ap-petsm} then \emph{deparameterizes} $\pce$ by peeling away these binders to produce 1) the underlying proto-expression, $\ce$, with the variables that stand for the parameters free; 2) a corresponding deparameterized type, $\tau_\text{proto}$, that uses the same free variables to stand for the parameters; 3) a \emph{substitution}, $\omega$, that pairs the applied parameters from $\epsilon_\text{normal}$ with the corresponding variables generated when peeling away the binders in $\pce$; and 4) a corresponding \emph{parameter context}, $\Omega_\text{params}$, that tracks the signatures of these variables. The two rules governing the proto-expression deparameterization judgement are below:
{\small\begin{mathpar}
\inferrule{ }{
  \prepce{\Omega_\text{app}}{\Psi, \petsmdefn{a}{\rho}{\eparse}}{\apceexp{\ce}}{\ce}{\adefref{a}}{\rho}{\emptyset}{\emptyset}
}

\vspace{-4px}\inferrule{
  \prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon}{\aeallmods{\sigma}{X}{\rho}}{\omega}{\Omega}\\
  X \notin \domof{\Omega_\text{app}}
}{
  \prepce{\Omega_\text{app}}{\Psi}{\apcebindmod{X}{\pce}}{\ce}{\aeapmod{X'}{\epsilon}}{\rho}{(\omega, X'/X)}{(\Omega, X : \sigma)}
}
\end{mathpar}}%
This judgement can be pronounced ``when applying peTLM $\epsilon$, $\pce$ has deparameterization $\ce$ leaving $\rho$ with parameter substitution $\omega$''. 
Notice based on the second rule that every module binding in $\pce$ must pair with a corresponding module parameter application. Moreover, the variables standing for parameters must not appear in $\Omega_\text{app}$, i.e. $\domof{\Omega_\text{params}}$ must be disjoint from $\domof{\Omega_\text{app}}$ (this requirement can always be discharged by alpha-variation.)

The final row of premises checks that the segmentation of $\ce$ is valid and  performs proto-expansion validation under the parameter context, $\Omega_\text{param}$ (rather than the empty context, as was the case in $\miniVersePat$.) The conclusion of the rule applies the parameter substitution, $\omega$, to the resulting expression and the deparameterized type.

Proto-expansion validation operates conceptually as in $\miniVersePat$. The only subtlety has to do with the type annotations on references to spliced terms. As described at the end of Sec. \ref{sec:ptsms-by-example}, these annotations might refer to the parameters, so the parameter substitution, $\omega$, which is tracked by the splicing scene, must be applied to the type annotation before proceeding recursively to expand the referenced unexpanded term. However, the spliced term itself must treat parameters parametrically, so the substitution is not applied in the conclusion of the following rule:
\begin{mathpar}\label{rule:cvalidE-P-splicede}
\inferrule{
  \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\
    \cvalidC{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\ctau}{\tau}{\akty}\\
  \expandsP{\uOmega}{\uPsi}{\uPhi}{\ue}{e}{[\omega]\tau}\\\\
  \uOmega=\uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}\\
  \domof{\Omega} \cap \domof{\Omega_\text{app}} = \emptyset
}{
  \cvalidEP{\Omega}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\acesplicede{m}{n}{\ctau}}{e}{\tau}
}
\end{mathpar}
(This is only sensible because we maintain the invariant that $\Omega$ is always an extension of $\Omega_\text{params}$.)

The calculus enjoys metatheoretic properties analagous to those described in Sec. \ref{sec:s-metatheory}, modified to account for the presence of modules, kinds and parameterization. The following theorem establishes the abstract reasoning principles available when applying a parametric expression TLM. The clauses are directly analagous to those of Theorem \ref{thm:setlm-reasoning}, so for reasons of space we do not repeat the inline descriptions. The \textbf{Kinding} clauses can be understood by analogy to the \textbf{Typing} clauses. The details of parametric pattern TLMs (ppTLMs) are analagous (see supplement.)
\vspace{-3px}
\begin{theorem}[peTLM Reasoning Principles]
If $\expandsP{\uOmega}{\uPsi}{\uPhi}{\utsmap{\uepsilon}{b}}{e}{\tau}$ then:
\begin{enumerate}[nolistsep,noitemsep]
  \item $\uOmega=\uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}$
  \item $\uPsi=\uAS{\uA}{\Psi}$
  \item (\textbf{Typing 1}) $\tsmexpExpandsExp{\uOmega}{\uPsi}{\uepsilon}{\epsilon}{\aetype{\tau}}$ and $\hastypeP{\Omega_\text{app}}{e}{\tau}$
  \item $\tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}$
  \item $\tsmdefof{\epsilon_\text{normal}}=a$
  \item $\Psi = \Psi', \petsmdefn{a}{\rho}{\eparse}$
  \item $\encodeBody{b}{\ebody}$
    \item $\evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{e_\text{pproto}}}$
  \item $\decodePCEExp{e_\text{pproto}}{\pce}$
  \item $\prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon_\text{normal}}{\aetype{\tau_\text{proto}}}{\omega}{\Omega_\text{params}}$
  \item (\textbf{Segmentation}) $\segOK{\segof{\ce}}{b}$
  \item $\cvalidEP{\Omega_\text{params}}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\ce}{e'}{\tau_\text{proto}}$
  \item $e = [\omega]e'$
  \item $\tau = [\omega]\tau_\text{proto}$
  \item $
    \segofexisitng-{\ce} = \sseq{\acesplicedk{m_i}{n_i}}{\nkind} \cup \sseq{\acesplicedc{m'_i}{n'_i}{\cekappa'_i}}{\ncon} \cup $ \\
     ~~~~$          \sseq{\acesplicede{m''_i}{n''_i}{\ctau_i}}{\nexp}
    $
  \item (\textbf{Kinding 1}) $\sseq{\kExpands{\uOmega}{\parseUKindF{\bsubseq{b}{m_i}{n_i}}}{\kappa_i}}{\nkind}$ and \\ ~~~~$\sseq{\iskind{\Omega_\text{app}}{\kappa_i}}{\nkind}$
  \item (\textbf{Kinding 2}) $\sseq{\cvalidK{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\cekappa'_i}{\kappa'_i}}{\ncon}$ and $\sseq{\iskind{\Omega_\text{app}}{[\omega]\kappa'_i}}{\ncon}$
  \item (\textbf{Kinding 3}) $\sseq{\cExpands{\uOmega}{\parseUConF{\bsubseq{b}{m'_i}{n'_i}}}{c_i}{[\omega]\kappa'_i}}{\ncon}$ and $\sseq{\haskind{\Omega_\text{app}}{c_i}{[\omega]\kappa'_i}}{\ncon}$
  \item (\textbf{Kinding 4}) $\sseq{\cvalidC{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\ctau_i}{\tau_i}{\akty}}{\nexp}$ and $\sseq{\haskind{\Omega_\text{app}}{[\omega]\tau_i}{\akty}}{\nexp}$
  \item (\textbf{Typing 2}) $\sseq{\expandsP{\uOmega}{\uPsi}{\uPhi}{\parseUExpF{\bsubseq{b}{m''_i}{n''_i}}}{e_i}{[\omega]\tau_i}}{\nexp}$ and $\sseq{\hastypeP{\Omega_\text{app}}{e_i}{[\omega]\tau_i}}{\nexp}$
  \item (\textbf{Capture Avoidance}) $e = [\sseq{\kappa_i/k_i}{\nkind}, \sseq{c_i/u_i}{\ncon}, \sseq{e_i/x_i}{\nexp}, \omega]e''$ for some $e''$ and fresh $\sseq{k_i}{\nkind}$ and fresh $\sseq{u_i}{\ncon}$ and fresh $\sseq{x_i}{\nexp}$
  \item (\textbf{Context Independence}) $\mathsf{fv}(e'') \subset \sseq{k_i}{\nkind} \cup \sseq{u_i}{\ncon} \cup \sseq{x_i}{\nexp} \cup \domof{\OParams}$
  % $\hastypeP{\sseq{\Khyp{k_i}}{\nkind} \cup \sseq{u_i :: [\omega]\kappa'_i}{\ncon} \cup \sseq{x_i : [\omega]\tau_i}{\nexp}}{[\omega]e''}{\tau}$\todo{maybe restate this in terms of free variables of e'' here and elsewhere, because context isn't technically well-formed here?}
\end{enumerate}
\end{theorem}
\end{comment}

\end{document}
