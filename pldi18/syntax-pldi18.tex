\PassOptionsToPackage{svgnames,dvipsnames,svgnames}{xcolor}
%% For double-blind review submission
\documentclass[sigplan,review,10pt,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
% \documentclass[acmsmall,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission
%\documentclass[acmlarge,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission
%\documentclass[acmlarge]{acmart}\settopmatter{}

%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format should change 'acmlarge' to
%% 'sigplan,10pt'.


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

%% Cyrus packages
\usepackage{microtype}
\usepackage{todonotes}
\usepackage{mdframed}
\usepackage{colortab}
\usepackage{mathpartir}
\usepackage{enumitem}
\usepackage{bbm}
\definecolor{light-gray}{gray}{0.95}

% \newtheorem{theorem}{Theorem}[chapter]
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \newtheorem{condition}[theorem]{Condition}

% \usepackage{titlesec}
% \titlespacing{\paragraph}{0pt}{10pt}{10pt}

\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{0.25ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother


%% Listings
\definecolor{mygray}{rgb}{0.75,0.75,0.75}
\usepackage{listings}
\lstset{tabsize=2, 
basicstyle=\ttfamily\fontsize{8pt}{1em}\selectfont, 
commentstyle=\itshape\ttfamily\color{gray},
stringstyle=\ttfamily\color{red},
mathescape=false,escapechar=~,
numbers=left, numberstyle=\scriptsize\color{mygray}, language=ML,moredelim=[il][\sffamily]{?},showspaces=false,showstringspaces=false,xleftmargin=0pt, numbersep=-6pt, morekeywords=[1]{try,spliced,tyfam,opfam,let,fn,val,def,casetype,objtype,metadata,of,*,string,datatype,new,toast,syntax,module,switch,where,using,import,for,ana,syn,opcon,tycon,metasignature,metamodule,metasig,metamod,static,at,by,tycase,mod,macro,match,pattern,in,patterns,expressions,implicit,forall,exptsm,pattsm},deletekeywords={double},classoffset=0, xleftmargin=-10pt, 
aboveskip=4pt,belowskip=2pt,
moredelim=**[is][\color{red}]{SSTR}{ESTR},
moredelim=**[is][\color{OliveGreen}]{SHTML}{EHTML},
moredelim=**[is][\color{purple}]{SCSS}{ECSS},
moredelim=**[is][\color{brown}]{SSQL}{ESQL},
moredelim=**[is][\color{orange}]{SCOLOR}{ECOLOR},
moredelim=**[is][\color{magenta}]{SPCT}{EPCT}, 
moredelim=**[is][\color{gray}]{SNAT}{ENAT}, 
moredelim=**[is][\color{OliveGreen}]{SURL}{EURL},
moredelim=**[is][\color{SeaGreen}]{SQT}{EQT},
moredelim=**[is][\color{Periwinkle}]{SGRM}{EGRM},
moredelim=**[is][\color{YellowGreen}]{SID}{EID},
moredelim=**[is][\color{Sepia}]{SUS}{EUS},
deletestring=[d]{"},
}
\lstset{morecomment=[n]{/*}{*/}}

\newcommand{\liv}[1]{\lstinline{#1}}
\newcommand{\li}[1]{\lstinline[basicstyle=\ttfamily\fontsize{9pt}{1em}\selectfont]{#1}}


\makeatletter\if@ACM@journal\makeatother
%% Journal information (used by PACMPL format)
%% Supplied to authors by publisher for camera-ready submission
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{1}
\acmArticle{1}
\acmYear{2017}
\acmMonth{1}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\else\makeatother
%% Conference information (used by SIGPLAN proceedings format)
%% Supplied to authors by publisher for camera-ready submission
\acmConference[PL'17]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2017}{New York, NY, USA}
\acmYear{2017}
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\fi


%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission
\setcopyright{none}             %% For review submission
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2017}           %% If different from \acmYear


%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
% \citestyle{acmauthoryear}   %% For author/year citations

\input{macros}


\begin{document}

%% Title information
\title[Reasonably Programmable Literal Notation]{Reasonably Programmable Literal Notation}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
% \titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
% \subtitle{Subtitle}                     %% \subtitle is optional
% \subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Paper note
%% The \thanks command may be used to create a "paper note" ---
%% similar to a title note or an author note, but not explicitly
%% associated with a particular element.  It will appear immediately
%% above the permission/copyright statement.
% \thanks{with paper note}                %% \thanks is optional
                                        %% can be repeated if necesary
                                        %% contents suppressed with 'anonymous'


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
%Many languages define convenient list literal forms. This paper questions this common practice: why should an otherwise ordinary data structure be privileged in the definition of a general-purpose programming language? 
General-purpose programming languages typically define literal notation for only a small number of data structures, e.g. lists. This design practice is unsatisfying because there are many other data structures for which literal notation might be useful, e.g. finite maps, regexes, HTML data, SQL queries, syntax trees and encodings of chemical structures. %A less \emph{ad hoc} design would be one that allows library providers to define new literal forms in a decentralized manner. However, the mechanisms available to programmers today, e.g. Camlp4 \cite{ocaml-manual} and Sugar* \cite{erdweg2011sugarj,erdweg2013framework}, are \emph{unreasonable}. In particular, they do not support modular reasoning about syntactic determinism, so separately defined literal forms can and do conflict syntactically with one another. Moreover, clients cannot reason abstractly about types and binding when they encounter an unfamiliar literal form (like they can when they encounter an unfamiliar function being applied.)% Instead, they must reason transparently, i.e. about the underlying expansion. This increases cognitive cost, defeating much of the purpose of convenient literal forms. %This makes it difficult to program ``in the large'', and these mechanisms have not been widely adopted.% Hygienic, typed term-rewriting systems, e.g. the Scala macro system \cite{ScalaMacros2013}, are only somewhat more reasonable, and do not offer direct syntactic control.
This paper introduces \emph{typed literal macros (TLMs)}, which allow library providers to define new literal notation of nearly arbitrary form at any specified type or parameterized family of types. 
% TLMs give library providers programmatic control over the parsing and expansion of expressions and patterns of a flexible \emph{generalized literal form} at {a} specified type or parameteric family of types. % The mechanism is {strictly hygienic}, meaning that 1) the expansion must be context-independent; and 2)  splicing is capture avoiding. 
% Partial parameter application lowers the syntactic cost of this strict style. 
Compared to existing approaches, including direct grammar extension systems and prior  macro-based parsing systems, TLMs are uniquely \emph{reasonable}: TLM providers can reason modularly about syntactic ambiguity, and TLM clients can reason abstractly, i.e. without examining the underlying expansion, about types and binding. The system only needs to convey to clients via secondary notation the inferred \emph{segmentation} of each literal, which gives the locations of spliced subterms. 
%Maintaining information about segmentation is necessary for the novel TLM hygiene mechanism, i.e. the mechanism that ensures that clients can reason abstractly about binding (previously, only unhygienic macro systems supported splicing of terms out of string literal bodies.) 
%The system can determine this {segmentation} automatically and convey it to the client via syntax highlighting, or by presenting the client with a context-free grammar (without revealing the semantic actions associated with each production.) 
% This mechanism captures many common syntactic idioms while avoiding the problem of syntactic conflicts by construction and supplying clients with clear abstract reasoning principles. 
This paper establishes these abstract reasoning principles formally with a calculus of typed expressions, patterns and modules. This calculus is the first detailed type-theoretic account of a hygienic macro system, of any design, for a language with these essential features of ML, and the first macro system with support for literal parsing that comes equipped with these essential reasoning principles. We are integrating TLMs into Reason, an increasingly popular alternative front-end for OCaml.% This calculus is the first such formalization of a typed macro system.

% \emph{Typed syntax matcros (TLMs)}, proposed in a recent short paper by Omar et al. \cite{sac15}, give library providers programmatic control over the parsing and expansion of only terms of {(generalized) literal form}. This appears to occupy a ``sweet spot'' in that it captures many common syntactic idioms while avoiding the problem of conflict. TLMs also maintain a reasonable type and binding discipline.% In particular, clients can use any combination of TLMs in a program without needing to consider conflicts between them, and the language validates each expansion that a TLM generates to maintain 1) a \emph{type discipline} (meaning clients can determine the type of an unexpanded expression without examining its expansion directly); 
% %and 2) a \emph{hygienic binding discipline}.
% %, meaning that the expansion cannot make any assumptions about bindings at the application site, nor  introduce ``hidden bindings'' into subterms. 

% TLMs have only been described minimally -- it is not clear how they should be adapted for integration into languages like ML that support {pattern matching}, {parameterized datatypes}, {modules} and abstract types. Moreover, the prior work makes several simplifying assumptions related to binding that are impractically restrictive.% bno mechanism for binding values for use across TLM definitions has been described and the hygiene mechanism makes giving the expansions that they generate access to ``helper functions'' awkward.

% This paper gives a complete account of TLMs that addresses these deficiencies. In particular, we 1) integrate TLMs with pattern matching; 2) introduce a distinct static phase of evaluation, which gives TLM definitions access to libraries; and 3) introduce type and module parameters, which serve two purposes: they allow for TLMs that operate uniformly at a parameterized family of types (rather than only at a single type), and they give expansions explicit, hygienic access to libraries. Support for partial application  of parameters lowers the syntactic cost of this explicit approach. 

% Put succinctly, we design a programming language in the ML tradition with a \emph{reasonably} programmable syntax.
\vspace{-10px}
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
% \begin{CCSXML}
% <ccs2012>
% <concept>
% <concept_id>10011007.10011006.10011008</concept_id>
% <concept_desc>Software and its engineering~General programming languages</concept_desc>
% <concept_significance>500</concept_significance>
% </concept>
% <concept>
% <concept_id>10003456.10003457.10003521.10003525</concept_id>
% <concept_desc>Social and professional topics~History of programming languages</concept_desc>
% <concept_significance>300</concept_significance>
% </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Software and its engineering~General programming languages}
% \ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
% \keywords{keyword1, keyword2, keyword3}  %% \keywords is optional


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle

% \vspace{-4px}
\newcommand{\introSec}{Introduction}
\section{\protect\introSec}
\label{sec:intro}
% The surface syntax of a programming language is its user interface. 
When designing the surface syntax of a general-purpose programming language, it is common to build in \emph{literal notation} that decreases the syntactic cost of constructing and pattern matching over values of a few standard data structures. For example, in Standard ML (SML), the list literal \li{[x, y, z]} is equivalent by desugaring to \li{Cons(x,Cons(y,Cons(z,Nil)))} where \li{Nil} and \li{Cons} stand for the constructors of the standard \li{list} datatype \cite{mthm97-for-dart,harper1997programming}. 
%\footnote{In SML, list literals work even in contexts where the list expression constructors have been shadowed by other bindings, i.e. the meaning of literal forms is \emph{context independent}. 
% We will return to this concept again later.} 
Similarly, Ur/Web's surface syntax extends Ur's syntax with expression and pattern literals for encodings of XML and HTML data \cite{conf/popl/Chlipala15,conf/pldi/Chlipala10}. Figure \ref{fig:urweb} shows two such HTML literals, one that ``splices in'' a string expression (Line 1) and the other an HTML expression (Line 2).
%The desugarings of these literal expressions, not shown, are substantially more verbose and less readable to programmers familiar with XHTML. 

This language design practice is decidedly \emph{ad hoc} -- it is quite easy to come up with other examples of data structures that are not privileged with literal forms in these languages, but for which mathematicians, scientists or programmers have invented specialized notation \cite{DBLP:journals/cacm/Iverson80,cajori1928history}. For example, 1) clients of a ``collections'' library might also want matrix, set and map literals following usual mathematical conventions; 2) clients of a ``web programming'' library might like not just HTML literals but also CSS literals (which Ur/Web lacks); 3) a compiler writer might like ``quotation'' literals for the terms of the object language and various intermediate languages; and 4) clients of a ``chemistry'' library might like chemical structure literals based on the SMILES standard \cite{anderson1987smiles}.

Although requests for new literal notations are easy to dismiss as superficial, the reality is that literal notation, or the absence thereof, can have a substantial influence on software quality. For example, \citet{Bravenboer:2007:PIA:1289971.1289975} finds that literal notation for structured encodings of queries, like the SQL-based query literals now found in many languages \cite{meijer2006linq}, reduce the temptation to use string encodings of queries and therefore reduce the prevalence of string injection attacks, which are amongst the most common and serious security vulnerabilities on web servers today \cite{owasp2017}. More generally, evidence suggests that programmers frequently engage in ``stringly-typed programming'', i.e. they choose strings instead of composite data structures largely for reasons of notational convenience. In particular, \citet{TSLs} sampled strings from open source projects and found that about 15\% of them were parseable according to some readily apparent type-specific grammar, e.g. for SQL queries, regexes, file system paths, URLs and many other data structures. Literal forms, with support for splicing, would decrease the syntactic cost of composite encodings of such data, which are more amenable to programmatic manipulation and compositional reasoning than flat string encodings. %Literal forms would  preferable to string encodings because they support compositional construction and decomposition by pattern matching via splicing.

\begin{figure*}
\vspace{-2px}
\begin{lstlisting}
  fun heading first_name = SURL<xml><h1>Hello, {[EURLfirst_nameSURL]}!</h1></xml>EURL
  val body = SURL<xml><body>{EURLheading "World"SURL} ...</body></xml>EURL
\end{lstlisting}
\vspace{-10px}
\caption{HTML literals with support for splicing at two different types are built primitively into Ur/Web \cite{conf/popl/Chlipala15}.}
\vspace{-10px}
\label{fig:urweb}
\end{figure*}

Of course, it would not be sensible to ask general-purpose language designers to accomodate all known notations with built-in literal forms. Instead, there has been considerable interest for many years in mechanisms that allow library providers to define new literal notation on their own. For example, direct grammar extension systems like Camlp4 \cite{ocaml-manual} and SugarJ \cite{erdweg2011sugarj,erdweg2013framework}, macro-based systems like Omar et al's \emph{type-specific languages} (TSLs) \cite{TSLs}, and others that we will discuss in Sec. \ref{sec:existing-approaches} can all be used to define new literal notation. 

\paragraph{Problem} The problem that motivates this paper is that it is impossible to reason abstractly, i.e. by inspecting only the surface syntax, about such fundamental issues as types and variable binding when a program text is littered with  unfamiliar user-defined literal forms. Instead, programmers can only reason transparently, i.e. by inspecting the underlying expansion or the extension responsible for producing the expansion. In some existing systems, even determining which extension is responsible for a literal form is difficult.
%, i.e. when developing large programs ``consisting of many small programs (modules), possibly written by different people''

\begin{figure}[t!]
\vspace{-2px}
\begin{lstlisting}
  let w = compute_w();
  let x = compute_x(w);
  let y = [|(!R)@&{&/x!/:2_!x}'!R}|];
\end{lstlisting}
\vspace{-10px}
\caption{Syntax extension systems produce unreasonable program text.}
\vspace{-14px}
\label{fig:K-dialect}
\end{figure}

Consider, for example, the perspective of a programmer attempting to comprehend, i.e. reason about, the program text in Figure \ref{fig:K-dialect}, which is written in a dialect of OCaml's syntax called Reason \cite{reason} that has, hypothetically, been extended with some large number of new literal forms by a grammar extension system, like Camlp4 \cite{ocaml-manual} or Sugar* \cite{erdweg2011sugarj,erdweg2013framework}. 
The literal form on Line 3 constructs an encoding of a query in the niche stack-based database query language K, using its intentionally terse notation \cite{Whitney:2001:LOR:376284.375783}. The problem is that a programmer unfamiliar with this notation cannot fall back on any abstract reasoning principles to answer questions like:

\begin{enumerate}[nolistsep,noitemsep,leftmargin=10pt]
\item \textbf{(Responsibility)} Which syntax extension determined the expansion of the literal form on Line 3? Might different extensions generate different expansions for this form (i.e. is there syntactic ambiguity)?
\item \textbf{(Segmentation)} Are the characters \li{x} and \li{R} on Line 3 parsed as spliced expressions (meaning that they appear directly in the underlying expansion), or are they parsed in some other way peculiar to this literal form (e.g. as operators in the K query language)?
\item \textbf{(Capture)} If \li{x} is in fact a spliced expression, does it refer to the binding of \li{x} on Line 2, or might it capture an unseen binding of the same identifier in the expansion of Line 3?
\item \textbf{(Context Dependence)} If \li{w}, on Lines 1-2, is renamed, could that possibly break Line 3 or change its meaning because the expansion assumes that \li{w} is in scope?% What other hidden assumptions might the desugaring be making?
\item \textbf{(Typing)} What type does \li{y} have?
\end{enumerate}

% In short, the problem is that library providers cannot reason modularly about syntactic determinism (because other extensions might define overlapping forms), and if the desugaring of the program text is held abstract, programmers can no longer reason about types, binding and segmentation (i.e. answer questions like those above.) This is burdensome at all scales, but particularly when programming in the large, where it is common to encounter a wide variety of libraries \cite{DBLP:conf/sac/LammelPS11}. 
Forcing the programmer to reason transparently to answer basic questions like these defeats the ultimate purpose of syntactic sugar: decreasing cognitive cost \cite{Green89}. Analagous problems do not arise when programming without syntax extensions in languages like ML and Scala -- the binding discipline of the language ensures that programmers can reason lexically about binding, and types mediate abstraction over function and module implementations. % From this perspective, it is unsurprising that the use of Camlp4 has been deprecated in the OCaml system.

\paragraph{Approach} 



This paper formally defines a more reasonable mechanism for defining new literal notation: \emph{typed literal macros (TLMs)}. TLMs give library providers  programmatic control over the parsing and expansion of \emph{generalized literal forms} \cite{TSLs}, which syntactically subsume other literal forms because their bodies are left initially unparsed according to the fixed context-free syntax of the language (i.e. they are initially parsed like raw string literals.) 


\begin{figure}
\vspace{-2px}
\begin{lstlisting}
  let w = compute_w();
  let x = compute_x(w);
  let y = $kquery [|SURL(!R)@&{&/EURLxSURL!/:2_!EURLxSURL}'!R}EURL|];
\end{lstlisting}
\vspace{-10px}
\caption{TLMs make examples like the one from Figure \ref{fig:K-dialect} more reasonable.}
\vspace{-14px}
\label{fig:K-tsm-example}
\end{figure}

For example, Figure \ref{fig:K-tsm-example} applies a TLM named \li{$kquery} to a generalized literal form delimited by \li{[|} and \li{|]} to express the K query example from Figure \ref{fig:K-dialect} just discussed. The applied TLM parses the literal body (i.e. the characters between the delimiters) and generates an expansion statically. During the typing phase, a validation step ensures that programmers can reason abstractly about the five issues just identified. We will characterize these reasoning principles precisely as we proceed, but for now, let us develop the basic intuition -- the programmer can reason abstractly about Line 3 as follows:
\begin{enumerate}[noitemsep,nolistsep,leftmargin=10pt]
\item \textbf{(Responsibility)} The applied TLM, \li{$kquery}, parses and determines the expansion of the literal body. The context-free syntax of the language is fixed, so the TLM provider is able to modularly establish that the syntax that this parser implements is unambiguous (even in situations where a spliced term itself applies a TLM, not shown here.)
\item \textbf{(Segmentation)} The intermediate output that the TLM generates is structured so that the system can infer from it an accurate \emph{segmentation} of the literal body that distinguishes spliced terms from segments that are parsed in some other way by the TLM. The segmentation can be communicated by a code editor using secondary notation, e.g. colors in this document. So by examining Figure \ref{fig:K-tsm-example}, the programmer knows that the two instances of \li{x} on Line 3 are parsed as spliced expressions (because they are black), whereas the \li{R}'s must be parsed in some other way, e.g. as operators in the K language (because they are  green.)
\item \textbf{(Capture)} Splicing is capture-avoiding, so the spliced expression \li{x} must refer to the binding of \li{x} on Line 2. It cannot capture a coincidental binding of \li{x} in the expansion.
\item \textbf{(Context Dependence)} The system enforces context independence, so the expansion of Line 3 cannot  rely on the fact that, e.g., \li{w} is in scope. The client is free to rename \li{w}.
\item \textbf{(Typing)} A type annotation on the definition of \li{$kquery} (not shown) determines the type that the expansion must have. We will see an example of a TLM definition in the next section. Moreover, each spliced segment in the segmentation also comes with a type annotation. %This information is usually not necessary to reason about typing, but it can be conveyed to the programmer upon request by the program editor if desired. %TLM definitions follow the usual scoping rules, so it is easy to ``jump to the definition'' of \li{$kquery}.
\end{enumerate}

%\footnote{In general, spliced expressions might themselves apply TLMs, in which case the convention is to use a distinct color for unspliced segments at each depth. For example, if strings were not primitive, we might write \li{`SURL<body>\{EURLheading ($str "SSTRWorld!ESTR")SURL\} ...</body>EURL`}.}
\begin{figure*}[t]

\begin{subfigure}[t]{0.48\textwidth}
\begin{lstlisting}
  type html_attrs = list(string, string)
  type html = BodyElement(html_attrs, list(html))
            | H1Element(html_attrs, list(html))
            | TextNode(string) 
            | /* ... */;
\end{lstlisting}
\vspace{-5px}
\caption{The \li{html} type, which classifies encodings of HTML data.}
\label{fig:html-type-def}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
\begin{lstlisting}[mathescape=|]
  syntax $html at html by static {
    fun(b : body) : parse_result(proto_expr) => 
      try (parse_html b |> html_to_ast |> Success) {
      | InvalidHTML msg loc => ParseError msg loc }
  };
\end{lstlisting}
\vspace{-5px}
\caption{The \li{$html} TLM definition. Figure \ref{fig:candidate-exp-Reason} defines   TLM-related types.}
\label{fig:html-tlm-def}
\end{subfigure}

\begin{subfigure}[t]{\textwidth}
\begin{lstlisting}
  let heading first_name = $html [|SURL<h1>Hello, {[EURLfirst_nameSURL]}!</h1>EURL|]
  let body = $html [|SURL<body>{EURLheading "World!"SURL} ...</body>EURL|]
\end{lstlisting}
\vspace{-5px}
\caption{Two examples of the \li{$html} TLM being applied. Compare to Ur/Web's built in HTML literal notation from Figure \ref{fig:urweb}.  %In each case, literal body, between backticks, is initially left unparsed according to the language's context-free syntax. %The applied TLM determines a segmentation and expansion during the typed expansion phase, which generalizes the usual typing phase.
}
\label{fig:first-tsm-example}
\end{subfigure}
\vspace{-8px}
\caption{Case Study: HTML literals}
\vspace{-5px}
\end{figure*}


\paragraph{Outline} 
% \begin{itemize}
% \item 
We begin in Sec.~\ref{sec:setsms} with a more detailed case study: a TLM that implements the HTML literal notation shown in Figure \ref{fig:urweb}. We use this case study as a running example to informally describe how TLMs operate, from start to finish.
% \item 
Sec.~\ref{sec:sptsms} then introduces {pattern TLMs}, i.e. TLMs that generate patterns, rather than expressions. %The hygiene condition for pattern TLMs is interesting.
% \item 
Sec.~\ref{sec:ptsms} adds type functions and an ML-style module system and introduces \emph{parametric TLMs} (pTLMs), i.e. TLMs that can take type and module parameters. This section is organized around a second case study: finite map literals. 
Sec.~\ref{sec:static-eval} considers the topic of term evaluation during the  expansion phase, i.e. \emph{static evaluation}, in  more detail. Sec \ref{sec:static-eval} also gives examples of TLMs that are useful for defining other TLMs, e.g. TLMs that implement parser generators and quasiquotation, and uses these in a third case study of the mechanism: IPv4 address literals. 
Sec.~\ref{sec:setsms-formally} defines a type-theoretic calculus of simple expression and pattern TLMs and formally establishes the reasoning principles implied above. 
% \item 
Sec.~\ref{sec:existing-approaches} compares TLMs to related work using the rubric of five reasoning principles just discussed. 
% \item 
Sec.~\ref{sec:discussion} concludes with a summary of the specific contributions of this paper and a discussion of the limitations of TLMs, together with suggestions for future work.

% Parameters serve two important purposes. First, they allow a TLM to operate not just at a single type, but over a type- and module-parameterized family of types. We give as an example a parametric TLM for string-keyed dictionaries that operates uniformly over different implementations of the abstract dictionary signature, and also over different choices of the range type. Second, parameters lower the syntactic cost of the strict context independence discipline that TLMs enforce because they give expansions access to terms other than those explicitly spliced in to the literal body by the client, and because %This is particularly convenient for working with abstract types \emph{a la} ML. 
% partial parameter application is possible. % Sec. \ref{sec:ptsms} culminates with a calculus that extends the calculus of Sec. \ref{sec:setsms-formally} with type functions, ML-style modules and parametric TLMs.
% \item
% \item 
% \end{itemize}


Due to the space limitations, certain technical details and extensions to the calculi described in this paper, as well as detailed proofs, are given in the supplemental material. Some notes on implementation and a fourth case study, involving an encoding of regular expressions as an abstract data type, are also both given in the supplement. 

%  We say more about LISP macros in Sec. \ref{sec:existing-approaches}.% For the present purposes, we can consider Reason as essentially the calculus defined in the supplement extended with various conveniences that are commonly found in other ML-like languages and, notionally, orthogonal to TLMs. %Reason is, as its name suggests, a conceptual descendent of ML. It diverges from other dialects of ML that have a similar type structure in that it has a bidirectional type system \cite{Pierce:2000:LTI:345099.345100} (like, for example, Scala \cite{OdeZenZen01}) for reasons that have to do with the mechanism of TLM implicits described in Chapters \ref{chap:tsls} and \ref{chap:ptsms}. 
%The reason we will not follow Standard ML \cite{mthm97-for-dart} in giving a complete formal definition of Reason in this work is both to emphasize that the primitives we introduce are ``insensitive'' to the details of the underlying type structure of the language (so TLMs can be considered for inclusion in a variety of languages, not only dialects of ML), and to avoid distracting the reader (and the author) with definitions that are already well-understood in the literature and that are orthogonal to those that are the focus of this work. 
% We will not formally define these features mainly to avoid unnecessarily complicating our presentation with details that are not essential to the ideas presented herein. As such, 
% All examples written in Reason should be understood to be informal motivating material for the subsequent formal material. %We anticipate that future full-scale language specifications will be able to combine the ideas  in the proposed work without trouble. %The purpose of the work being proposed is to serve as a reference for those interested in the new constructs we introduce, not to serve as a language specification. 
%We will give a brief overview of these languages are organized in Sec. \ref{sec:Reason}.




% \subsection{Contributions}\label{sec:contributions}
% This work introduces a system of \textbf{typed literal macros (TLMs)} that gives library providers substantially more syntactic control than existing typed term-rewriting macro systems while maintaining the ability to reason abstractly about types, binding and segmentation.% abstract reasoning principles. % comparable to the level of control they have when defining a syntax dialect.

% Clients apply TLMs to \emph{generalized literal forms}. 

% Because the context-free syntax is never extended, syntactic conflicts are not a concern.

%As such, the semantics can take the type and binding structure of the surrounding program into account when validating the expansion that the TLM programmatically generates to ensure that clients can answer critical questions related to types and binding, like those enumerated in Section \ref{sec:abs-reasoning-intro}. Clients need not have knowledge of the implementation of the TLM or of the generated expansion, i.e. there are useful principles of syntactic abstraction.

% The primary technical challenge has to do with the fact that the applied TLM needs to be able to splice terms out of the literal body for inclusion in the expansion. For example, 

 % We design our mechanism such that these locations can easily be determined from the output of the TLM. This is essential for our hygiene mechanism, and it is also useful in that this information can be presented to the user (e.g. as shown in Figure \ref{fig:first-tsm-example-marked}). %As such, we must develop a mechanism where 1) the positions of spliced subterms can be determined without examining the macro implementation (e.g. so that they can be presented to the user differently by an editor or pretty-printer, ;  and 2) the hygiene mechanism must give only portions of the expansion that correspond to these spliced subterms access to the application site context. 



%In order to reason about types and binding, client programmers need only have knowledge of 1) the segmentation (e.g. by examining a figure like this presented by a code editor or pretty-printer) and 2) a type annotation on the definition of the applied TLM. No other details about the applied TLM's implementation or the expansion that it generates need to be revealed to the client programmer. In other words, 


% \begin{figure}[h]
% \begin{lstlisting}
% PElement Nil Seq(
%   TextNode "Hello, ", 
%   Seq(TextNode (join(" ", Cons(first, Cons(second, Nil)))), 
%   TextNode "!"))
% \end{lstlisting}
% \caption{The desugaring.}
% \end{figure}


% There is also no ambiguity with regard to which TLM has control over each form, and searching for the definition of a TLM is no more difficult than searching for any other binding, i.e. there are well-defined scoping rules.

% In other words, TLMs maintain a useful notion of syntactic abstraction. %More specifically, TLMs maintain a \emph{hygienic binding discipline}, meaning that questions Questions 4 and 5 above were concerned with are disallowed entirely. 
% We will, of course, make this notion more technically precise as we continue.


% \begin{figure}[h]
% \begin{lstlisting}[numbers=none,xleftmargin=0px]
% let syntax $strlist = $list string in 
% $html `SURL<p>Hello, {[EURLjoin ($str ' ') ($strlist [firstSURL,EURL last])SURL]}</p>EURL`
% \end{lstlisting} 
% \caption{The example from Figure \ref{fig:first-tsm-example-marked}, expressed using a parametric TLM.}
% \label{fig:first-ptsm-example-marked}
% \end{figure}

% In Secs. \ref{chap:uetsms} and \ref{chap:uptsms}, we assume for the sake of technical simplicity that each TLM definition is self-contained, needing no access to libraries or to other TLMs. This is an impractical assumption in practice. We relax this assumption in In 

% %\item \textbf{Type-specific languages}, or \textbf{TSLs}. TSLs, described 
% In Chapter \ref{chap:tsls}, we develop a mechanism of \emph{TLM implicits} that allows library clients to contextually designate, for any type, a privileged TLM at that type. The semantics applies this privileged TLM implicitly to unadorned literal forms that appear where a term of the associated type is expected. For example, if we designate 
% %\li{$str} 
% as the privileged TLM at the \li{string} type and 
% %\li{$strlist}
% as the privileged TLM at the \li{list(string)} type, we can express the example from Figure \ref{fig:first-tsm-example-marked} instead as shown in Figure \ref{fig:first-tsm-example-implicit} (assuming \li{join} has type \li{string -> list(string) -> string}.) 
% \begin{figure}[h]
% \begin{lstlisting}[numbers=none]
% $html`SURL<p>Hello, {[EURLjoin ' ' [firstSURL,EURL last]SURL]}</p>EURL`
% \end{lstlisting}
% \caption{The example from Figure \ref{fig:first-tsm-example-marked} drawn to take advantage of TLM implicits.}
% \label{fig:first-tsm-example-implicit}
% \end{figure}

% \noindent This approach is competitive in cost with library-specific syntax dialects (e.g. compare Figure \ref{fig:first-tsm-example-implicit} to Figure \ref{fig:urweb}), while maintaining the abstract reasoning principles characteristic of our approach.
%\item \textbf{Metamodules}, introduced in Sec. \ref{sec:metamodules}, reduce the need to primitively build in the type structure of constructs like records (and variants thereof),  labeled sums and other interesting constructs that we will introduce later by giving library providers programmatic ``hooks'' directly into the semantics, which are specified as a \emph{type-directed translation semantics} targeting a small \emph{typed internal language} (introduced in Sec. \ref{sec:Reason}). %For example, a library provider can implement the type structure of records with a metamodule that:
%\begin{enumerate}
%\item introduces a type constructor, \lstinline{record}, parameterized by finite mappings from labels to types, and defines, programmatically, a translation to unary and binary product types (which are built in to the internal language); and 
%\item introduces operators used to work with records, minimally record introduction and elimination (but perhaps also various functional update operators), and directly implements the logic governing their typechecking and translation to the IL (which builds in only nullary and binary products). 
%\end{enumerate}
%We will see direct analogies between ML-style modules (which our mechanisms also support) and metamodules later.
%\end{enumerate} 


% As vehicles for this work, we will define a small programming language in each of the three parts just mentioned, each building conceptually upon the previous language. All of our formal contributions are relative to these small languages.


%TLMs, like other macro systems, perform \emph{static code generation} (also sometimes called \emph{static} or \emph{compile-time metaprogramming}), meaning that the relevant rules in the static semantics of the language call for the evaluation of \emph{static functions} that generate term encodings. Static functions are functions that are evaluated statically, i.e. during typing. %Library providers write these static functions using the Reason \emph{static language} (SL).  
%Maintaining a separation between the static (or ``compile-time'') phase and the dynamic (or ``run-time'') phase is an important facet of Reason's design. % static code generation. %We will  also introduce a simple variant of each of these primitives that leverages Reason's support for local type inference to further reduce syntactic cost in certain common situations. 

\vspace{-4px}
\newcommand{\seTLMsSec}{Expression TLMs}
\section{\protect\seTLMsSec}
\label{sec:setsms}
\vspace{-2px}


Let us begin by detailing the TLM mechanism by way of a case study: HTML literal notation for expressions of the \li{html} type defined in Figure \ref{fig:html-type-def}.



\vspace{-4px}
\subsection{TLM Definition and Application}
\vspace{-2px}
The definition of the \li{$html} TLM is given in Figure \ref{fig:html-tlm-def}. TLM names are prefixed by \li{$} 
 to distinguish them from variables. Every TLM definition also provides a \emph{type annotation}, here \li{at html}, and a \emph{parse function} between \li{by static \{} and \li{\}}. % The TLM definition follows the same scoping rules as type definitions. We will describe how TLM definitions are packaged into libraries in Sec \ref{sec:static-eval}.



Using the TLM defined in Figure \ref{fig:html-tlm-def}, we can express the example Ur/Web program from Figure \ref{fig:urweb} as shown in Figure \ref{fig:first-tsm-example}. On both lines, we apply 
\li{$html} 
 to a \emph{generalized literal form} delimited by \li{[|} and \li{|]}. Generalized literal forms, which first arose in related work by \citet{TSLs} that we will detail in Sec. \ref{sec:existing-approaches}, subsume other literal forms because the context-free syntax of the language only specifies the outer delimiters. In this paper, we will use  \li{[|} and \li{|]}, but \citet{TSLs} formally specified several other choices, including layout-sensitive delimitation. \emph{Literal bodies}, i.e. the characters between these delimiters, are constrained only in that occurrences of \li{[|} must be balanced by \li{|]}. %In particular, there is no pre-specified syntactic to splice out sub-terms.%The initial context-free parser therefore parses generalized literal forms in much the same way as parsers typically parse ``raw'' string literals (i.e. string literals where escape sequences like \li{\\n} have not yet been processed.) 




 The system delegates responsibility over the parsing and expansion of each literal body to the applied TLM's parse function during a semantic phase called \emph{typed expansion}, which generalizes the typing phase. Because the parse function is applied during this phase, rather than at run-time, we call it a \emph{static function}. Static functions  cannot refer to the surrounding variable bindings (because those variables stand for run-time values.) Instead, there is a separate class of \emph{static bindings}. We will return to static bindings in Sec. \ref{sec:static-eval}.

 The parse function must have the following type: 
 \begin{lstlisting}[numbers=none]
      body -> parse_result(proto_expr)
 \end{lstlisting}
 The input type, \li{body}, classifies encodings of literal {bodies}. Literal bodies are sequences of characters, so we define \li{body} as a synonym of \li{string}, as shown in \autoref{fig:indexrange-and-parseresult}. The return type is a sum type, defined by applying the type function \li{parse_result} defined in \autoref{fig:indexrange-and-parseresult}, that distinguishes between parse errors and successful parses.\footnote{We might alternatively have used an exception to signal a parse error, but sum types are formally simpler.} Let us consider these two possibilities in turn.

 If the parse function determines that the literal body is not well-formed according to whatever syntax definition that it implements, it must return \li{ParseError \{msg=}$e_\text{msg}$\li{, loc=}$e_\text{loc}$\li{\}} 
where $e_\text{msg}$ is a custom error message and $e_\text{loc}$ is a value of type \li{segment}, defined in Figure \ref{fig:indexrange-and-parseresult}, that designates a segment of the literal body as the origin of the error \cite{DBLP:journals/jsc/DeursenKT93}. This information is for use in reporting the problem to the programmer.

If instead parsing succeeds, the parse function returns \li{Success} ~$\ecand$, 
where $\ecand$ is called the \emph{encoding of the proto-expansion}. For expression TLMs, the proto-expansion is a \emph{proto-expression} and it is encoded as a value of the recursive datatype \li{proto_expr} that is outlined in Figure \ref{fig:candidate-exp-Reason}. 
Most of the constructors of \li{proto_expr} are individually uninteresting -- they encode OCaml's various expression forms. 
Expressions can mention types, so we also need the type \li{proto_typ} also outlined in Figure \ref{fig:candidate-exp-Reason}. It is only the \li{SplicedE} and \li{SplicedT} constructors that are novel. These are discussed next.



% \section{Simple Expression TLMs By Example}\label{sec:tsms-by-example}
% We begin in this section with a ``tutorial-style'' introduction to seTLMs in Reason. %In particular, we will define an seTLM for constructing values of the recursive labeled sum type \li{rx} that was defined in Figure \ref{fig:datatype-rx}. 
% Sec. \ref{sec:tsms-minimal-formalism} then formally defines a reduced dialect of Reason called $\miniVerseUE$. This will serve as a ``conceptually minimal'' core calculus of TLMs, in the style of the simply typed lambda calculus.   %We conclude in Sec. \ref{sec:uetsms-discussion} 


% \subsection{TLM Application}\label{sec:uetsms-usage}
% The following Reason expression, drawn textually, is of \emph{TLM application} form. Here, a TLM named \li{$rx} is applied to the \emph{generalized literal form} \li{/SURLA|T|G|CEURL/}:
% \begin{lstlisting}[numbers=none,mathescape=|]
% $rx /SURLA|T|G|CEURL/
% \end{lstlisting}
% Generalized literal forms are left unparsed according to the context-free syntax of Reason. Several other outer delimiters are also available, as summarized in Figure \ref{fig:literal-forms}. The client is free to choose any of these for use with any TLM, as long as the \emph{literal body} (shown in green above) satisfies the requirements stated in Figure \ref{fig:literal-forms}. For example, we could have equivalently written the example above as \li{$rx `SURLA|T|G|CEURL`}. (In fact, this would have been convenient if we had wanted to express a regex containing forward slashes but not backticks.) 

% It is only during the subsequent \emph{typed expansion} phase that the applied TLM parses the {body} of the literal form to generate a \emph{proto-expansion}. The language then \emph{validates} this proto-expansion according to criteria that we will describe in Sec. \ref{sec:uetsms-validation}. If proto-expansion validation succeeds, the language generates the \emph{final expansion} (or more concisely, simply the \emph{expansion}) of the TLM application. The behavior of the program is determined by its expansion. 

% For example, the expansion of the TLM application above is equivalent to the following expression when the regex value constructors \li{Or} and \li{Str} are in scope:
% \begin{lstlisting}[numbers=none]
% Or(Str "SSTRAESTR", Or(Str "SSTRTESTR", Or(Str "SSTRGESTR", Str "SSTRCESTR")))
% \end{lstlisting}
% To avoid the assumption that the variables \li{Or} and \li{Str} are in scope at the TLM application site, the expansion actually uses the explicit \li{fold} and \li{inj} operators, as described in Sec. \ref{sec:lists}. In fact, the proto-expansion validation process enforces this notion of context independence -- we will return to proto-expansion validation below. (We will show how TLM parameters can reduce the awkwardness of this requirement in Chapter \ref{chap:ptsms}.)
% %The constructors above are those of the type \li{Rx} that was defined in Figure \ref{fig:datatype-rx}.

% % A number of literal forms, ,  are available in Reason's concrete syntax. Any literal form can be used with any TLM,  TLMs have access only to the literal bodies. Because TLMs do not extend the concrete syntax of the language directly, there cannot be syntactic conflicts between TLMs.

%  %The form does not directly determine the expansion. 

% \begin{figure}
% \begin{lstlisting}
% 'SURLbody cannot contain an apostropheEURL'
% `SURLbody cannot contain a backtickEURL`
% [SURLbody cannot contain unmatched square bracketsEURL]
% {|SURLbody cannot contain unmatched barred curly bracesEURL|}
% /SURLbody cannot contain a forward slashEURL/
% \SURLbody cannot contain a backslashEURL\
% \end{lstlisting}
% %SURL<tag>body includes enclosing tags</tag>EURL
% \caption[Available Generalized Literal Forms]{Generalized literal forms available for use in Reason's textual syntax. The characters in green indicate the literal bodies and describe how the literal body is constrained by the form shown on that line. The Wyvern language defines additional forms, including whitespace-delimited forms \cite{TSLs} and multipart forms \cite{sac15}, but for simplicity we leave these out of Reason.}
% \label{fig:literal-forms}
% \end{figure}

\vspace{-4px}
\subsection{Splicing}\label{sec:splicing-and-hygiene}
\vspace{-2px}
When the parse function determines that some segment of the literal body should be taken as a spliced expression, according to whatever syntactic criteria it deems suitable, it must not directly insert the syntax tree of that expression into the encoding of the expansion. Instead, 
the TLM must indirectly refer to the spliced expression using the \li{SplicedE} variant of \li{proto_expr}, which takes a value of type \li{segment} that indicates the zero-indexed location of the spliced expression relative to the start of the provided literal body. The \li{SplicedE} variant also requires a value of type \li{proto_typ}, which indicates the type that the spliced expression is expected to have. Types can be spliced out by using the \li{SplicedT} variant of \li{proto_typ} analagously.

\begin{figure}
\vspace{-2px}
\begin{lstlisting}[numbers=none,xleftmargin=0pt]
type body = string;
type segment = {startIdx: int, endIdx: int};
type parse_result('a) 
  = ParseError {msg: string, loc: segment}
  | Success('a);
type var_t = string;
type proto_typ = TyVar(var_t)
               | Arrow(proto_typ, proto_typ)
               | StringTy
               | /* ... */ 
               | SplicedT(segment);
type proto_expr = Var(var_t)
                | Fn(var_t, proto_typ, proto_expr)
                | Ap(proto_expr, proto_expr)
                | /* ... */ 
                | SplicedE(segment, proto_typ);
\end{lstlisting}
\vspace{-10px}
\caption[Definitions of various types used by TLM definitions.]{Definitions of various types available ambiently to TLM definitions.}
\label{fig:indexrange-and-parseresult}
\label{fig:candidate-exp-Reason}
\vspace{-12px}
\end{figure}

For example, consider again the TLM application on Line 1 of Figure \ref{fig:first-tsm-example}:
\li{$html [|SURL<h1>Hello, {[EURLfirst_nameSURL]}</h1>EURL|]}. The proto-expansion generated by \li{$html}, if written in a textual syntax for proto-expressions where references to spliced expressions are written \li{spliced<startIdx; endIndex; ty>}, is:
\begin{lstlisting}[numbers=none]
  H1Element(Nil, Cons(TextNode "Hello, ", Cons(
                      TextNode spliced<13; 22; string>, Nil)))
\end{lstlisting}
The proto-expression \li{spliced<13; 22; string>} is a reference to the spliced string expression \li{first_name} by its location relative to the start of the literal body being expanded. It corresponds to the following encoding:
\li{SplicedE (\{startIdx: 13, endIdx: 22\}, StringTy)}.

Requiring that TLMs refer to spliced expressions indirectly in this manner ensures that a TLM cannot ``forge'' spliced terms (i.e. claim that some sub-term of the expansion should be given the privileges of a spliced term, discussed below, when it does not in fact appear in the literal body.)% Our semantics distinguishes spliced expressions when validating the proto-expansion, which is important for reasons of hygiene that we will detail shortly. 


% The parse function can similarly extract \emph{spliced types} from a literal body using the \li{SplicedT} variant of \li{proto_typ}. %In particular, the parse function must provide the index range of spliced subexpressions to the \li{Spliced} constructor of the type \li{MarkedExp}. %Only subexpressions that actually appear in the body of the literal form can be marked as spliced subexpressions.

%For example, had the  would not be a valid expansion, because the  that are not inside spliced subexpressions:
%\begin{lstlisting}[numbers=none]
%Q.Seq(Q.Str(name), Q.Seq(Q.Str ": ", ssn))
%\end{lstlisting}


% \subsection{Splice Summaries and Segmentations}
The \emph{segmentation} inferred from a proto-expansion is the finite set of references to spliced terms contained within. For example, the segmentation inferred from the proto-expression above is the finite set containing only \li{spliced<13; 22; string>}. % Notice that no information about  the spliced terms appear is communicated by the splice summary.
The semantics checks that all of the locations in the segmentation are 1) in bounds relative to the literal body; and 2) non-overlapping. 
This resolves the problem of \textbf{Segmentation} described in Sec. \ref{sec:intro}, i.e. every literal body in a well-typed program has a well-defined segmentation. A program editor or pretty-printer can communicate this location information to the programmer, e.g. by coloring non-spliced segments green as is our convention in this document. In general, spliced expressions might themselves apply TLMs, in which case the convention is to use a distinct color for unspliced segments at each depth. For example, if strings were not primitive but rather defined as sequences of characters, we might define a TLM \li{$str} to recover string literal notation and write \li{$html [|SURL<body>\{EURLheading ($str [|SSTRWorld!ESTR|])SURL\} ...</body>EURL|]}.

A program editor or pretty-printer can also communicate the type of each spliced expression, as specified by the segmentation, to the programmer upon request (the editor packages for working with Reason defer to the Merlin tool when the programmer requests the type of an expression.) %When there is a type mismatch, the type annotation on the spliced segment also allows the error message to report the problem without revealing the full expansion.

\vspace{-6px}
\subsection{Proto-Expansion Validation}\label{sec:uetsms-validation}
Three important concerns described in Sec. \ref{sec:intro} remain: those related to reasoning abstractly about \textbf{Capture}, \textbf{Context Dependence} and \textbf{Typing}. Addressing these concerns is the purpose of the \emph{proto-expansion validation} process, which occurs once a proto-expansion has been generated by the parse function. 
Proto-expansion validation results in the \emph{final expansion}, which is simply the proto-expansion with the references to spliced segments replaced with their own final expansions. 


\subsubsection{Capture}
Proto-expansion validation ensures that spliced terms have access \emph{only} to the bindings at the application site -- spliced terms cannot capture the bindings in the proto-expansion. For example, consider the following application site:
\begin{lstlisting}[numbers=none]
  let tmp = /* ... application site temporary ... */;
  $html [|SURL<h1>{EURLf(tmp)SURL}</h1>EURL|];
\end{lstlisting}
Now consider the scenario where the proto-expansion generated by \li{$html} has the following form:
\begin{lstlisting}[numbers=none]
  let tmp = /* ... expansion-internal temporary ... */;
  H1Element(tmp, spliced<5; 10; html>);
\end{lstlisting}
Na\"ively, the binding of the variable \li{tmp} in the proto-expansion could shadow the application-site binding of \li{tmp} in the final expansion.

To address this problem, splicing is guaranteed to be capture-avoiding. When generating the final expansion, the system discharges the requirement that capture not occur by implicitly alpha-varying the bindings in the proto-expansion as needed. For example, the final expansion of the example above might take the following form:
\begin{lstlisting}[numbers=none]
  let tmp = /* ... application site temporary ... */; 
  let tmp' = /* ... expansion-internal temporary ... */;
  H1Element(tmp', f(tmp));
\end{lstlisting}
Notice that the expansion-internal binding of \li{tmp} has been alpha-varied to \li{tmp'}. The reference to \li{tmp} in the spliced expression then refers, as intended, to the application site binding. 

For TLM providers, the benefit of this mechanism is that they can name the variables used internally within expansions freely. There is no need to explicitly deploy a mechanism that generates ``fresh variables''. TLM clients can, in turn, can reason abstractly about capture, i.e. without examining the expansion that the spliced term appears within.

This does prevent library providers from intentionally introducing bindings into spliced terms. For example, Haskell's literal notation for monadic commands, i.e. \li{do}-notation,  supports binding the result of executing a command to a variable that is then available in the subsequent commands in the command sequence. Using TLMs in Reason, this notation cannot be expressed -- values can be communicated from the expansion to a spliced expression only via function arguments. It would be possible to distinguish spliced identifiers, much as we do spliced terms, and explicitly make them available to spliced expressions by extending the \li{SplicedE} and \li{SplicedT} constructors to take finite sets of spliced identifiers (following the basic approach taken by \citet{DBLP:conf/esop/HermanW08}.) However, the opinion of the authors is that this would unjustifiably increase the reasoning burden on clients when they encounter an unfamiliar literal form: \emph{might this new literal form be doing obscure things with binding too?} Haskell-style infix notation for monadic commands, where bind is \li{e >>= f}, can be expressed using TLMs and splicing.
%We will show an alternative formulation of Haskell's syntax for monadic commands that uses Reason's anonymous function syntax to bind variables in Sec. \ref{sec:application-monadic-commands}. 

\vspace{-6px}
\subsubsection{Context Dependence}\label{sec:context-dependence}
%The prohibition on shadowing ensures only that variables that appear in spliced terms do not refer to bindings that appear in the surrounding expansion. 
The proto-expansion validation process also ensures that variables that appear in the proto-expansion do not refer to bindings that appear either at the TLM definition or the application site. In other words, expansions must be completely \emph{context independent} -- they can make no assumptions about the surrounding context whatsoever. 
A minimal example of a ``broken'' TLM that never generates context-independent proto-expansions is below:
\begin{lstlisting}[numbers=none]
  syntax $broken at rx by static { fun(_) => Success (Var "SSTRxESTR") };
\end{lstlisting}
The proto-expansion that this TLM generates (for any given literal body) refers to a variable \li{x} that it does not itself bind. If proto-expansion validation permitted such a proto-expansion, it would be well-typed only under those application site typing contexts where \li{x} is bound. This ``hidden assumption'' makes reasoning about binding and renaming difficult, so this proto-expansion is deemed invalid (even when \li{$broken} is applied where \li{x} is coincidentally bound.)

Of course, this prohibition does not extend into the spliced terms in a proto-expansion -- spliced terms appear at the application site, so they can justifiably refer to application site bindings. Indeed, we saw examples of spliced terms that referred to variables bound at the application site above. Because proto-expansions refer to spliced terms indirectly, enforcing context independence is straightforward -- we need only that the proto-expansion itself be closed.% In the next section, we will formalize this intuition. % The TLM provider can only refer to them opaquely.

One subtlety is that we assumed in the examples above that constructors like \li{H1Element} were available to the proto-expansion. In OCaml and other dialects of ML where datatype constructors are injected into the context as variables when the datatype is defined, this would violate context independence. As such, we will have to assume for now and in our formalism in Sec. \ref{sec:setsms-formally} that explicit injections into labeled sum types are available. Once we introduce module parameters in Sec. \ref{sec:ptsms}, we will be able to pass in a module exporting the datatype constructors as a parameter and partially apply that parameter to hide this detail from clients.

% This prohibition on context dependence explains why the expansion generated by the TLM application in Sec. \ref{sec:uetsms-usage} cannot make use of the regex value constructors, e.g. \li{Str} and \li{Or}, directly. (In Chapter \ref{chap:ptsms}, we will relax this restriction to allow proto-expansions to access explicit parameters.)

% Collectively, we refer to the prohibition on capture and the prohibition on context dependence as \emph{hygiene properties}, by conceptual analogy to corresponding properties in term-rewriting macro systems (see Sec. \ref{sec:macro-systems}.) The novelty here comes from the fact that spliced terms are being extracted from an initially unparsed sequence of characters.
% In the examples in Sec. \ref{sec:uetsms-usage} and Sec. \ref{sec:splicing-and-hygiene}, the expansion used constructors associated with the \li{Rx} type, e.g. \li{Seq} and \li{Str}. This might appear to violate our prohibition on context-dependent expansions. This is not the case only because in Reason, constructor labels are not variables or scoped symbols. Syntactically, they must begin with a capital letter (like Haskell's datatype constructors). Different labeled sum types can use common constructor labels without conflict because the type the term is being checked against -- e.g. \li{Rx}, due to the type ascription on \li{$rx} -- determines which type of value will be constructed. For dialects of ML where datatype definitions do introduce new variables or scoped symbols, we need parameterized TLMs. We will return to this topic in Chapter \ref{chap:ptsms}. % Indeed, we used the label \li{Spliced} for two different recursive labeled sum types in Figure \ref{fig:candidate-exp-Reason}.

\subsubsection{Typing}
Finally, proto-expansion validation maintains a reasonable \emph{typing discipline} by:
\begin{enumerate}[noitemsep,nolistsep]
\item checking that the expansion is of the type specified by the TLM's type annotation; %For example, the type annotation on \li{$rx} is \li{at rx}, so proto-expansion validation ensures that the final expansion is of type \li{rx}.
\item checking that each spliced type is valid; 
\item checking that the type annotation on each spliced expression is valid; and 
\item checking each spliced expression against the specified type annotation.
\end{enumerate}
%The OCaml type system is not strong enough to allow us to express only contextually well-typed syntax trees. In The details are discussed as future work in Sec. \ref{sec:discussion}.
 % This addresses the problem of reasoning abstractly about \textbf{Typing} described in Sec. \ref{sec:intro}, i.e.:
 % \begin{enumerate}
 %   \item determining the type of an expansion requires examining only the type annotation on the TLM definition (much as determining the type of a function application requires examining only the function's type); and 
 % \item determining the type that a spliced expression must have requires only the information in the splice summary (rather than complete knowledge of the proto-expansion).
 % \end{enumerate}

% The language \emph{validates} proto-expansions before a final expansion is generated. One aspect of proto-expansion validation is checking  the proto-expansion against the type annotation specified by the TLM, e.g. the type \li{Rx} in the example above. This maintains a \emph{type discipline}: if a programmer sees a TLM being applied when examining a well-typed program, they need only look up the TLM's type annotation to determine the type of the generated expansion. Determining the type does not require examining the expansion directly.


% \subsection{Hygiene}
% The spliced subexpressions that the proto-expansion refers to (by their position within the literal body, cf. above) must be parsed, typed and expanded during the proto-expansion validation process (otherwise, the language would not be able to check the type of the proto-expansion). To maintain a useful \emph{binding discipline}, i.e. to allow programmers to reason also about variable binding without examining expansions directly, the validation process maintains two additional properties related to spliced subexpressions: \textbf{context independent expansion} and \textbf{expansion independent splicing}. These are collectively referred to as the \emph{hygiene properties} (because they are conceptually related to the concept of hygiene in term rewriting macro systems, cf. Sec. \ref{sec:term-rewriting}.) 

% \paragraph{Context Independent Expansion} 

% \paragraph{Expansion Independent Splicing} 
% %These properties suffice to ensure that programmers and tools can freely rename a variable without changing the meaning of the program. The only information that is necessary to perform such a \emph{rename refactoring} is the locations of spliced subexpressions within all the literal forms for which the variable being renamed is in scope; the expansions need not otherwise be examined. It would be straightforward to develop a tool and/or editor plugin to indicate the locations of spliced subexpressions to the user, like we do in this document (by coloring spliced subexpressions black). We discuss tool support as future work in Sec. \ref{sec:interaction-with-tools}.

% \subsubsection{Final Expansion}

% For example, the final expansion of the body of \li{lookup_rx} is equivalent to the following, under an environment where the regex value constructors are available:
% \begin{lstlisting}[numbers=none]
% Seq(Str(name), Seq(Str "SSTR: ESTR", ssn))
% \end{lstlisting}
% (Again, due to the prohibition on context dependent expansions, the final expansion actually involves explicit \li{fold} and \li{inj} operators.)


\vspace{-4px}
\newcommand{\spTLMsSec}{Simple Pattern TLMs (spTLMs)}
\section{\protect\spTLMsSec}
\label{sec:sptsms}
\vspace{-2px}

Let us now briefly consider the topic of TLMs that expand to \emph{patterns}, rather than expressions. For example, we can pattern match on a value \li{x : html} by applying a pattern TLM \li{$html} as follows:
\begin{lstlisting}[numbers=none]
  let children_of_h1 x => 
    switch x {
    | $html [|SURL<h1>{EURLcsSURL}</h1>EURL|] -> Some cs; /* cs : list(html) */
    | _ -> None; };
\end{lstlisting}
Any list pattern, including one generated by another TLM application, can appear where \li{cs} appears in the example pattern above. For longer \li{switch} expressions, the shorthand \li{switch x using $html} applies \li{$html} to every rule where the outermost pattern is of generalized literal form.

Pattern TLM definitions look much like expression TLM definitions:
\begin{lstlisting}
  syntax $html at html for patterns by static {
    fun(b : body) : parse_result(proto_pat) => 
      /* ... Ur/Web-style HTML pattern parser here ...  */
  };
\end{lstlisting}
The \emph{sort qualifier}, \li{for patterns}, indicates that this is a pattern TLM definition. Expression TLM definitions can also include a sort qualifier, \li{for expressions}, but this is the default if omitted. 
Notice that we used the same name, \li{$html}, for this pattern TLM as for the expression TLM defined in the previous section. This is possible because patterns and expressions are distinct sorts of terms. It does not make sense to apply an expression TLM in pattern position, and \emph{vice versa}.\footnote{The fact that some patterns look like expressions in the textual syntax is immaterial to this fundamental semantic distinction. Many expression-level constructs, e.g. lambdas, do not correspond even syntactically to patterns.}  
The return type of the parse function is \li{parse_result(proto_pat)}, rather than \li{parse_result(proto_expr)}. The type \li{proto_pat}, outlined in Figure \ref{fig:CEPat}, classifies encodings of \emph{proto-patterns}.

\begin{figure}[h]
\vspace{-10px}
\begin{lstlisting}[numbers=none]
  type proto_pat = /* IMPORTANT: no variable pattern form! */
                 | Wild
                 | /* ... other standard pattern forms ... */
                 | SplicedP(segment, proto_typ);
\end{lstlisting}
\vspace{-10px}
\caption[Abbreviated definition of \li{proto_pat} in Reason]{Abbreviated definition of \li{proto_pat}.}
\label{fig:CEPat}
\vspace{-10px}
\end{figure}

% \subsection{Splicing}
The constructor \li{SplicedP} serves much like \li{SplicedE} to allow a proto-pattern to refer indirectly to spliced pattern segments by their location within the literal body. For example, the proto-pattern generated for the example at the top of this section would be written concretely as follows:
\begin{lstlisting}[numbers=none]
  H1Element (_, spliced<6; 7; list(html)>)
\end{lstlisting}


% \subsection{Proto-Pattern Validation}
% Proto-pattern validation serves, like proto-expression validation, to maintain the ability to reason abstractly about binding and typing. 

To maintain a reasonable abstract binding discipline, i.e. to allow clients to reason about variable binding without examining pattern TLM expansions directly, variable patterns can appear only within spliced patterns. Enforcing this restriction is straightforward: we simply have not defined a variant of the \li{proto_pat} type that encodes variable patterns (though wildcards are allowed.)  This prohibition ensures that no variables other than those visible to the client in a spliced pattern are captured by the corresponding branch of the match expression.

Patterns have no way to refer to surrounding bindings (they only induce bindings in other expressions, e.g. in the corresponding branch of the \li{switch} expression.) However, type annotations on references to spliced patterns could refer to type variables, so we also need to enforce context independence in the manner discussed in the previous section. % In languages like OCaml, which support arbitrary boolean guard expressions, we would also need to enforce both context independence and capture avoidance as discussed in the previous section for spliced expressions that end up in a guard expression. In our formalism, we do not support guard expressions.

% \subsubsection{Typing} 
To maintain a reasonable abstract typing discipline, proto-pattern validation checks:
\begin{enumerate}[noitemsep,nolistsep]
  \item that the final expansion is a pattern that matches values of the type specified by the TLM's type annotation;
  \item that each spliced pattern matches values of the type indicated in the segmentation; and 
  \item that each of these types are themselves well-formed types.
\end{enumerate}

\vspace{-4px}
\newcommand{\seTLMsFormallySec}{Simple TLMs, Formally}
\section{\protect\seTLMsFormallySec}
\label{sec:setsms-formally}
\vspace{-2px}

Before continuing on to consider more general parametric TLMs, let us develop a calculus of simple expression and pattern TLMs called $\miniVersePat$. This calculus consists of an \emph{unexpanded language}, or \emph{UL}, defined by typed expansion to an \emph{expanded language}, or \emph{XL}. Figs. \ref{fig:U-unexpanded-terms}  and \ref{fig:U-expanded-terms} summarize the syntax of the UL and  the XL, respectively. Programs are written as unexpanded expressions but evaluate as well-typed expanded expressions. We will start with a brief overview of our XL before turning in the remainder of the section on the UL and the typed expansion process.

\vspace{-4px}
\subsection{Expanded Language (XL)}\label{sec:s-XL}
\vspace{-2px}
The {XL} of $\miniVersePat$ forms a standard pure functional language with partial function types, quantification over types, recursive types, labeled product types and labeled sum types. 
 The reader is directed to \emph{PFPL} \cite{pfple1} for a detailed introductory account of these standard constructs. We will only tersely summarize the statics and dynamics of the XL below because the particularities of the XL are not critical to the ideas we will introduce below.

% \subsubsection{Statics of the Expanded Language}
The \emph{statics of the XL} is defined by hypothetical judgements of the following form:
\[\begin{array}{ll}
% \textbf{Judgement Form} & \textbf{Description}\\
\istypeU{\Delta}{\tau} & \text{$\tau$ is a well-formed type}\\
%\isctxU{\Delta}{\Gamma} & \text{$\Gamma$ is a well-formed typing context assuming $\Delta$}\\
\hastypeU{\Delta}{\Gamma}{e}{\tau} & \text{$e$ is assigned type $\tau$}\\
\ruleType{\Delta}{\Gamma}{r}{\tau}{\tau'} & \text{$r$ takes values of type $\tau$ to values of type $\tau'$}\\
\patType{\pctx}{p}{\tau} & \text{$p$ matches values of type $\tau$ and generates hypotheses $\pctx$} 
\end{array}\]
\emph{Type formation contexts}, $\Delta$, are finite sets of hypotheses of the form $\Dhyp{t}$. %Empty finite sets are written $\emptyset$, or omitted entirely within judgements, and non-empty finite sets are written as comma-separated finite sequences identified up to exchange and contraction. We write $\Delta, \Dhyp{t}$ when $\Dhyp{t} \notin \Delta$ for $\Delta$ extended with the hypothesis $\Dhyp{t}$. %Finite sets are written as finite sequences identified up to exchange.% We write $\Dcons{\Delta}{\Delta'}$ for the union of $\Delta$ and $\Delta'$.
\emph{Typing contexts}, $\Gamma$, are finite functions that map each variable $x \in \domof{\Gamma}$, where $\domof{\Gamma}$ is a finite set of variables, to the hypothesis $\Ghyp{x}{\tau}$, for some $\tau$. The judgements above are inductively defined in the supplemental material and validate standard lemmas, also given in the supplement.
%Empty typing contexts are written $\emptyset$, or omitted entirely within judgements, and non-empty typing contexts are written as finite sequences of hypotheses identified up to exchange and contraction. We write $\Gamma, \Ghyp{x}{\tau}$, when $x \notin \domof{\Gamma}$, for the extension of $\Gamma$ with a mapping from $x$ to $\Ghyp{x}{\tau}$, and $\Gcons{\Gamma}{\Gamma'}$ when $\domof{\Gamma} \cap \domof{\Gamma'} = \emptyset$ for the typing context mapping each $x \in \domof{\Gamma} \cup \domof{\Gamma'}$ to $x : \tau$ if $x : \tau \in \Gamma$ or $x : \tau \in \Gamma'$. % We write $\isctxU{\Delta}{\Gamma}$ if every type in $\Gamma$ is well-formed relative to $\Delta$.
% \begin{definition}[Typing Context Formation] \label{def:isctxU}
% $\isctxU{\Delta}{\Gamma}$ iff for each hypothesis $x : \tau \in \Gamma$, we have $\istypeU{\Delta}{\tau}$.
% \end{definition}

% \begin{subequations}\label{rules:istypeU}
% \begin{equation*}\label{rule:istypeU-var}
% \inferrule{ }{\istypeU{\Delta, \Dhyp{t}}{t}}
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-parr}
% \inferrule{
%   \istypeU{\Delta}{\tau_1}\\
%   \istypeU{\Delta}{\tau_2}
% }{\istypeU{\Delta}{\aparr{\tau_1}{\tau_2}}}
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-all}
%   \inferrule{
%     \istypeU{\Delta, \Dhyp{t}}{\tau}
%   }{
%     \istypeU{\Delta}{\aall{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-rec}
%   \inferrule{
%     \istypeU{\Delta, \Dhyp{t}}{\tau}
%   }{
%     \istypeU{\Delta}{\arec{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-prod}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}
%   }{
%     \istypeU{\Delta}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:istypeU-sum}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}
%   }{
%     \istypeU{\Delta}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \end{subequations}
% Premises of the form $\{{J}_i\}_{i \in \labelset}$ mean that for each $i \in \labelset$, the judgement ${J}_i$ must hold. 


\begin{figure}[t!]
\begin{minipage}{\textwidth}
\small
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{UTyp} & \utau & ::= & 
\ut ~\vert~ 
\parr{\utau}{\utau} ~\vert~
\forallt{\ut}{\utau} ~\vert~
\rect{\ut}{\utau} ~\vert~
\prodt{\mapschema{\utau}{i}{\labelset}} ~\vert~
\sumt{\mapschema{\utau}{i}{\labelset}}\\
\mathsf{UExp} & \ue & ::= & 
\ux ~\vert~
% \asc{\ue}{\utau} ~\vert~
% \letsyn{\ux}{\ue}{\ue} & \text{value binding}\\
\lam{\ux}{\utau}{\ue} ~\vert~
\ap{\ue}{\ue} ~\vert~
\Lam{\ut}{\ue} ~\vert~
\App{\ue}{\utau} ~\vert~
\fold{\ue} ~\vert~
% \unfold{\ue} ~\vert~
\tpl{\mapschema{\ue}{i}{\labelset}} ~\vert~
\inj{\ell}{\ue} ~\vert~
\matchwith{\ue}{\seqschemaX{\urv}}
 \\
& & \vert & \uesyntaxq{\tsmv}{\utau}{e}{\ue} ~\vert~ \utsmap{\tsmv}{b} \\   
& & \vert & \usyntaxup{\tsmv}{\utau}{e}{\ue} \\
\mathsf{URule} & \urv & ::= & 
%& \aumatchrule{\upv}{\ue} 
\matchrule{\upv}{\ue} \\
\mathsf{UPat} & \upv & ::= & 
%& \ux 
\ux ~\vert~
\wildp ~\vert~ 
\foldp{\upv} ~\vert~
\tplp{\mapschema{\upv}{i}{\labelset}} ~\vert~
\injp{\ell}{\upv} ~\vert~
\utsmap{\tsmv}{b}

% \LCC  &  & 
% %& \lightgray 
% & \color{Yellow} & \color{Yellow} \\
% &&
% %& \audefuetsm{\utau}{e}{\tsmv}{\ue} 
% & \uesyntax{\tsmv}{\utau}{e}{\ue} & \text{seTLM definition}\\ 
% &&
% %& \autsmap{b}{\tsmv} 
% & \utsmap{\tsmv}{b} & \text{seTLM application}\ECC
\end{array}$
\end{minipage}
\vspace{-10px}
\caption[Syntax of the $\miniVersePat$ unexpanded language (UL)]{Syntax of the $\miniVersePat$ unexpanded language (UL). Metavariable $\ut$ ranges over type identifiers, $\ux$ over expression identifiers, $\ell$ over labels, $\labelset$ over finite sets of labels, $\tsmv$ over TLM names and $b$ over literal bodies. We write $\mapschema{\utau}{i}{\labelset}$ for a finite mapping of each label $i$ in $\labelset$ to some unexpanded type $\utau_i$, and similarly for other sorts. We write $\seqschemaX{\urv}$ for a finite sequence of $n$ unexpanded rules.
}
\vspace{-6px}
\label{fig:U-unexpanded-terms}
\end{figure}
\begin{figure}
%\hspace{-5px}
\begin{minipage}{\textwidth}
\small
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{Typ} & \tau & ::= & t ~\vert~ \aparr{\tau}{\tau} ~\vert~ \aall{t}{\tau} ~\vert~ \arec{t}{\tau} ~\vert~ \aprod{\labelset}{\mapschema{\tau}{i}{\labelset}} ~\vert~ \asum{\labelset}{\mapschema{\tau}{i}{\labelset}}\\
\mathsf{Exp} & e & ::= & x ~\vert~ \aelam{\tau}{x}{e} ~\vert~ \aeap{e}{e} ~\vert~ \aetlam{t}{e} ~\vert~ \aetap{e}{\tau} ~\vert~ \aefold{e} ~\vert~ \aetpl{\labelset}{\mapschema{e}{i}{\labelset}}  \\
& & \vert & \aein{\ell}{e} ~\vert~ \aematchwith{n}{e}{\seqschemaX{r}}\\
\mathsf{Rule} & r & ::= & \aematchrule{p}{e}\\
\mathsf{Pat} & p & ::= & x  ~\vert~ \aewildp ~\vert~ \aefoldp{p} ~\vert~ \aetplp{\labelset}{\mapschema{p}{i}{\labelset}} ~\vert~ \aeinjp{\ell}{p}
\end{array}$
\end{minipage}
\vspace{-10px}
\caption[Syntax of the XL of $\miniVersePat$]{Syntax of the $\miniVersePat$ expanded language (XL). XL terms are \emph{abstract binding trees} (ABTs) identified up to alpha-equivalence, so we follow the syntactic conventions of \citet{pfple1}. Metavariable $x$ ranges over variables and $t$ over type variables.  %When using stylized forms, the label set is omitted when it can be inferred, e.g. the labeled product type $\prodt{\finmap{\mapitem{\ell_1}{e_1}, \mapitem{\ell_2}{e_2}}}$ leaves the label set $\{\ell_1, \ell_2\}$ implicit. 
% When we use the stylized forms, we assume that the reader can infer suppressed indices and arguments from the surrounding context.
}
\vspace{-4px}
\label{fig:U-expanded-terms}
\end{figure}
% \begin{subequations}\label{rules:hastypeU}
% \begin{equation*}\label{rule:hastypeU-var}
%   \inferrule{ }{
%     \hastypeU{\Delta}{\Gamma, \Ghyp{x}{\tau}}{x}{\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-lam}
%   \inferrule{
%     \istypeU{\Delta}{\tau}\\
%     \hastypeU{\Delta}{\Gamma, \Ghyp{x}{\tau}}{e}{\tau'}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-ap}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e_1}{\aparr{\tau}{\tau'}}\\
%     \hastypeU{\Delta}{\Gamma}{e_2}{\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aeap{e_1}{e_2}}{\tau'}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-tlam}
%   \inferrule{
%     \hastypeU{\Delta, \Dhyp{t}}{\Gamma}{e}{\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aetlam{t}{e}}{\aall{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-tap}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\aall{t}{\tau}}\\
%     \istypeU{\Delta}{\tau'}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aetap{e}{\tau'}}{[\tau'/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-fold}
%   \inferrule{\
%     \istypeU{\Delta, \Dhyp{t}}{\tau}\\
%     \hastypeU{\Delta}{\Gamma}{e}{[\arec{t}{\tau}/t]\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aefold{e}}{\arec{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-unfold}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\arec{t}{\tau}}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aeunfold{e}}{[\arec{t}{\tau}/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-tpl}
%   \inferrule{
%     \{\hastypeU{\Delta}{\Gamma}{e_i}{\tau_i}\}_{i \in \labelset}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aetpl{\labelset}{\mapschema{e}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-pr}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\aprod{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \ell \hookrightarrow \tau}}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aepr{\ell}{e}}{\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-in}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}\\
%     \istypeU{\Delta}{\tau}\\
%     \hastypeU{\Delta}{\Gamma}{e}{\tau}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aein{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \ell \hookrightarrow \tau}{e}}{\asum{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \ell \hookrightarrow \tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:hastypeU-case}
%   \inferrule{
%     \hastypeU{\Delta}{\Gamma}{e}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}\\
%     \istypeU{\Delta}{\tau}\\
%     \{\hastypeU{\Delta}{\Gamma, x_i : \tau_i}{e_i}{\tau}\}_{i \in \labelset}
%   }{
%     \hastypeU{\Delta}{\Gamma}{\aecase{\labelset}{e}{\mapschemab{x}{e}{i}{\labelset}}}{\tau}
%   }
% \end{equation*}
% \end{subequations}

%Rules (\ref{rules:istypeU}) and (\ref{rules:hastypeU}) are syntax-directed, so we assume an inversion lemma for each rule as needed without stating it separately. 
% The following standard lemmas also hold. 

% \vspace{-4px}
% \subsubsection{Evaluation Semantics}\label{sec:dynamics-U}
% \vspace{-2px}
The \emph{evaluation semantics} of $\miniVersePat$ is organized around the judgements $\isvalU{e}$, which says that $e$ is a value, and $\evalU{e}{e'}$, which says that $e$ evaluates to the value $e'$. Additional  judgements, not shown, are needed to define the dynamics of pattern matching, but they do not appear directly in our subsequent developments, so we omit them. We assume an eager dynamics and the standard type safety theorem (see supplement.) 

\vspace{-4px}
\subsection{Syntax of the Unexpanded Language}\label{sec:syntax-U}\label{sec:s-UL}
\vspace{-2px}
Unexpanded types and expressions are simple inductive structures. Unlike expanded types and expressions, they are \textbf{not} abstract binding trees -- we do \textbf{not} define the standard notions of renaming, alpha-equivalence or substitution for unexpanded terms. This is because unexpanded expressions remain ``partially parsed'' due to the presence of literal bodies, $b$, from which spliced terms might be extracted during typed expansion. In fact, unexpanded types and expressions do not involve variables at all, but rather \emph{type identifiers}, $\ut$, and \emph{expression identifiers}, $\ux$. Identifiers are given meaning by expansion to variables during typed expansion, as we will see. This distinction between identifiers and variables is technically crucial to our developments, and we return to it below. %We \textbf{cannot} adopt the usual definitions of $\alpha$-renaming of identifiers, because unexpanded types and expressions are still in a ``partially parsed'' state -- the literal bodies, $b$, within an unexpanded expression might contain spliced subterms that are ``surfaced'' by a TLM only during typed expansion, as we will detail below. %identifiers are given meaning by expansion to variables. %In other words, unexpanded expressions are not abstract binding trees, nor sequences of characters, but a ``transitional'' structure with some characteristics of each of these. 
%For this reason, we will need to handle generating fresh variables explicitly at binding sites in our semantics. %To do so, we distinguish \emph{type identifiers}, $\ut$, and \emph{expression identifiers}, $\ux$, from type variables, $t$, and expression variables, $x$. identifiers will be given meaning by expansion to variables (which, in turn, are given meaning by substitution, as described above). 

% There are only two unexpanded expression forms, highlighted in gray in Figure \ref{fig:U-unexpanded-terms}, that do not correspond to expanded expression forms -- the seTLM definition form and the seTLM application form. %These are the ``interesting'' forms. % These are the ``interesting'' forms. % Let us define this correspondence by the metafunction $\Uof{e}$:
%\[
%\begin{split}
%\Uof{x} & = x\\
%\Uof{\aelam{\tau}{x}{e}} & = \aulam{\tau}{x}{\Uof{e}}\\
%\Uof{\aeap{e_1}{e_2}} & = \auap{\Uof{e_1}}{\Uof{e_2}}
%\end{split}
%\] and so on for the remaining expanded expression forms.
Most of the unexpanded forms in Figure \ref{fig:U-unexpanded-terms}  mirror the expanded forms. We refer to these as the \emph{common forms}. The mapping from expanded forms to common unexpanded forms is defined explicitly in the supplement. 

There is also a corresponding context-free textual syntax for the UL. 
Giving a complete definition of the context-free textual syntax as, e.g., a context-free grammar, is not critical to our purposes here. 
%Our paper on Wyvern defines a textual syntax for a similar system \cite{TSLs}. 
Instead, we need only posit partial metafunctions $\parseUTypF{b}$, $\parseUExpF{b}$ and $\parseUPatF{b}$  that go from character sequences, $b$, to unexpanded types, expressions and patterns (the supplement states the necessary condition in full.) 
% \begingroup
% \def\thetheorem{\ref{condition:textual-representability-SES}}
% \begin{condition}[Textual Representability] ~
% \begin{enumerate}[nolistsep]
% \item For each $\utau$, there exists $b$ such that $\parseUTyp{b}{\utau}$. 
% \item For each $\ue$, there exists $b$ such that $\parseUExp{b}{\ue}$.
% \item For each $\upv$, there exists $b$ such that $\parseUPat{b}{\upv}$.
% \end{enumerate}
% \end{condition}
% \endgroup

\subsection{Typed Expansion}\label{sec:typed-expansion-U}\label{sec:s-TE}
Unexpanded terms are checked and expanded simultaneously according to the central judgements of our calculus, the \emph{typed expansion judgements}:
\vspace{-3px}\[\begin{array}{ll}
% \textbf{Judgement Form} & \textbf{Description}\\
\expandsTU{\uDelta}{\utau}{\tau} & \text{$\utau$ has well-formed expansion $\tau$}\\
\expandsUPX{\ue}{e}{\tau} & \text{$\ue$ has expansion $e$ of type $\tau$}\\
\ruleExpands{\uDelta}{\uGamma}{\uPsi}{\uPhi}{\urv}{r}{\tau}{\tau'} & \text{$\urv$ has expansion $r$ taking values of type $\tau$ to values of type $\tau'$}\\
\patExpands{\upctx}{\uPhi}{\upv}{p}{\tau} & \text{$\upv$ has expansion $p$ matching against $\tau$ generating hypotheses $\upctx$}
\end{array}\]\vspace{-3px}
%\newcommand{\gray}[1]{{\color{gray} #1}}

% These judgements are inductively defined in the supplement. 
% \begingroup 
% \def\thetheorem{\ref{thm:typed-expansion-short-U}}

%These rules validate the following theorem, which establishes that typed expansion produces an expansion of the assigned type. 
%\begin{theorem}[Typed Expression Expansion] If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uPsi}{\ue}{e}{\tau}$ and $\uetsmenv{\Delta}{\uPsi}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.\end{theorem}
%\begin{proof} This is the first part of Theorem \ref{thm:typed-expansion-U}, defined and proven below.\end{proof}


The typed expansion rules that handle common forms mirror the corresponding typing rules. The \emph{expression TLM context}, $\uPsi$, and the \emph{pattern TLM context}, $\uPhi$, pass through these rules opaquely. For example, the rules for variables and lambdas are below, and the remainder are in the supplement:
%Each of these rules is based on the corresponding typing rule, i.e. Rules (\ref{rule:hastypeU-var}) through (\ref{rule:hastypeU-case}), respectively. For example, the following typed expansion rules are based on the typing rules (\ref{rule:hastypeU-var}), (\ref{rule:hastypeU-lam}) and (\ref{rule:hastypeU-ap}), respectively:% for unexpanded expressions of variable, function and application form, respectively: 
{\small\begin{mathpar}
  \inferrule[ee-id]{ }{\expandsUP{\uDelta}{\uGamma, \uGhyp{\ux}{x}{\tau}}{\uPsi}{\uPhi}{\ux}{x}{\tau}}

  \inferrule[ee-lam]{
    \expandsTU{\uDelta}{\utau}{\tau}\\
    \expandsUP{\uDelta}{\uGamma, \uGhyp{\ux}{x}{\tau}}{\uPsi}{\uPhi}{\ue}{e}{\tau'}
  }{\expandsUPX{\lam{\ux}{\utau}{\ue}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}}
\end{mathpar}}

The only subtlety here has to do with the relationship between identifiers, $\ux$, in the UL and variables, $x$, in the XL. To understand this, we must first describe in detail how unexpanded contexts work. \emph{Unexpanded typing contexts}, $\uGamma$, are pairs of the form $\uGG{\uG}{\Gamma}$, where $\uG$ is an \emph{expression identifier expansion context}, and $\Gamma$ is a standard typing context. An expression identifier expansion context, $\uG$, is a finite function that maps each expression identifier $\ux \in \domof{\uG}$ to the hypothesis $\vExpands{\ux}{x}$, for some expression variable, $x$, called its expansion. We write $\ctxUpdate{\uG}{\ux}{x}$ for the expression identifier expansion context that maps $\ux$ to $\vExpands{\ux}{x}$ and defers to $\uG$ for all other expression identifiers (i.e. the previous mapping is \textbf{updated}.) Note the distinction between update and extension (which requires that the new identifier is not already in the domain.) %We write $\uGammaOK{\uGamma}$ when $\uGamma=\uGG{\uG}{\Gamma}$ and each expression variable in $\uG$ is assigned a type by $\Gamma$.
%\begin{definition} $\uGammaOK{\uGG{\uG}{\Gamma}}$ iff for each $\vExpands{\ux}{x} \in \uG$, we have $\Ghyp{x}{\tau} \in \Gamma$ for some $\tau$.\end{definition}
%\noindent 
We define $\uGamma, \uGhyp{\ux}{x}{\tau}$ when $\uGamma = \uGG{\uG}{\Gamma}$ as an abbreviation of $\uGG{\ctxUpdate{\uG}{\ux}{x}}{\Gamma, \Ghyp{x}{\tau}}$.

To develop an intuition for why the update operation is necessary, it is instructive to inspect the derivation of the expansion of the unexpanded expression $\lam{\ux}{\utau}{\lam{\ux}{\utau}{\ux}}$ to $\aelam{\tau}{x_1}{\aelam{\tau}{x_2}{x_2}}$ assuming $\expandsTU{\uDelta}{\utau}{\tau}$:
{\small\begin{mathpar}
\inferrule{
  \inferrule{ }{\expandsTU{\uDelta}{\utau}{\tau}}\\
  \inferrule{
    \inferrule{ }{\expandsTU{\uDelta}{\utau}{\tau}}\\
    \inferrule{ }{
      \expandsUP{\uDelta}{\uGG{\vExpands{\ux}{x_2}}{x_1 : \tau, x_2 : \tau}}{\uPsi}{\uPhi}{\ux}{x_2}{\tau}
    }~\textsc{ee-id}
  }{
    \expandsUP{\uDelta}{\uGG{\vExpands{\ux}{x_1}}{x_1 : \tau}}{\uPsi}{\uPhi}{\lam{\ux}{\utau}{\ux}}{\aelam{\tau}{x_2}{x_2}}{\aparr{\tau}{\tau}}
    % \expandsU{\uDelta}{\uGG}
  }~\textsc{ee-lam}
}{
  \expandsUP{\uDelta}{\uGG{\emptyset}{\emptyset}}{\uPsi}{\uPhi}{\lam{\ux}{\utau}{\lam{\ux}{\utau}{\ux}}}{\aelam{\tau}{x_1}{\aelam{\tau}{x_2}{x_2}}}{\aparr{\tau}{\aparr{\tau}{\tau}}}
}~\textsc{ee-lam}
\end{mathpar}}
\noindent
Notice that when Rule \textsc{ee-lam} is applied, the type identifier expansion context is updated but the typing context is extended with a (necessarily fresh) variable, first $x_1$ then $x_2$. Without this mechanism, expansions for unexpanded terms with shadowing, like this minimal example, would not exist, because we cannot implicitly alpha-vary the unexpanded term to sidestep this problem in the usual manner.

\emph{Unexpanded type formation contexts}, $\uDelta$, consist of a \emph{type identifier expansion context}, $\uD$, paired with a standard type formation context, $\Delta$, and operate analagously (see supplement.)
% of the form $\uDD{\uD}{\Delta}$, i.e. they . We similarly define $\uDelta, \uDhyp{\ut}{t}$ when $\uDelta=\uDD{\uD}{\Delta}$ as an abbreviation of $\uDD{\ctxUpdate{\uD}{\ut}{t}}{\Delta, \Dhyp{t}}$.%type identifier expansion context is always extended/updated together with 


Before we continue, let us state an important invariant: that typed expression expansion produces a well-typed expression.
\begin{theorem}[Typed Expression Expansion]\label{thm:typed-expansion-short-U}
If $\expandsUP{\uDD{\uD}{\Delta}\hspace{-3px}}{\uGG{\uG}{\Gamma}\hspace{-3px}}{\uPsi}{\uPhi}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
\end{theorem}
For the typed expansion rules governing common forms, like the two example rules above, the typed expansion rules mirror the corresponding typing rules so it is easy to see that this invariant holds. The details are in the supplement. The  rules of particular interest are the rules governing TLM definitions and TLM application, which are the topic of the remainder of this section. 
% \endgroup
% \subsubsection{Type Expansion}
% \emph{unexpanded type formation contexts}, $\udelta$, are of the form $\udd{\ud}{\delta}$, i.e. they consist of a \emph{type identifier expansion context}, $\ud$, paired with a standard type formation context, $\delta$. 

% A \emph{type identifier expansion context}, $\uD$, is a finite function that maps each type identifier $\ut \in \domof{\uD}$ to the hypothesis $\vExpands{\ut}{t}$, for some type variable $t$. We write $\ctxUpdate{\uD}{\ut}{t}$ for the type identifier expansion context that maps $\ut$ to $\vExpands{\ut}{t}$ and defers to $\uD$ for all other type identifiers (i.e. the previous mapping is \emph{updated}.) We define $\uDelta, \uDhyp{\ut}{t}$ when $\uDelta=\uDD{\uD}{\Delta}$ as an abbreviation of $\uDD{\ctxUpdate{\uD}{\ut}{t}}{\Delta, \Dhyp{t}}$.%type identifier expansion context is always extended/updated together with 

% The \emph{type expansion judgement}, $\expandsTU{\uDelta}{\utau}{\tau}$, is inductively defined by the rules given in the supplement. The first three of these rules are reproduced below:
% % \begin{subequations}%\label{rules:expandsTU}
% \begin{mathpar}
% \inferrule[te-id]{ }{\expandsTU{\uDelta, \uDhyp{\ut}{t}}{\ut}{t}}

% \inferrule[te-parr]{
%   \expandsTU{\uDelta}{\utau_1}{\tau_1}\\
%   \expandsTU{\uDelta}{\utau_2}{\tau_2}
% }{\expandsTU{\uDelta}{\parr{\utau_1}{\utau_2}}{\aparr{\tau_1}{\tau_2}}}

% \inferrule[te-all]{
%     \expandsTU{\uDelta, \uDhyp{\ut}{t}}{\utau}{\tau}
%   }{
%     \expandsTU{\uDelta}{\forallt{\ut}{\utau}}{\aall{t}{\tau}}
% }
% \end{mathpar}
% % \begin{equation*}\label{rule:expandsTU-rec}
% %   \inferrule{
% %     \expandsTU{\uDelta, \uDhyp{\ut}{t}}{\utau}{\tau}
% %   }{
% %     \expandsTU{\uDelta}{\aurec{\ut}{\utau}}{\arec{t}{\tau}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:expandsTU-prod}
% %   \inferrule{
% %     \{\expandsTU{\uDelta}{\utau_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \expandsTU{\uDelta}{\auprod{\labelset}{\mapschema{\utau}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:expandsTU-sum}
% %   \inferrule{
% %     \{\expandsTU{\uDelta}{\utau_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \expandsTU{\uDelta}{\ausum{\labelset}{\mapschema{\utau}{i}{\labelset}}}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}
% % \end{subequations}
% %We write $\uDeltaOK{\uDelta}$ when $\uDelta=\uDD{\uD}{\Delta}$ and each type variable in $\uD$ also appears in $\Delta$.
% %\begin{definition}\label{def:uDeltaOK} $\uDeltaOK{\uDD{\uD}{\Delta}}$ iff for each $\vExpands{\ut}{t} \in \uD$, we have $\Dhyp{t} \in \Delta$.\end{definition}



% The Type Expansion Lemma establishes that the expansion of an unexpanded type is a well-formed type.

% % \begingroup
% % \def\thetheorem{\ref{lemma:type-expansion-U}}
% \begin{lemma}[Type Expansion] If $\expandsTU{\uDD{\uD}{\Delta}}{\utau}{\tau}$ then $\istypeU{\Delta}{\tau}$.\end{lemma}
% % \begin{proof} By rule induction over Rules (\ref{rules:expandsTU}). In each case, we apply the IH to or over each premise, then apply the corresponding type formation rule in Rules (\ref{rules:istypeU}). \end{proof}
% % \endgroup
% \begin{subequations}\label{rules:expandsU}




% The rules for the remaining expressions of common form are entirely straightforward, mirroring the corresponding typing rules, i.e. Rules (\ref{rules:hastypeU}). %In particular, observe that, in each of these rules, the unexpanded and expanded expression forms in the conclusion correspond, and each premise corresponds to a premise of the corresponding typing rule. %Type formation premises in the typing rule give rise to  type expansion premises in the corresponding typed expansion rule, and each typed expression expansion premise in each rule above corresponds to a typing premise in the corresponding typing rule. 
% The type assigned in the conclusion of each rule above is identical to the type assigned in the conclusion of the corresponding typing rule. The seTLM context, $\uPsi$, passes opaquely through these rules (we will define seTLM contexts below.) As such, the corresponding cases in the proof of Theorem \ref{thm:typed-expansion-short-U} are by application of the induction hypothesis and the  corresponding typing rule. %Rules (\ref{rules:expandsTU}) could similarly have been generated by mechanically transforming Rules (\ref{rules:istypeU}).

% We can express this scheme more precisely with the rule transformation given in Appendix \ref{appendix:SES-uexps}. For each rule in Rules (\ref{rules:istypeU}) and Rules (\ref{rules:hastypeU}),
% \begin{mathpar}
% \refstepcounter{equation}
% % \label{rule:expandsU-tlam}
% % \refstepcounter{equation}
% % \label{rule:expandsU-tap}
% % \refstepcounter{equation}
% \label{rule:expandsU-fold}
% \refstepcounter{equation}
% \label{rule:expandsU-unfold}
% \refstepcounter{equation}
% \label{rule:expandsU-tpl}
% \refstepcounter{equation}
% \label{rule:expandsU-pr}
% \refstepcounter{equation}
% \label{rule:expandsU-in}
% \refstepcounter{equation}
% \label{rule:expandsU-case}
% \inferrule{J_1\\ \cdots \\ J_k}{J}
% \end{mathpar}
% the corresponding typed expansion rule is 
% \begin{mathpar}
% \inferrule{
%   \Uof{J_1} \\
%   \cdots\\
%   \Uof{J_k}
% }{
%   \Uof{J}
% }
% \end{mathpar}
% where
% \[\begin{split}
% \Uof{\istypeU{\Delta}{\tau}} & = \expandsTU{\Uof{\Delta}}{\Uof{\tau}}{\tau} \\
% \Uof{\hastypeU{\Gamma}{\Delta}{e}{\tau}} & = \expandsU{\Uof{\Gamma}}{\Uof{\Delta}}{\uPsi}{\Uof{e}}{e}{\tau}\\
% \Uof{\{J_i\}_{i \in \labelset}} & = \{\Uof{J_i}\}_{i \in \labelset}
% \end{split}\]
% and where:
% \begin{itemize}
% \item $\Uof{\tau}$ is defined as follows:
%   \begin{itemize}
%   \item When $\tau$ is of definite form, $\Uof{\tau}$ is defined as in Sec. \ref{sec:syntax-U}.
%   \item When $\tau$ is of indefinite form, $\Uof{\tau}$ is a uniquely corresponding metavariable of sort $\mathsf{UTyp}$ also of indefinite form. For example, in Rule (\ref{rule:istypeU-parr}), $\tau_1$ and $\tau_2$ are of indefinite form, i.e. they match arbitrary types. The rule transformation simply ``hats'' them, i.e. $\Uof{\tau_1}=\utau_1$ and $\Uof{\tau_2}=\utau_2$.
%   \end{itemize}
% \item $\Uof{e}$ is defined as follows
% \begin{itemize}
% \item When $e$ is of definite form, $\Uof{e}$ is defined as in Sec. \ref{sec:syntax-U}. 
% \item When $e$ is of indefinite form, $\Uof{e}$ is a uniquely corresponding metavariable of sort $\mathsf{UExp}$ also of indefinite form. For example, $\Uof{e_1}=\ue_1$ and $\Uof{e_2}=\ue_2$.
% \end{itemize}
% \item $\Uof{\Delta}$ is defined as follows:
%   \begin{itemize} 
%   \item When $\Delta$ is of definite form, $\Uof{\Delta}$ is defined as above.
%   \item When $\Delta$ is of indefinite form, $\Uof{\Delta}$ is a uniquely corresponding metavariable ranging over unexpanded type formation contexts. For example, $\Uof{\Delta} = \uDelta$.
%   \end{itemize}
% \item $\Uof{\Gamma}$ is defined as follows:
%   \begin{itemize}
%   \item When $\Gamma$ is of definite form, $\Uof{\Gamma}$ produces the corresponding unexpanded typing context as follows:
% \begin{align*}
% \Uof{\emptyset} & = \uGG{\emptyset}{\emptyset}\\
% \Uof{\Gamma, \Ghyp{x}{\tau}} & = \Uof{\Gamma}, \uGhyp{\identifierof{x}}{x}{\tau}
% \end{align*}
%   \item When $\Gamma$ is of indefinite form, $\Uof{\Gamma}$ is a uniquely corresponding metavariable ranging over unexpanded typing contexts. For example, $\Uof{\Gamma} = \uGamma$.
% \end{itemize}
% \end{itemize}

% It is instructive to use this rule transformation to generate Rules (\ref{rules:expandsTU}) and Rules (\ref{rule:expandsU-var}) through (\ref{rule:expandsU-tap}) above. We omit the remaining rules, i.e. Rules (\ref*{rule:expandsU-fold}) through (\ref*{rule:expandsU-case}). By instead defining these rules solely by the rule transformation just described, we avoid having to write down a number of rules that are of limited marginal interest. Moreover, this demonstrates the general technique for generating typed expansion rules for unexpanded types and expressions of common form, so our exposition is somewhat ``robust'' to changes to the inner core. 
%o that when the inner core changes,  typed expansion rules  our exposition somewhat robust to changes to the inner core (though not to changes to the judgement forms in the statics of the inner core).% Even if changes to the judgement forms in the statics of the inner core are needed (e.g. the addition of a symbol context), it is easy to see would correspond to changes in the generic specification above.
% \begin{subequations}\label{rules:expandsU}
% \begin{equation*}\label{rule:expandsU-var}
%   \inferrule{ }{\expandsU{\Delta}{\Gamma, x : \tau}{\uPsi}{x}{x}{\tau}}
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-lam}
%   \inferrule{
%     \istypeU{\Delta}{\tau}\\
%     \expandsU{\Delta}{\Gamma, x : \tau}{\uPsi}{\ue}{e}{\tau'}
%   }{\expandsUX{\aulam{\tau}{x}{\ue}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}}
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-ap}
%   \inferrule{
%     \expandsUX{\ue_1}{e_1}{\aparr{\tau}{\tau'}}\\
%     \expandsUX{\ue_2}{e_2}{\tau}
%   }{
%     \expandsUX{\auap{\ue_1}{\ue_2}}{\aeap{e_1}{e_2}}{\tau'}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tlam}
%   \inferrule{
%     \expandsU{\Delta, \Dhyp{t}}{\Gamma}{\uPsi}{\ue}{e}{\tau}
%   }{
%     \expandsUX{\autlam{t}{\ue}}{\aetlam{t}{e}}{\aall{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tap}
%   \inferrule{
%     \expandsUX{\ue}{e}{\aall{t}{\tau}}\\
%     \istypeU{\Delta}{\tau'}
%   }{
%     \expandsUX{\autap{\ue}{\tau'}}{\aetap{e}{\tau'}}{[\tau'/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-fold}
%   \inferrule{
%     \istypeU{\Delta, \Dhyp{t}}{\tau}\\
%     \expandsUX{\ue}{e}{[\arec{t}{\tau}/t]\tau}
%   }{
%     \expandsUX{\aufold{t}{\tau}{\ue}}{\aefold{e}}{\arec{t}{\tau}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-unfold}
%   \inferrule{
%     \expandsUX{\ue}{e}{\arec{t}{\tau}}
%   }{
%     \expandsUX{\auunfold{\ue}}{\aeunfold{e}}{[\arec{t}{\tau}/t]\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tpl}
%   \inferrule{
%     \{\expandsUX{\ue_i}{e_i}{\tau_i}\}_{i \in \labelset}
%   }{
%     \expandsUX{\autpl{\labelset}{\mapschema{\ue}{i}{\labelset}}}{\aetpl{\labelset}{\mapschema{e}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-pr}
%   \inferrule{
%     \expandsUX{\ue}{e}{\aprod{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}}
%   }{
%     \expandsUX{\aupr{\ell}{\ue}}{\aepr{\ell}{e}}{\tau}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-in}
%   \inferrule{
%     \{\istypeU{\Delta}{\tau_i}\}_{i \in \labelset}\\
%     \istypeU{\Delta}{\tau}\\
%     \expandsUX{\ue}{e}{\tau}
%   }{
%     \left\{\shortstack{$\Delta~\Gamma \vdash_\uPsi \auin{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}{\ue}$\\$\leadsto$\\$\aein{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}{e} : \asum{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}$\vspace{-1.2em}}\right\}
%   }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-case}
%   \inferrule{
%     \expandsUX{\ue}{e}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}\\
%     \{\expandsU{\Delta}{\Gamma, \Ghyp{x_i}{\tau_i}}{\uPsi}{\ue_i}{e_i}{\tau}\}_{i \in \labelset}
%   }{
%     \expandsUX{\aucase{\labelset}{\ue}{\mapschemab{x}{\ue}{i}{\labelset}}}{\aecase{\labelset}{e}{\mapschemab{x}{e}{i}{\labelset}}}{\tau}
%   }
% \end{equation*}
% \end{subequations}


% \begin{equation*}\label{rule:expandsU-syntax}
% \inferrule{
%   \istypeU{\Delta}{\tau}\\
%   \expandsU{\emptyset}{\emptyset}{\emptyset}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultExp}}\\\\
%   a \notin \domof{\uPsi}\\
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\ue}{e}{\tau'}
% }{
%   \expandsUX{\audefuetsm{\tau}{\ueparse}{\tsmv}{\ue}}{e}{\tau'}
% }
% \end{equation*}
% \begin{equation*}\label{rule:expandsU-tsmap}
% \inferrule{
%   \encodeBody{b}{\ebody}\\
%   \evalU{\ap{\eparse}{\ebody}}{\inj{\lbltxt{SuccessE}}{\ecand}}\\
%   \decodeCondE{\ecand}{\ce}\\\\
%   \cvalidE{\emptyset}{\emptyset}{\esceneU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{b}}{\ce}{e}{\tau}
% }{
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\autsmap{b}{\tsmv}}{e}{\tau}
% }
% \end{equation*}
%\end{subequations}

%Notice that each form of expanded expression (Figure \ref{fig:U-expanded-terms}) corresponds to a form of unexpanded expression (Figure \ref{fig:U-unexpanded-terms}). For each typing rule in Rules (\ref{rules:hastypeU}), there is a corresponding typed expansion rule -- Rules (\ref{rule:expandsU-var}) through (\ref{rule:expandsU-case}) -- where the unexpanded and expanded forms correspond. The premises also correspond -- if a typing judgement appears as a premise of a typing rule, then the corresponding premise in the corresponding typed expansion rule is the corresponding typed expansion judgement. The seTLM context is not extended or inspected by these rules (it is only ``threaded through'' them opaquely).

%There are two unexpanded expression forms that do not correspond to an expanded expression form: the seTLM definition form, and the seTLM application form. The rules governing these two forms interact with the seTLM context, and are the topics of the next two subsections, respectively.

\vspace{-4px}
\subsection{TLM Definitions}\label{sec:U-uetsm-definition}\label{sec:s-TLM-def}
\vspace{-2px}
% An unexpanded expression of seTLM definition form, $\uesyntaxq{\tsmv}{\utau}{\eparse}{\ue}$, 
%The operational form corresponding to this stylized form is \[\audefuetsm{\utau}{\eparse}{\tsmv}{\ue}\]
 % defines an {seTLM} identified as $\tsmv$ with \emph{unexpanded type annotation} $\utau$ and \emph{parse function} $\eparse$ for use within $\ue$. 
 The rule below defines typed expansion of the seTLM definition form:
% \begin{subequations}[resume]
% \begin{equation*}\label{rule:expandsU-syntax}
% \inferrule{
%   \istypeU{\Delta}{\tau}\\
%   \expandsU{\emptyset}{\emptyset}{\emptyset}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultExp}}\\\\
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\ue}{e}{\tau'}
% }{
%   \expandsUX{\audefuetsm{\tau}{\ueparse}{\tsmv}{\ue}}{e}{\tau'}
% }
% \end{equation*}
{\small\begin{mathpar}
\inferrule[ee-def-setsm]{
  \expandsTU{\uDelta}{\utau}{\tau}\\
  \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultExp}}\\\\
  \evalU{\eparse}{\eparse'}\\
  \expandsUP{\uDelta}{\uGamma}{\uPsi, \uShyp{\tsmv}{a}{\tau}{\eparse'}}{\uPhi}{\ue}{e}{\tau'}
}{
  \expandsUPX{\uesyntaxq{\tsmv}{\utau}{\eparse}{\ue}}{e}{\tau'}
}
\end{mathpar}}
% \end{subequations}
\vspace{-3px}

The first premise expands the unexpanded type annotation. The second premise checks that $\eparse$ is a closed expanded function\footnote{In Sec. \ref{sec:static-eval}, we add the machinery necessary for parse functions that are neither closed nor yet expanded.} of type $\aparr{\tBody}{\tParseResultExp}$.

The type abbreviated $\tBody$ classifies encodings of literal bodies, $b$. The mapping from literal bodies, $b$, to values of type $\tBody$ is defined by the \emph{body encoding judgement} $\encodeBody{b}{\ebody}$. An inverse mapping can also be defined   by the \emph{body decoding judgement} $\decodeBody{\ebody}{b}$. Rather than defining $\tBody$ explicitly, and these judgements inductively against that definition (which would be tedious and uninteresting), it suffices to take as a condition that there is an isomorphism between literal bodies and values of type $\tBody$ mediated by these judgements (see supplement.)

The return type, $\tParseResultExp$, abbreviates a labeled sum type that distinguishes parse errors from successful parses:
{$
% L_\mathtt{SE} & \defeq \lbltxt{ParseError}, \lbltxt{SuccessE}\\ \asumNL{
  \mapitem{\lbltxt{ParseError}}{\prodt{}}, 
  \mapitem{\lbltxt{SuccessE}}{\tCEExp}
$}.
% \end{align*}} %[\mapitem{\lbltxt{ParseError}}{\prodt{}}, \mapitem{\lbltxt{SuccessE}}{\tCEExp}]
% \] 

The type abbreviated $\tCEExp$ classifies encodings of \emph{proto-expressions}, $\ce$ (pronounced ``grave $e$''.) The syntax of proto-expressions, defined in Figure \ref{fig:U-candidate-terms}, will be described when we describe proto-expansion validation in Sec. \ref{sec:ce-syntax-U}. The mapping from proto-expressions to values of type $\tCEExp$ is defined by the \emph{proto-expression encoding judgement}, $\encodeCondE{\ce}{e}$. An inverse mapping is defined by the \emph{proto-expression decoding judgement}, $\decodeCondE{e}{\ce}$. Again, rather than picking a particular definition of $\tCEExp$ and defining the judgements above inductively against it, we take as a condition that there is an isomorphism between values of type $\tCEExp$ and closed proto-expressions mediated by these judgements (see supplement.)

The third premise of Rule \textsc{ee-def-setsm} evaluates the parse function to a value. This is not semantically necessary, but it is the choice one would expect to make in an eager language.

The final premise of Rule \textsc{ee-def-setsm} extends the expression TLM context, $\uPsi$, with the newly determined {seTLM definition}, and proceeds to assign a type, $\tau'$, and expansion, $e$, to $\ue$. The conclusion of the rule then assigns this type and expansion to the seTLM definition as a whole. % i.e. TLMs define behavior that is relevant during typed expansion, but not during evaluation. 
{Expression TLM contexts}, $\uPsi$, are of the form $\uAS{\uA}{\Psi}$, where $\uA$ is a \emph{TLM identifier expansion context} and $\Psi$ is an \emph{expression TLM definition context}. 

A {TLM identifier expansion context}, $\uA$, is a finite function mapping each TLM identifier $\tsmv \in \domof{\uA}$ to the \emph{TLM identifier expansion}, $\vExpands{\tsmv}{a}$, for some \emph{TLM name}, $a$. We distinguish TLM identifiers, $\tsmv$, from TLM names, $a$, for much the same reason that we distinguish type and expression identifiers from type and expression variables: in order to allow a TLM definition to shadow a previously defined TLM definition without relying on an implicit identification convention.

An {expression TLM definition context}, $\Psi$, is a finite function mapping each TLM name $a \in \domof{\Psi}$ to an \emph{expanded seTLM definition}, $\xuetsmbnd{a}{\tau}{\eparse}$, where $\tau$ is the seTLM's type annotation, and $\eparse$ is its parse function. 
We define $\uPsi, \uShyp{\tsmv}{a}{\tau}{\eparse}$, when $\uPsi=\uAS{\uA}{\Psi}$, as an abbreviation of $\uAS{\ctxUpdate{\uA}{\tsmv}{a}}{\Psi, \xuetsmbnd{a}{\tau}{\eparse}}$.

%Moreover, this distinction will be crucial in the semantics of TLM abbreviations in Chapter \ref{chap:ptsms}. 

% \end{enumerate}


The spTLM definition form operates analagously. The rule is reproduced below, and the details, which mirror those for seTLMs, are in the supplement. Notice that the spTLM context, $\uPhi$, rather than the $\uPsi$ is updated. This allows expression and pattern TLMs to use the same identifiers.
{\vspace{-3px}\small\begin{mathpar}
\inferrule[ee-def-sptsm]{
  \expandsTU{\uDelta}{\utau}{\tau}\\
  \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultPat}}\\\\
  \evalU{\eparse}{\eparse'}\\
  \expandsUP{\uDelta}{\uGamma}{\uPsi}{\uPhi, \uPhyp{\tsmv}{a}{\tau}{\eparse'}}{\ue}{e}{\tau'}
}{
  \expandsUPX{\usyntaxup{\tsmv}{\utau}{\eparse}{\ue}}{e}{\tau'}
}
\end{mathpar}}
\vspace{-3px}

% \[\begin{array}{ll}
% \textbf{Judgement Form} & \textbf{Description}\\
% \uetsmenv{\Delta}{\uPsi} & \text{$\uPsi$ is well-formed assuming $\Delta$}\end{array}\]
% This judgement is inductively defined by the following rules:
% \begin{subequations}[intermezzo]\label{rules:uetsmenv-U}
% \begin{equation*}\label{rule:uetsmenv-empty}
% \inferrule{ }{\uetsmenv{\Delta}{\emptyset}}
% \end{equation*}
% \begin{equation*}\label{rule:uetsmenv-ext}
% \inferrule{
%   \uetsmenv{\Delta}{\uPsi}\\
%   \istypeU{\Delta}{\tau}\\
%   \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultExp}}
% }{
%   \uetsmenv{\Delta}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}
% }
% \end{equation*}
% \end{subequations}

\vspace{-4px}
\subsection{TLM Application}\label{sec:U-uetsm-application}\label{sec:s-TLM-ap}
\vspace{-2px}
The unexpanded expression form for applying an seTLM named $\tsmv$ to a literal form with literal body $b$ is $\utsmap{\tsmv}{b}$. 
% This form uses forward slashes\todo{use backticks} to delimit the literal body, but other generalized literal forms could also be included as derived forms in the textual syntax. % (we omit them for simplicity).
%The corresponding operational form is $\autsmap{b}{\tsmv}$. %i.e. for each literal body $b$, the operator $\texttt{uapuetsm}[b]$ is indexed by the TLM name $\tsmv$ and takes no arguments. %\footnote{This is in following the conventions in \emph{PFPL} \cite{pfpl}, where operators parameters allow for the use of metatheoretic objects that are not syntax trees or binding trees, e.g. $\mathsf{str}[s]$ and $\mathsf{num}[n]$.} This operator is indexed by the TLM name $\tsmv$ and takes no arguments. 
The typed expansion rule governing seTLM application is below:
% \begin{subequations}[resume]
% \begin{equation*}\label{rule:expandsU-tsmap}
% \inferrule{
%   \encodeBody{b}{\ebody}\\
%   \evalU{\ap{\eparse}{\ebody}}{\inj{\lbltxt{SuccessE}}{\ecand}}\\
%   \decodeCondE{\ecand}{\ce}\\\\
%   \cvalidE{\emptyset}{\emptyset}{\esceneU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{b}}{\ce}{e}{\tau}
% }{
%   \expandsU{\Delta}{\Gamma}{\uPsi, \xuetsmbnd{\tsmv}{\tau}{\eparse}}{\autsmap{b}{\tsmv}}{e}{\tau}
% }
% \end{equation*}
{\small\begin{mathpar}
\inferrule[ee-ap-setsm]{
  \encodeBody{b}{\ebody}\\
  \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{\ecand}}\\
  \decodeCondE{\ecand}{\ce}\\\\
  \segOK{\segof{\ce}}{b}\\
  \cvalidE{\emptyset}{\emptyset}{\esceneUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{b}}{\ce}{e}{\tau}
}{
  \expandsUP{\uDelta}{\uGamma}{\uPsi', \uShyp{\tsmv}{a}{\tau}{\eparse}}{\uPhi}{\utsmap{\tsmv}{b}}{e}{\tau}
}
\end{mathpar}}

The first premise determines the encoding of the literal body, $\ebody$, which, as discussed above, is a value of type $\tBody$.

The second premise applies the parse function $\eparse$ to the encoding of the literal body. If parsing succeeds, i.e. a value of the form $\aein{\mathtt{SuccessE}}{\ecand}$ results from evaluation, then $\ecand$ will be a value of type $\tCEExp$ (assuming a well-formed expression TLM context, by application of the Type Safety assumption.) We call $\ecand$ the \emph{encoding of the proto-expansion}. If the parse function produces a value labeled $\lbltxt{ParseError}$, then typed expansion fails and formally, no rule is necessary.

The third premise decodes the encoding of the proto-expansion.

The fourth premise determines the segmentation of the proto-expansion, $\segof{\ce}$, and ensures that it is valid with respect to $b$ via the predicate $\segOK{\psi}{b}$, which  checks that each segment in the finite set of segments $\psi$ has non-negative length and is within bounds of $b$, and that the segments in $\psi$ do not overlap.

The final premise \emph{validates} the proto-expansion and simultaneously generates the \emph{final expansion}, $e$, which appears in the conclusion of the rule. The proto-expression validation judgement is defined in the next subsection.

The typed pattern expansion rule governing spTLM application is analagous:
{\small\begin{mathpar}
\inferrule[pe-ap-sptsm]{
  \encodeBody{b}{\ebody}\\
  \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessP}}{\ecand}}\\
  \decodeCEPat{\ecand}{\cpv}\\\\
  \segOK{\segof{\cpv}}{b}\\
  \cvalidP{\upctx}{\pscene{\uDelta}{\uPhi}{b}}{\cpv}{p}{\tau}
}{
  \patExpands{\upctx}{\uPhi', \uPhyp{\tsmv}{a}{\tau}{\eparse}}{\utsmap{\tsmv}{b}}{p}{\tau}
}
\end{mathpar}}

% \subsection{Syntax of Proto-Expansions}\label{sec:ce-syntax-U}

\vspace{-4px}
\subsection{Proto-Expansion Validation}\label{sec:ce-validation-U}\label{sec:ce-syntax-U}\label{sec:s-PEV}
\vspace{-2px}


\begin{figure}
\vspace{-10px}
\begin{minipage}{\textwidth}
\small
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{PrTyp} & \ctau & ::= & t ~\vert~
\aceparr{\ctau}{\ctau} ~\vert~
\aceall{t}{\ctau} ~\vert~
\acerec{t}{\ctau} ~\vert~
\aceprod{\labelset}{\mapschema{\ctau}{i}{\labelset}} \\
& & \vert & \acesum{\labelset}{\mapschema{\ctau}{i}{\labelset}} ~\vert~ \acesplicedt{m}{n}\\
\mathsf{PrExp} & \ce & ::= & x ~\vert~
% \aceasc{\ctau}{\ce} ~\vert~
% \aceletsyn{x}{\ce}{\ce} ~\vert~
\acelam{\ctau}{x}{\ce} ~\vert~
\aceap{\ce}{\ce} ~\vert~
\acetlam{t}{\ce} ~\vert~
\acetap{\ce}{\ctau} ~\vert~
\acefold{\ce} \\
& & \vert & \acetpl{\labelset}{\mapschema{\ce}{i}{\labelset}} ~\vert~
\acein{\ell}{\ce} ~\vert~
\acematchwith{n}{\ce}{\seqschemaX{\crv}} ~\vert~
\acesplicede{m}{n}{\ctau}\\
\mathsf{PrRule} & \crv & ::= & \acematchrule{p}{\ce}\\
\mathsf{PrPat} & \cpv & ::= & \acewildp ~\vert~
\acefoldp{p} ~\vert~
\acetplp{\labelset}{\mapschema{\cpv}{i}{\labelset}} ~\vert~
\aceinjp{\ell}{\cpv} ~\vert~
\acesplicedp{m}{n}{\ctau} 
\end{array}$
\end{minipage}
\vspace{-5px}
\caption[Syntax of $\miniVersePat$ proto-types and proto-expressions]{Syntax of $\miniVersePat$ proto-expansions. Proto-expansion terms are ABTs identified up to alpha-equivalence.}
\label{fig:U-candidate-terms}
\vspace{-10px}
\end{figure}

Finally, the \emph{proto-expansion validation judgements} validate the proto-expansions generated by TLMs and simultaneously generate their final expansions:% are types and expanded expressions, respectively.
\[\begin{array}{ll}
\cvalidT{\Delta}{\tscenev}{\ctau}{\tau} & \text{$\ctau$ has well-formed expansion $\tau$}\\
\cvalidE{\Delta}{\Gamma}{\escenev}{\ce}{e}{\tau} & \text{$\ce$ has expansion $e$ of type $\tau$}\\
\cvalidR{\Delta}{\Gamma}{\escenev}{\crv}{r}{\tau}{\tau'} & \text{$\crv$ has expansion $r$ taking values of type $\tau$ to values of type $\tau'$}\\
\cvalidP{\upctx}{\pscenev}{\cpv}{p}{\tau} & \text{$\cpv$ has expansion $p$ matching against $\tau$ generating assumptions $\upctx$}
\end{array}\]
The purpose of the \emph{splicing scenes} $\tscenev$, $\escenev$ and $\pscenev$ is to ``remember'' the contexts and literal body from the TLM application site (cf. Rules \textsc{ee-ap-setsm} and \textsc{pe-ap-sptsm}) for when validation encounters spliced terms. \emph{Type splicing scenes}, $\tscenev$, are of the form $\tsceneUP{\uDelta}{b}$. \emph{Expression splicing scenes}, $\escenev$, are of the form $\esceneUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{b}$. \emph{Pattern splicing scenes}, $\pscenev$, are of the form $\pscene{\uDelta}{\uPhi}{b}$. 

\subsubsection{Common Forms} Most of the proto-expansion forms mirror corresponding expanded forms. The rules governing proto-expansion validation for these common forms correspondingly mirror the typing rules. They are given in the supplement. Splicing scenes pass opaquely through these rules, i.e. none of these rules can access the application site contexts. This maintains context independence (defined formally below.)

Notice that proto-rules, $\crv$, involve expanded patterns, $p$, not proto-patterns, $\cpv$. The reason is that proto-rules appear in proto-expressions, which are generated by expression TLMs. Proto-patterns, in contrast, arise only from pattern TLMs. There is not a variable proto-pattern form.

\subsubsection{References to Spliced Terms} The only interesting forms are the references to spliced unexpanded types, expressions and patterns. Let us first consider the rule for references to spliced unexpanded types:
{\small\begin{mathpar}
  \inferrule[ptv-spliced]{
    \parseUTyp{\bsubseq{b}{m}{n}}{\utau}\\
    \expandsTU{\uDD{\uD}{\Delta_\text{app}}}{\utau}{\tau}\\
    \Delta \cap \Delta_\text{app} = \emptyset
  }{
    \cvalidT{\Delta}{\tsceneU{\uDD{\uD}{\Delta_\text{app}}}{b}}{\acesplicedt{m}{n}}{\tau}
  }
\end{mathpar}}

\noindent
This first premise of this rule parses out the requested segment of the literal body, $b$, to produce an unexpanded type, $\utau$. It then invokes type expansion to ensure that this unexpanded type is well-formed \emph{under the application site context}, $\uDD{\uD}{\Delta_\text{app}}$, but \emph{not} the expansion-local type formation context, $\Delta$.  The final premise requires that the application site type formation context is disjoint from the expansion-local type formation context. Because proto-expansions are ABTs identified up to alpha-equivalence, we can always discharge the final premise by alpha-varying the proto-expansion. Collectively, this ensures that type variable capture does not occur (we will formally state this property in the next section.) 

The rule for references to spliced unexpanded expressions is fundamentally analagous:
{\small\begin{mathpar}
\inferrule[pev-spliced]{
  \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\
  \cvalidT{\emptyset}{\tsceneUP{\uDelta}{b}}{\ctau}{\tau}\\
  \expandsUP{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{\uPhi}{\ue}{e}{\tau}\\\\
  \Delta \cap \Delta_\text{app} = \emptyset\\
  \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset
}{
  \cvalidE{\Delta}{\Gamma}{\esceneUP{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{\uPhi}{b}}{\acesplicede{m}{n}{\ctau}}{e}{\tau}
}
\end{mathpar}}

\noindent
We first splice out the requested segment. The second premise expands the type annotation under an empty context, because the type annotation must be meaningful at the application site (so, independent of $\Delta$ and $\Gamma$) and not itself make any assumptions about the application site context. Spliced types can appear in the annotation. The third premise performs typed expansion of the spliced unexpanded expression under the application site contexts, but not the expansion-local contexts. The final two premises ensure that these contexts are disjoint, again to force capture avoidance.

The rule for references to spliced unexpanded patterns is entirely analagous:
{\small\begin{mathpar}
\inferrule[ppv-spliced]{
  \parseUPat{\bsubseq{b}{m}{n}}{\upv}\\
  \cvalidT{\emptyset}{\tsceneUP{\uDelta}{b}}{\ctau}{\tau}\\
  \patExpands{\upctx}{\uPhi}{\upv}{p}{\tau}
}{
  \cvalidP{\upctx}{\pscene{\uDelta}{\uPhi}{b}}{\acesplicedp{m}{n}{\ctau}}{p}{\tau}
}
\end{mathpar}}

\vspace{-4px}
\subsection{Metatheory}\label{sec:s-metatheory}
\vspace{-2px}
\subsubsection{Typed Expansion} Let us now return to Theorem \ref{thm:typed-expansion-short-U}, the typed expression expansion theorem that was mentioned at the end of Sec. \ref{sec:typed-expansion-U}. As it turns out, in order to prove this theorem, we must  prove the following stronger theorem, because the proto-expression validation judgement is defined mutually inductively with the typed expansion judgement (due to the three rules just described.)

% \begingroup
% \def\thetheorem{\ref{thm:typed-expansion-full-U}}
\begin{theorem}[Typed Expression Expansion (Strong)] ~
\begin{enumerate}[nolistsep]
\item If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uAS{\uA}{\Psi}}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
\item If $\cvalidE{\Delta}{\Gamma}{\esceneU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uAS{\uA}{\Psi}}{b}}{\ce}{e}{\tau}$ and $\Delta \cap \Delta_\text{app} = \emptyset$ and $\domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset$ then $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$.
\end{enumerate}
\end{theorem}
% \endgroup
The additional second clause simply states that the final expansion produced by proto-expression validation is well-typed under the combined application site and expansion-internal context (because spliced terms are distinguished only in the proto-expansion, not in the final expansion.) Such combined contexts can only be formed if the constituents are disjoint.

The proof proceeds by mutual rule induction and appeal to simple lemmas about type expansion and proto-type validation (see supplement). The proof is straightforward but for one issue: it is not immediately clear that the mutual induction is well-founded, because the case in the proof of part 2 for Rule \textsc{pev-spliced} invokes part 1 of the induction hypothesis on a term that is not a sub-term of the conclusion, but rather parsed out of the literal body, $b$. To establish that the mutual induction is well-founded, then, we need to explicitly establish a decreasing metric. The intuition is that parsing a term of out a literal body cannot produce a bigger term than the term that contained that very literal body. More specifically, {the sum of the lengths of the literal bodies that appear in the term strictly decreases each time you perform a nested TLM application} because some portion of the term has to be consumed by the TLM name and the delimiters. The details are given in the supplemental material. A similar argument is needed to prove Typed Pattern Expansion:
\begin{theorem}[Typed Pattern Expansion (Strong)] ~
\begin{enumerate}[nolistsep]
  \item If $\pExpandsSP{\uDD{\uD}{\Delta}}{\uAS{\uA}{\Phi}}{\upv}{p}{\tau}{\uGG{\uG}{\pctx}}$ then $\patType{\pctx}{p}{\tau}$.
  \item If $\cvalidP{\uGG{\uG}{\pctx}}{\pscene{\uDD{\uD}{\Delta}}{\uAP{\uA}{\Phi}}{b}}{\cpv}{p}{\tau}$ then $\patType{\pctx}{p}{\tau}$.
\end{enumerate}
\end{theorem}

\vspace{-8px}
\subsubsection{seTLM Reasoning Principles} 
The following theorem summarizes the abstract reasoning principles that programmers can rely on when applying an seTLM. Informal descriptions of the labeled clauses are given inline, in gray boxes. 
% \begingroup
% \def\thetheorem{\ref{thm:tsc-SES}}
\vspace{-4px}
\begin{theorem}[seTLM Reasoning Principles]\label{thm:setlm-reasoning}
If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uPsi}{\utsmap{\tsmv}{b}}{e}{\tau}$ then:
\begin{enumerate}[nolistsep]
\item (\textbf{Typing 1}) $\uPsi = \uPsi', \uShyp{\tsmv}{a}{\tau}{\eparse}$ and $\hastypeU{\Delta}{\Gamma}{e}{\tau}$
  \begin{quote}
  \begin{grayparbox}
     The type of the expansion is consistent with the type annotation on the applied seTLM definition.
  \end{grayparbox}
  \end{quote}
\item $\encodeBody{b}{\ebody}$
\item $\evalU{\ap{\eparse}{\ebody}}{\aein{\lbltxt{SuccessE}}{\ecand}}$
\item $\decodeCondE{\ecand}{\ce}$
\item (\textbf{Segmentation}) $\segOK{\segof{\ce}}{b}$
        \begin{quote}
          \begin{grayparbox}
        The segmentation determined by the proto-expansion actually segments the literal body (i.e. each segment is in-bounds and the segments are non-overlapping.)
          \end{grayparbox}
\end{quote}
\item $\segof{\ce} = \sseq{\acesplicedt{m'_i}{n'_i}}{\nty} \cup \sseq{\acesplicede{m_i}{n_i}{\ctau_i}}{\nexp}$
\item \textbf{(Typing 2)} $\sseq{
      \expandsTU{\uDD{\uD}{\Delta}}
      {
        \parseUTypF{\bsubseq{b}{m'_i}{n'_i}}
      }{\tau'_i}
    }{\nty}$ and $\sseq{\istypeU{\Delta}{\tau'_i}}{\nty}$
      \begin{quote}
        \begin{grayparbox}

        Each spliced type has a well-formed expansion.
        \end{grayparbox}
\end{quote}
\item \textbf{(Typing 3)} $\sseq{
  \cvalidT{\emptyset}{
    \tsceneUP
      {\uDD
        {\uD}{\Delta}
      }{b}
  }{
    \ctau_i
  }{\tau_i}
}{\nexp}$ and $\sseq{\istypeU{\Delta}{\tau_i}}{\nexp}$
    \begin{quote}
      \begin{grayparbox}
      Each type annotation on a reference to a spliced expression has a well-formed expansion.
      \end{grayparbox}
\end{quote}
\item \textbf{(Typing 4)} $\sseq{
  \expandsU
    {\uDD{\uD}{\Delta}}
    {\uGG{\uG}{\Gamma}}
    {\uPsi}
    {\parseUExpF{\bsubseq{b}{m_i}{n_i}}}
    {e_i}
    {\tau_i}
}{\nexp}$ and $\sseq{\hastypeU{\Delta}{\Gamma}{e_i}{\tau_i}}{\nexp}$
    \begin{quote}
      \begin{grayparbox}
      Each spliced expression has a well-typed expansion consistent with the type annotation in the segmentation.
      \end{grayparbox}
\end{quote}
\item (\textbf{Capture Avoidance}) $e = [\sseq{\tau'_i/t_i}{\nty}, \sseq{e_i/x_i}{\nexp}]e'$ for some variables $\sseq{t_i}{\nty}$ and $\sseq{x_i}{\nexp}$, and $e'$
    \begin{quote}
    \begin{grayparbox}
      The final expansion can be decomposed into a  term with variables in place of each spliced type or expression. The expansions of these spliced types and expressions can be substituted into this term in the standard capture avoiding manner.
    \end{grayparbox}
    \end{quote}
\item (\textbf{Context Independence}) $\mathsf{fv}(e') \subset \sseq{t_i}{\nty} \cup \sseq{x_i}{\nexp}$
    \begin{quote}
      \begin{grayparbox}

      The decomposed term makes no mention of bindings in the application site context, i.e. the only free variables are those standing for spliced terms.
      \end{grayparbox}
\end{quote}
  % $\hastypeU
  % {\sseq{\Dhyp{t_i}}{\nty}}
  % {\sseq{x_i : \tau_i}{\nexp}}
  % {e'}{\tau}$
\end{enumerate}
\end{theorem}
\vspace{-4px}

The proof, which involves auxiliary lemmas about the decomposition of proto-types and proto-expressions, is given in the supplement.

Notice that we were able to state the hygiene properties (\textbf{Capture Avoidance} and \textbf{Context Independence}) without needing a notion of alpha-equivalence of source terms, as is the case in the typical formal accounts of hygiene \cite{Kohlbecker86a,DBLP:conf/popl/Adams15,DBLP:conf/popl/ClingerR91,DBLP:journals/lisp/DybvigHB92,DBLP:conf/esop/HermanW08,Herman10:Theory}. Instead, we used standard notions of capture avoiding substitution and free variables combined with the context disjointness conditions in the rules above. This is possible only because we keep track of spliced terms explicitly in the proto-expansion, rather than going straight to the final expansion. In fact, doing so is critical for TLMs -- there is no notion of alpha-conversion for partially parsed terms, so any notion of hygiene that relies on this notion would be inapplicable.

\vspace{-5px}
\subsubsection{spTLM Reasoning Principles} Finally, the following theorem summarizes the abstract reasoning principles available to programmers when applying an spTLM. Most of the labeled clauses are analagous to those described above, so we omit their descriptions.
% \begingroup
% \def\thetheorem{\ref{thm:spTLM-Typing-Segmentation}}
\begin{theorem}[spTLM Reasoning Principles]
% \label{thm:spTLM-Typing-Segmentation}
If $\patExpands{\upctx}{\uPhi}{\utsmap{\tsmv}{b}}{p}{\tau}$ where $\uDelta=\uDD{\uD}{\Delta}$ and $\uGamma=\uGG{\uG}{\Gamma}$ then all of the following hold:
\begin{enumerate}[noitemsep,nolistsep]
        \item (\textbf{Typing 1}) $\uPhi=\uPhi', \uPhyp{\tsmv}{a}{\tau}{\eparse}$ and $\patType{\pctx}{p}{\tau}$
        \item $\encodeBody{b}{\ebody}$
        \item $\evalU{\eparse(\ebody)}{\aein{\mathtt{SuccessP}}{\ecand}}$
        \item $\decodeCEPat{\ecand}{\cpv}$
        \item (\textbf{Segmentation}) $\segOK{\segof{\cpv}}{b}$
        \item $\segof{\cpv} = \sseq{\acesplicedt{n'_i}{m'_i}}{\nty} \cup \sseq{\acesplicedp{m_i}{n_i}{\ctau_i}}{\npat}$
        \item (\textbf{Typing 2}) $\sseq{
              \expandsTU{\uDelta}
              {
                \parseUTypF{\bsubseq{b}{m'_i}{n'_i}}
              }{\tau'_i}
            }{\nty}$ and $\sseq{\istypeU{\Delta}{\tau'_i}}{\nty}$
        \item (\textbf{Typing 3}) $\sseq{
          \cvalidT{\emptyset}{
            \tsceneUP
              {\uDelta}{b}
          }{
            \ctau_i
          }{\tau_i}
        }{\npat}$ and $\sseq{\istypeU{\Delta}{\tau_i}}{\npat}$
        \item (\textbf{Typing 4}) $\sseq{
          \patExpands
            {\uGG{\uG_i}{\pctx_i}}
            {\uPhi}
            {\parseUPatF{\bsubseq{b}{m_i}{n_i}}}
            {p_i}
            {\tau_i}
        }{\npat}$  and $\sseq{\patType{\pctx_i}{p_i}{\tau_i}}{\npat}$
      \item (\textbf{Visibility}) $\uG = \biguplus_{0 \leq i < \npat} \uG_i$ and $\Gamma = \bigcup_{0 \leq i < \npat} \pctx_i$
        \begin{quote}
          \begin{grayparbox}
          The hypotheses generated by the TLM application are exactly those generated by the spliced patterns.
          \end{grayparbox}
        \end{quote}
\end{enumerate}
\end{theorem}
\vspace{-3px}
% \begin{proof} 
% The proof, in the supplement, relies on an auxiliary lemma about decomposing proto-patterns.

\subsection{Retrospective} Let us step back and briefly review what we have accomplished so far. First, we defined an entirely standard expanded language (Sec. \ref{sec:s-XL}). Then we mirrored the forms in this language to form the common forms of the unexpanded language (Sec. \ref{sec:s-UL}), taking care to handle identifiers carefully (Sec. \ref{sec:s-TE}). We also added forms for defining TLMs (Sec. \ref{sec:s-TLM-def}). We then considered TLM application, which invokes a parse function to programmatically produce an encoding of a proto-expansion (Sec. \ref{sec:s-TLM-ap}). Each proto-expansion is then decoded, validated and inductively expanded  (Sec. \ref{sec:s-PEV}) to establish the abstract reasoning principles just described (Sec. \ref{sec:s-metatheory}). 
% We write $\tsfrom{\escenev}$ for the type splicing scene constructed by dropping unnecessary contexts from $\escenev$:
% \[\tsfrom{\esceneUP{\uDelta}{\uGamma}{\uPsi}{\uPhi}{b}} = \tsceneUP{\uDelta}{b}\]

% Figure \ref{fig:U-candidate-terms} defines the syntax of proto-types, $\ctau$, and proto-expressions, $\ce$. Proto-types and -expressions 

% Each expanded form maps onto a proto-expansion form. We refer to these as the \emph{common proto-expansion forms}. The mapping is given explicitly in Appendix \ref{appendix:proto-expansions-SES}.

% There are two ``interesting'' proto-expansion forms, highlighted in yellow in Figure \ref{fig:U-candidate-terms}: a proto-type form for \emph{references to spliced unexpanded types}, $\acesplicedt{m}{n}$, and a proto-expression form for \emph{references to spliced unexpanded expressions}, $\acesplicede{m}{n}{\ctau}$, where $m$ and $n$ are natural numbers.%TLM utilize these to splice types and unexpanded expressions out of literal bodies.


% \subsubsection{Proto-Type Validation}\label{sec:SE-proto-type-validation}
% The \emph{proto-type validation judgement}, $\cvalidT{\Delta}{\tscenev}{\ctau}{\tau}$, is inductively defined by Rules (\ref{rules:cvalidT-U}).

% \paragraph{Common Forms} Rules (\ref{rule:cvalidT-U-tvar}) through (\ref{rule:cvalidT-U-sum}) validate proto-types of common form. The first three of these are reproduced below.
% %Each of these rules is defined based on the corresponding type formation rule, i.e. Rules (\ref{rule:istypeU-var}) through (\ref{rule:istypeU-sum}), respectively. For example, the following proto-types validation rules are based on type formation rules (\ref{rule:istypeU-var}), (\ref{rule:istypeU-parr}) and (\ref{rule:istypeU-all}), respectively: 
% % \begin{subequations}%\label{rules:cvalidT-U}
% \begin{equation*}\tag{\ref{rule:cvalidT-U-tvar}}
% \inferrule{ }{
%   \cvalidT{\Delta, \Dhyp{t}}{\tscenev}{t}{t}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidT-U-parr}}
%   \inferrule{
%     \cvalidT{\Delta}{\tscenev}{\ctau_1}{\tau_1}\\
%     \cvalidT{\Delta}{\tscenev}{\ctau_2}{\tau_2}
%   }{
%     \cvalidT{\Delta}{\tscenev}{\aceparr{\ctau_1}{\ctau_2}}{\aparr{\tau_1}{\tau_2}}
%   }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidT-U-all}}
%   \inferrule {
%     \cvalidT{\Delta, \Dhyp{t}}{\tscenev}{\ctau}{\tau}
%   }{
%     \cvalidT{\Delta}{\tscenev}{\aceall{t}{\ctau}}{\aall{t}{\tau}}
%   }
% \end{equation*}
% % \begin{equation*}\label{rule:cvalidT-U-rec}
% %   \inferrule{
% %     \cvalidT{\Delta, \Dhyp{t}}{\tscenev}{\ctau}{\tau}
% %   }{
% %     \cvalidT{\Delta}{\tscenev}{\acerec{t}{\ctau}}{\arec{t}{\tau}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidT-U-prod}
% %   \inferrule{
% %     \{\cvalidT{\Delta}{\tscenev}{\ctau_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \cvalidT{\Delta}{\tscenev}{\aceprod{\labelset}{\mapschema{\ctau}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidT-U-sum}
% %   \inferrule{
% %     \{\cvalidT{\Delta}{\tscenev}{\ctau_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \cvalidT{\Delta}{\tscenev}{\acesum{\labelset}{\mapschema{\ctau}{i}{\labelset}}}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}

% These rules, like the rules for common unexpanded type forms,  mirror the corresponding type formation rules, i.e. Rules (\ref{rules:istypeU}). The type splicing scene, $\tscenev$, passes opaquely through these rules. 
% % We can express this scheme more precisely with the following rule transformation. For each rule in Rules (\ref{rules:istypeU}), 
% % \begin{mathpar}
% % % \refstepcounter{equation}
% % % \label{rule:cvalidT-U-rec}
% % % \refstepcounter{equation}
% % % \label{rule:cvalidT-U-prod}
% % % \refstepcounter{equation}
% % % \label{rule:cvalidT-U-sum}
% % % \inferrule{J_1\\\cdots\\J_k}{J}
% % \end{mathpar}
% % the corresponding proto-types validation rule is
% % \begin{mathpar}
% % \inferrule{
% %   \VTypof{J_1}\\
% %   \cdots\\
% %   \VTypof{J_k}
% % }{
% %   \VTypof{J}
% % }
% % \end{mathpar}
% % where 
% % \[\begin{split}
% % \VTypof{\istypeU{\Delta}{\tau}} & = \cvalidT{\Delta}{\tscenev}{\VTypof{\tau}}{\tau}\\
% % \VTypof{\{J_i\}_{i \in \labelset}} & = \{\VTypof{J_i}\}_{i \in \labelset}
% % \end{split}\]
% % and where $\VTypof{\tau}$, when $\tau$ is a metapattern of sort $\mathsf{Typ}$, is a metapattern of sort $\mathsf{CETyp}$ defined as follows:
% % \begin{itemize}
% % \item When $\tau$ is of definite form, $\VTypof{\tau}$ is defined as follows:
% % \begin{align*}
% % \VTypof{t} & = t\\
% % \VTypof{\aparr{\tau_1}{\tau_2}} & = \aceparr{\VTypof{\tau_1}}{\VTypof{\tau_2}}\\
% % \VTypof{\aall{t}{\tau}} & = \aceall{t}{\VTypof{\tau}}\\
% % \VTypof{\arec{t}{\tau}} & = \acerec{t}{\VTypof{\tau}}\\
% % \VTypof{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}} & = \aceprod{\labelset}{\mapschemax{\VTypofv}{\tau}{i}{\labelset}}\\
% % \VTypof{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}} & = \acesum{\labelset}{\mapschemax{\VTypofv}{\tau}{i}{\labelset}}
% % \end{align*}
% % \item When $\tau$ is of indefinite form, $\VTypof{\tau}$ is a uniquely corresponding metapattern also of indefinite form. For example, $\VTypof{\tau_1}=\ctau_1$ and $\VTypof{\tau_2}=\ctau_2$.
% % \end{itemize}

% % It is instructive to use this rule transformation to generate Rules (\ref{rule:cvalidT-U-tvar}) through (\ref{rule:cvalidT-U-all}) above. We omit the remaining rules, i.e. Rules (\ref*{rule:cvalidT-U-rec}) through (\ref*{rule:cvalidT-U-sum}). 

% Notice that in Rule (\ref{rule:cvalidT-U-tvar}), only type variables tracked by $\Delta$, the expansion's local type validation context, are well-formed. Type variables tracked by the application site unexpanded type formation context, which is a component of the type splicing scene, $\tscenev$, are not validated. %Indeed, $\tscenev$ passes opaquely through the rules above. %This achieves \emph{context-independent expansion} as described in Sec. \ref{sec:splicing-and-hygiene} for type variables -- seTLMs cannot impose ``hidden constraints'' on the application site unexpanded type formation context, because the type variables bound at the application site are simply not directly available to proto-types.

% \paragraph{References to Spliced Types} The only proto-type form that does not correspond to a type form is $\acesplicedt{m}{n}$, which is a \emph{reference to a spliced unexpanded type}, i.e. it indicates that an unexpanded type should be parsed out from the literal body, which appears in the type splicing scene $\tscenev$, beginning at position $m$ and ending at position $n$, where $m$ and $n$ are natural numbers. Rule (\ref{rule:cvalidT-U-splicedt}) governs this form:
% \begin{equation*}\tag{\ref{rule:cvalidT-U-splicedt}}
%   \inferrule{
%     \parseUTyp{\bsubseq{b}{m}{n}}{\utau}\\
%     \expandsTU{\uDD{\uD}{\Delta_\text{app}}}{\utau}{\tau}\\
%     \Delta \cap \Delta_\text{app} = \emptyset
%   }{
%     \cvalidT{\Delta}{\tsceneU{\uDD{\uD}{\Delta_\text{app}}}{b}}{\acesplicedt{m}{n}}{\tau}
%   }
% \end{equation*}
% The first premise of this rule extracts the indicated subsequence of $b$ using the partial metafunction $\bsubseq{b}{m}{n}$ and parses it using the partial metafunction $\mathsf{parseUTyp}(b)$, characterized in Sec. \ref{sec:syntax-U}, to produce the spliced unexpanded type itself, $\utau$.

% The second premise of Rule (\ref{rule:cvalidT-U-splicedt}) performs type expansion of $\utau$ under the application site unexpanded type formation context, $\uDD{\uD}{\Delta_\text{app}}$, which is a component of the type splicing scene. The hypotheses in the expansion's local type formation context, $\Delta$, are not made available to $\tau$. %This enforces the injunction on shadowing as described in Sec. \ref{sec:splicing-and-hygiene} for type variables that appear in proto-types. 

% The third premise of Rule (\ref{rule:cvalidT-U-splicedt}) imposes the constraint that the proto-expansion's type formation context, $\Delta$, be disjoint from the application site type formation context, $\Delta_\text{app}$. This premise can always be discharged by $\alpha$-varying the proto-expansion that the reference to the spliced type appears within. 

% Together, these two premises enforce the injunction on type variable capture as described in Sec. \ref{sec:uetsms-validation} -- the TLM provider can choose type variable names freely within a proto-expansion. We will consider this formally in Sec. \ref{sec:SE-metatheory} below. %, because the language prevents them from shadowing type variables at the application site (by $\alpha$-varying the proto-expansion as needed.)%Such a change in bound variable names is possible again because variables bound by the seTLM provider in a proto-expansion cannot ``leak into'' spliced terms because the hypotheses in $\Delta$ are not made available to the spliced type, $\tau$. 

% Rules (\ref{rules:cvalidT-U}) validate the following lemma, which establishes that the final expansion of a valid proto-type is a well-formed type under the combined type formation context.
% \begingroup
% \def\thetheorem{\ref{lemma:candidate-expansion-type-validation}}
% \begin{lemma}[Proto-Expansion Type Validation]
% If $\cvalidT{\Delta}{\tsceneU{\uDD{\uD}{\Delta_\text{app}}}{b}}{\ctau}{\tau}$ and $\Delta \cap \Delta_\text{app}=\emptyset$ then $\istypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\tau}$.
% \end{lemma}
% \endgroup

% \subsubsection{Proto-Expression Validation}
% The \emph{proto-expression validation judgement}, $\cvalidE{\Delta}{\Gamma}{\escenev}{\ce}{e}{\tau}$, is defined mutually inductively with the typed expansion judgement by Rules (\ref{rules:cvalidE-U}) as follows.% This is necessary because a typed expansion judgement appears as a premise in Rule (\ref{rule:cvalidE-U-splicede}) below, and a proto-expression validation judgement appears as a premise in Rule (\ref{rule:expandsU-tsmap}) above.

% \paragraph{Common Forms} Rules (\ref{rule:cvalidE-U-var}) through (\ref{rule:cvalidE-U-case}) validate proto-expressions of common form, as well as ascriptions and let binding. The first five of these rules are reproduced below:
% %For each expanded expression form defined in Figure \ref{fig:U-expanded-terms}, Figure \ref{fig:U-candidate-terms} defines a corresponding proto-expression form. The validation rules for proto-expressions of these forms are each based on the corresponding typing rule in Rules (\ref{rules:hastypeU}). For example, the validation rules for proto-expressions of variable, function and function application form  are based on Rules (\ref{rule:hastypeU-var}) through (\ref{rule:hastypeU-ap}), respectively:
% %\begin{subequations}%\label{rules:cvalidE-U}
% \begin{equation*}\tag{\ref{rule:cvalidE-U-var}}
% \inferrule{ }{
%   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{x}{x}{\tau}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidE-U-asc}}
% \inferrule{
%   \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
%   \cvalidE{\Delta}{\Gamma}{\escenev}{\ce}{e}{\tau}
% }{
%   \cvalidE{\Delta}{\Gamma}{\escenev}{\aceasc{\ctau}{\ce}}{e}{\tau}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidE-U-letsyn}}
%   \inferrule{
%     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_1}{e_1}{\tau_1}\\
%     \cvalidE{\Delta}{\Gamma, x : \tau_1}{\ce_2}{e_2}{\tau_2}
%   }{
%     \cvalidE{\Delta}{\Gamma}{\escenev}{\aceletsyn{x}{\ce_1}{\ce_2}}{
%       \aeap{\aelam{\tau_1}{x}{e_2}}{e_1}
%     }{\tau_2}
%   }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidE-U-lam}}
% \inferrule{
%   \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
%   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{\ce}{e}{\tau'}
% }{
%   \cvalidE{\Delta}{\Gamma}{\escenev}{\acelam{\ctau}{x}{\ce}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:cvalidE-U-ap}}
%   \inferrule{
%     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_1}{e_1}{\aparr{\tau}{\tau'}}\\
%     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_2}{e_2}{\tau}
%   }{
%     \cvalidE{\Delta}{\Gamma}{\escenev}{\aceap{\ce_1}{\ce_2}}{\aeap{e_1}{e_2}}{\tau'}
%   }
% \end{equation*}
% Once again, the rules for common forms mirror the typing rules, i.e. Rules (\ref{rules:hastypeU}). The expression splicing scene, $\escenev$, passes opaquely through these rules.


% Notice that in Rule (\ref{rule:cvalidE-U-var}), only variables tracked by the proto-expansion typing context, $\Gamma$, are validated. Variables  in the application site unexpanded typing context, which appears within the expression splicing scene $\escenev$, are not validated. This achieves \emph{context independence} as described in Sec. \ref{sec:uetsms-validation} -- seTLMs cannot impose ``hidden constraints'' on the application site unexpanded typing context, because the variable bindings at the application site are not directly available to proto-expansions. We will consider this formally in Sec. \ref{sec:SE-metatheory} below.

% \paragraph{References to Spliced Unexpanded Expressions} The only proto-expression form that does not correspond to an expanded expression form is $\acesplicede{m}{n}{\ctau}$, which is a \emph{reference to a spliced unexpanded expression}, i.e. it indicates that an unexpanded expression should be parsed out from the literal body beginning at position $m$ and ending at position $n$. Rule (\ref{rule:cvalidE-U-splicede}) governs this form:
% \begin{equation*}\tag{\ref{rule:cvalidE-U-splicede}}
% \inferrule{
%   \cvalidT{\emptyset}{\tsfrom{\escenev}}{\ctau}{\tau}\\
%   \escenev=\esceneU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{b}\\
%   \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\
%   \expandsU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{\ue}{e}{\tau}\\\\
%   \Delta \cap \Delta_\text{app} = \emptyset\\
%   \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset
% }{
%   \cvalidE{\Delta}{\Gamma}{\escenev}{\acesplicede{m}{n}{\ctau}}{e}{\tau}
% }
% \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-splicede}
% % \inferrule{
% %   \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\\\
% %   \expandsU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{\ue}{e}{\tau}\\
% %   \Delta \cap \Delta_\text{app} = \emptyset\\
% %   \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset
% % }{
% %   \cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\splicede{m}{n}}{e}{\tau}
% % }
% % \end{equation*}

% The first premise of this rule validates and expands the type annotation. This type must be context independent.

% The second premise of this rule serves simply to reveal the components of the expression splicing scene.

% The third premise of this rule extracts the indicated subsequence of $b$ using the partial metafunction $\bsubseq{b}{m}{n}$ and parses it using the partial metafunction $\mathsf{parseUExp}(b)$, characterized in Sec. \ref{sec:syntax-U}, to produce the referenced spliced unexpanded expression, $\ue$.

% The fourth premise of Rule (\ref{rule:cvalidE-U-splicede}) performs typed expansion of $\ue$ assuming the application site contexts that appear in the expression splicing scene. Notice that the hypotheses in $\Delta$ and $\Gamma$ are not made available to $\ue$. 

% The fifth premise of Rule (\ref{rule:cvalidE-U-splicede}) imposes the constraint that the proto-expansion's type formation context, $\Delta$, be disjoint from the application site type formation context, $\Delta_\text{app}$. Similarly, the sixth premise requires that the proto-expansion's typing context, $\Gamma$, be disjoint from the application site typing context, $\Gamma_\text{app}$. These two premises can always be discharged by $\alpha$-varying the proto-expression that the reference to the spliced unexpanded expression appears within. 
% Together, these premises enforce the prohibition on capture as described in Sec. \ref{sec:uetsms-validation} -- the TLM provider can choose variable names freely within a proto-expansion, because the language prevents them from shadowing those at the application site.
% %\end{subequations}
% % \begin{subequations}\label{rules:cvalidE-U}
% % \begin{equation*}\label{rule:cvalidE-U-var}
% % \inferrule{ }{
% %   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{x}{x}{\tau}
% % }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-lam}
% % \inferrule{
% %   \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
% %   \cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau}}{\escenev}{\ce}{e}{\tau'}
% % }{
% %   \cvalidE{\Delta}{\Gamma}{\escenev}{\acelam{\ctau}{x}{\ce}}{\aelam{\tau}{x}{e}}{\aparr{\tau}{\tau'}}
% % }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-ap}
% %   \inferrule{
% %     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_1}{e_1}{\aparr{\tau}{\tau'}}\\
% %     \cvalidE{\Delta}{\Gamma}{\escenev}{\ce_2}{e_2}{\tau}
% %   }{
% %     \cvalidE{\Delta}{\Gamma}{\escenev}{\aceap{\ce_1}{\ce_2}}{\aeap{e_1}{e_2}}{\tau'}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-tlam}
% %   \inferrule{
% %     \cvalidE{\Delta, \Dhyp{t}}{\Gamma}{\escenev}{\ce}{e}{\tau}
% %   }{
% %     \cvalidEX{\acetlam{t}{\ce}}{\aetlam{t}{e}}{\aall{t}{\tau}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-tap}
% %   \inferrule{
% %     \cvalidEX{\ce}{e}{\aall{t}{\tau}}\\
% %     \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau'}{\tau'}
% %   }{
% %     \cvalidEX{\acetap{\ce}{\ctau'}}{\aetap{e}{\tau'}}{[\tau'/t]\tau}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-fold}
% %   \inferrule{
% %     \cvalidT{\Delta, \Dhyp{t}}{\escenev}{\ctau}{\tau}\\
% %     \cvalidEX{\ce}{e}{[\arec{t}{\tau}/t]\tau}
% %   }{
% %     \cvalidEX{\acefold{t}{\ctau}{\ce}}{\aefold{e}}{\arec{t}{\tau}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-unfold}
% %   \inferrule{
% %     \cvalidEX{\ce}{e}{\arec{t}{\tau}}
% %   }{
% %     \cvalidEX{\aceunfold{\ce}}{\aeunfold{e}}{[\arec{t}{\tau}/t]\tau}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-tpl}
% %   \inferrule{
% %     \{\cvalidEX{\ce_i}{e_i}{\tau_i}\}_{i \in \labelset}
% %   }{
% %     \cvalidEX{\acetpl{\labelset}{\mapschema{\ce}{i}{\labelset}}}{\aetpl{\labelset}{\mapschema{e}{i}{\labelset}}}{\aprod{\labelset}{\mapschema{\tau}{i}{\labelset}}}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-pr}
% %   \inferrule{
% %     \cvalidEX{\ce}{e}{\aprod{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}}
% %   }{
% %     \cvalidEX{\acepr{\ell}{\ce}}{\aepr{\ell}{e}}{\tau}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-in}
% %   \inferrule{
% %     \{\cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau_i}{\tau_i}\}_{i \in \labelset}\\
% %     \cvalidT{\Delta}{\tsfrom{\escenev}}{\ctau}{\tau}\\
% %     \cvalidEX{\ce}{e}{\tau}
% %   }{
% %     \left\{\shortstack{$\Delta~\Gamma \vdash_\uPsi \acein{\labelset, \ell}{\ell}{\mapschema{\ctau}{i}{\labelset}; \mapitem{\ell}{\ctau}}{\ce}$\\$\leadsto$\\$\aein{\labelset, \ell}{\ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}{e} : \asum{\labelset, \ell}{\mapschema{\tau}{i}{\labelset}; \mapitem{\ell}{\tau}}$\vspace{-1.2em}}\right\}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-case}
% %   \inferrule{
% %     \cvalidEX{\ce}{e}{\asum{\labelset}{\mapschema{\tau}{i}{\labelset}}}\\
% %     \{\cvalidE{\Delta}{\Gamma, \Ghyp{x_i}{\tau_i}}{\escenev}{\ue_i}{e_i}{\tau}\}_{i \in \labelset}
% %   }{
% %     \cvalidEX{\acecase{\labelset}{\ce}{\mapschemab{x}{\ce}{i}{\labelset}}}{\aecase{\labelset}{e}{\mapschemab{x}{e}{i}{\labelset}}}{\tau}
% %   }
% % \end{equation*}
% % \begin{equation*}\label{rule:cvalidE-U-splicede}
% % \inferrule{
% %   \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\\\
% %   \Delta \cap \Delta_\text{app} = \emptyset\\
% %   \domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset\\
% %   \expandsU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{\ue}{e}{\tau}
% % }{
% %   \cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\acesplicede{m}{n}}{e}{\tau}
% % }
% % \end{equation*}
% % \end{subequations}

% % Each form of expanded expression, $e$, corresponds to a form of proto-expression, $\ce$ (compare Figure \ref{fig:U-expanded-terms} and Figure \ref{fig:U-candidate-terms}). For each typing rule in Rules \ref{rules:hastypeU}, there is a corresponding proto-expression validation rule -- Rules (\ref{rule:cvalidE-U-var}) to (\ref{rule:cvalidE-U-case}) -- where the proto-expression and expanded expression correspond. The premises also correspond.


% %Candidate expansions cannot themselves define or apply TLMs. This simplifies our metatheory, though it can be inconvenient at times for TLM providers. We discuss adding the ability to use TLMs within proto-expansions in Sec. \ref{sec:tsms-in-expansions}.


% \subsection{Metatheory}\label{sec:SE-metatheory}
% \subsubsection{Typed Expansion}
% Let us now consider Theorem \ref{thm:typed-expansion-short-U}, which was mentioned at the beginnning of Sec. \ref{sec:typed-expansion-U} and is reproduced below:
% \begingroup
% \def\thetheorem{\ref{thm:typed-expansion-short-U}}
% \begin{theorem}[Typed Expression Expansion] \hspace{-3px}If $\expandsU{\uDD{\uD}{\Delta}\hspace{-3px}}{\uGG{\uG}{\Gamma}\hspace{-3px}}{\uPsi}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
% \end{theorem}
% \endgroup

%  To prove this theorem, we must  prove the following stronger theorem, because the proto-expression validation judgement is defined mutually inductively with the typed expansion judgement:

% \begingroup
% \def\thetheorem{\ref{thm:typed-expansion-full-U}}
% \begin{theorem}[Typed Expansion (Full)] ~
% \begin{enumerate}
% \item If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uAS{\uA}{\Psi}}{\ue}{e}{\tau}$ then $\hastypeU{\Delta}{\Gamma}{e}{\tau}$.
% \item If $\cvalidE{\Delta}{\Gamma}{\esceneU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uAS{\uA}{\Psi}}{b}}{\ce}{e}{\tau}$ and $\Delta \cap \Delta_\text{app} = \emptyset$ and $\domof{\Gamma} \cap \domof{\Gamma_\text{app}} = \emptyset$ then $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$.
% \end{enumerate}
% \end{theorem}
% \endgroup
% \begin{proof}
% By mutual rule induction over Rules (\ref{rules:expandsU}) and Rules (\ref{rules:cvalidE-U}). The full proof is given in Appendix \ref{appendix:SES-typed-expression-expansion-metatheory}. We will reproduce the interesting cases below. 

% The proof of part 1 proceeds by inducting over the typed expansion assumption. The only interesting cases are those related to seTLM definition and application, reproduced below. In the following cases, let $\uDelta=\uDD{\uD}{\Delta}$ and $\uGamma=\uGG{\uG}{\Gamma}$ and $\uPsi=\uAS{\uA}{\Psi}$.

% \begin{byCases}
% \item[\text{(\ref{rule:expandsU-syntax})}] We have 
% \begin{pfsteps}
%   \item \ue=\uesyntax{\tsmv}{\utau'}{\eparse}{\ue'} \BY{assumption}
%   \item \expandsTU{\uDelta}{\utau'}{\tau'} \BY{assumption} \pflabel{expandsTU}
%  \item \hastypeU{\emptyset}{\emptyset}{\eparse}{\aparr{\tBody}{\tParseResultExp}} \BY{assumption}\pflabel{eparse}
%   \item \expandsU{\uDelta}{\uGamma}{\uPsi, \uShyp{\tsmv}{a}{\tau'}{\eparse}}{\ue'}{e}{\tau} \BY{assumption}\pflabel{expandsU}
% %  \item \uetsmenv{\Delta}{\Psi} \BY{assumption}\pflabel{uetsmenv1}
%  \item \istypeU{\Delta}{\tau'} \BY{Lemma \ref{lemma:type-expansion-U} to \pfref{expandsTU}} \pflabel{istype}
% %  \item \uetsmenv{\Delta}{\Psi, \xuetsmbnd{\tsmv}{\tau'}{\eparse}} \BY{Definition \ref{def:seTLM-def-ctx-formation} on \pfref{uetsmenv1}, \pfref{istype} and \pfref{eparse}}\pflabel{uetsmenv3}
%   \item \hastypeU{\Delta}{\Gamma}{e}{\tau} \BY{IH, part 1(a) on \pfref{expandsU}}
% \end{pfsteps}
% \resetpfcounter 

% \item[\text{(\ref{rule:expandsU-tsmap})}] We have 
% \begin{pfsteps}
%   \item \ue=\utsmap{\tsmv}{b} \BY{assumption}
%   \item \uA = \uA', \vExpands{\tsmv}{a} \BY{assumption}
%   \item \Psi=\Psi', \xuetsmbnd{a}{\tau}{\eparse} \BY{assumption}
%   \item \encodeBody{b}{\ebody} \BY{assumption}
%   \item \evalU{\eparse(\ebody)}{\aein{\lbltxt{SuccessE}}{\ecand}} \BY{assumption}
%   \item \decodeCondE{\ecand}{\ce} \BY{assumption}
%   \item \cvalidE{\emptyset}{\emptyset}{\esceneU{\uDelta}{\uGamma}{\uPsi}{b}}{\ce}{e}{\tau} \BY{assumption}\pflabel{cvalidE}
% %  \item \uetsmenv{\Delta}{\Psi} \BY{assumption} \pflabel{uetsmenv}
%   \item \emptyset \cap \Delta = \emptyset \BY{finite set intersection} \pflabel{delta-cap}
%   \item {\emptyset} \cap \domof{\Gamma} = \emptyset \BY{finite set intersection} \pflabel{gamma-cap}
%   \item \hastypeU{\emptyset \cup \Delta}{\emptyset \cup \Gamma}{e}{\tau} \BY{IH, part 2 on \pfref{cvalidE}, \pfref{delta-cap}, and \pfref{gamma-cap}} \pflabel{penultimate}
%   \item \hastypeU{\Delta}{\Gamma}{e}{\tau} \BY{finite set and finite function identity over \pfref{penultimate}}
% \end{pfsteps}
% \resetpfcounter
% \end{byCases}

% The proof of part 2 proceeds by induction over the proto-expression validation assumption. The only interesting case governs references to spliced expressions. In the following cases, let $\uDelta_\text{app}=\uDD{\uD}{\Delta_\text{app}}$ and $\uGamma_\text{app}=\uGG{\uG}{\Gamma_\text{app}}$ and $\uPsi = \uAS{\uA}{\Psi}$.
% \begin{byCases}
% % \item[\text{(\ref{rule:cvalidE-U-var})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=x$ \BY{assumption}
% %   \item $e=x$ \BY{assumption}
% %   \item $\Gamma=\Gamma', \Ghyp{x}{\tau}$ \BY{assumption}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gamma', \Ghyp{x}{\tau}}{x}{\tau}$ \BY{Rule (\ref{rule:hastypeU-var})} \pflabel{hastypeU}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma', \Ghyp{x}{\tau}}{\Gamma_\text{app}}}{x}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} over $\Gamma_\text{app}$ to \pfref{hastypeU}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\text{(\ref{rule:cvalidE-U-lam})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=\acelam{\ctau_1}{x}{\ce'}$ \BY{assumption}
% %   \item $e=\aelam{\tau_1}{x}{e'}$ \BY{assumption}
% %   \item $\tau=\aparr{\tau_1}{\tau_2}$ \BY{assumption}
% %   \item $\cvalidT{\Delta}{\tsceneU{\uDelta_\text{app}}{b}}{\ctau_1}{\tau_1}$ \BY{assumption} \pflabel{cvalidT}
% %   \item $\cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau_1}}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce'}{e'}{\tau_2}$ \BY{assumption} \pflabel{cvalidE}
% % %  \item $\uetsmenv{\Delta_\text{app}}{\Psi}$ \BY{assumption} \pflabel{uetsmenv}
% %   \item $\Delta \cap \Delta_\text{app}=\emptyset$ \BY{assumption} \pflabel{delta-disjoint}
% %   \item $\domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{assumption} \pflabel{gamma-disjoint}
% %   \item $x \notin \domof{\Gamma_\text{app}}$ \BY{identification convention} \pflabel{x-fresh}
% %   \item $\domof{\Gamma, x : \tau_1} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{\pfref{gamma-disjoint} and \pfref{x-fresh}} \pflabel{gamma-disjoint2}
% %   \item $\istypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\tau_1}$ \BY{Lemma \ref{lemma:candidate-expansion-type-validation} on \pfref{cvalidT} and \pfref{delta-disjoint}} \pflabel{istype}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma, \Ghyp{x}{\tau_1}}{\Gamma_\text{app}}}{e'}{\tau_2}$ \BY{IH, part 2 on \pfref{cvalidE}, \pfref{delta-disjoint} and \pfref{gamma-disjoint2}} \pflabel{hastype1}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}, \Ghyp{x}{\tau_1}}{e'}{\tau_2}$ \BY{exchange over $\Gamma_\text{app}$ on \pfref{hastype1}} \pflabel{hastype2}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aelam{\tau_1}{x}{e'}}{\aparr{\tau_1}{\tau_2}}$ \BY{Rule (\ref{rule:hastypeU-lam}) on \pfref{istype} and \pfref{hastype2}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\text{(\ref{rule:cvalidE-U-ap})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=\aceap{\ce_1}{\ce_2}$ \BY{assumption}
% %   \item $e=\aeap{e_1}{e_2}$ \BY{assumption}
% %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce_1}{e_1}{\aparr{\tau_2}{\tau}}$ \BY{assumption} \pflabel{cvalidE1}
% %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce_2}{e_2}{\tau_2}$ \BY{assumption} \pflabel{cvalidE2}
% % %  \item $\uetsmenv{\Delta_\text{app}}{\Psi}$ \BY{assumption} \pflabel{uetsmenv}
% %   \item $\Delta \cap \Delta_\text{app}=\emptyset$ \BY{assumption} \pflabel{delta-disjoint}
% %   \item $\domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{assumption} \pflabel{gamma-disjoint}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_1}{\aparr{\tau_2}{\tau}}$ \BY{IH, part 2 on \pfref{cvalidE1}, \pfref{delta-disjoint} and \pfref{gamma-disjoint}} \pflabel{hastypeU1}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_2}{\tau_2}$ \BY{IH, part 2 on \pfref{cvalidE2}, \pfref{delta-disjoint} and \pfref{gamma-disjoint}} \pflabel{hastypeU2}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aeap{e_1}{e_2}}{\tau}$ \BY{Rule (\ref{rule:hastypeU-ap}) on \pfref{hastypeU1} and \pfref{hastypeU2}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\text{(\ref{rule:cvalidE-U-tlam})}] ~
% % \begin{pfsteps}
% %   \item \ce=\acetlam{t}{\ce'} \BY{assumption}
% %   \item e = \aetlam{t}{e'} \BY{assumption}
% %   \item \tau = \aall{t}{\tau'}\BY{assumption}
% %   \item \cvalidE{\Delta, \Dhyp{t}}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce'}{e'}{\tau'} \BY{assumption} \pflabel{cvalidE}
% % %  \item \uetsmenv{\Delta_\text{app}}{\Psi} \BY{assumption} \pflabel{uetsmenv}
% %   \item \Delta \cap \Delta_\text{app}=\emptyset \BY{assumption} \pflabel{delta-disjoint}
% %   \item \domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset \BY{assumption} \pflabel{gamma-disjoint}
% %   \item \Dhyp{t} \notin \Delta_\text{app} \BY{identification convention}\pflabel{t-fresh}
% %   \item \Delta, \Dhyp{t} \cap \Delta_\text{app} = \emptyset \BY{\pfref{delta-disjoint} and \pfref{t-fresh}}\pflabel{delta-disjoint2}
% %   \item \hastypeU{\Dcons{\Delta, \Dhyp{t}}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e'}{\tau'} \BY{IH, part 2 on \pfref{cvalidE}, \pfref{delta-disjoint2} and \pfref{gamma-disjoint}}\pflabel{hastype1}
% %   \item \hastypeU{\Dcons{\Delta}{\Delta_\text{app}, \Dhyp{t}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e'}{\tau'} \BY{exchange over $\Delta_\text{app}$ on \pfref{hastype1}}\pflabel{hastype2}
% %   \item \hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aetlam{t}{e'}}{\aall{t}{\tau'}} \BY{Rule (\ref{rule:hastypeU-tlam}) on \pfref{hastype2}}
% % \end{pfsteps}
% % \resetpfcounter

% % \item[{\text{(\ref{rule:cvalidE-U-tap})}}~\textbf{through}~{\text{(\ref{rule:cvalidE-U-case})}}] These cases follow analagously, i.e. we apply the IH, part 2 to all proto-expression validation judgements, Lemma \ref{lemma:candidate-expansion-type-validation} to all proto-type validation judgements, the identification convention to ensure that extended contexts remain disjoint, weakening and exchange as needed, and the corresponding typing rule in Rules (\ref{rule:hastypeU-tap}) through (\ref{rule:hastypeU-case}).
% % \\

% \item[\text{(\ref{rule:cvalidE-U-splicede})}] ~
% \begin{pfsteps*}
%   \item $\ce=\acesplicede{m}{n}{\ctau}$ \BY{assumption}
%   \item $  \escenev=\esceneU{\uDD{\uD}{\Delta_\text{app}}}{\uGG{\uG}{\Gamma_\text{app}}}{\uPsi}{b}$ \BY{assumption}
%   \item   $\cvalidT{\emptyset}{\tsfrom{\escenev}}{\ctau}{\tau}$ \BY{assumption}
%   \item $\parseUExp{\bsubseq{b}{m}{n}}{\ue}$ \BY{assumption}
%   \item $\expandsU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{\ue}{e}{\tau}$ \BY{assumption} \pflabel{expands}
% %  \item $\uetsmenv{\Delta_\text{app}}{\Psi}$ \BY{assumption} \pflabel{uetsmenv}
%   \item $\Delta \cap \Delta_\text{app}=\emptyset$ \BY{assumption} \pflabel{delta-disjoint}
%   \item $\domof{\Gamma} \cap \domof{\Gamma_\text{app}}=\emptyset$ \BY{assumption} \pflabel{gamma-disjoint}
%   \item $\hastypeU{\Delta_\text{app}}{\Gamma_\text{app}}{e}{\tau}$ \BY{IH, part 1 on \pfref{expands}} \pflabel{hastype}
%   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} over $\Delta$ and $\Gamma$ and exchange on \pfref{hastype}}
% \end{pfsteps*}
% \resetpfcounter
% \end{byCases}

% The mutual induction can be shown to be well-founded by showing that the following numeric metric on the judgements that we induct over is decreasing:
% \begin{align*}
% \sizeof{\expandsU{\uDelta}{\uGamma}{\uPsi}{\ue}{e}{\tau}} & = \sizeof{\ue}\\
% \sizeof{\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\ce}{e}{\tau}} & = \sizeof{b}
% \end{align*}
% where $\sizeof{b}$ is the length of $b$ and $\sizeof{\ue}$ is the sum of the lengths of the literal bodies in $\ue$ (see Appendix \ref{appendix:SES-body-lengths}.)

% The only case in the proof of part 1 that invokes part 2 is Case (\ref{rule:expandsU-tsmap}). There, we have that the metric remains stable: \begin{align*}
%  & \sizeof{\expandsU{\uDelta}{\uGamma}{\uPsi}{\utsmap{\tsmv}{b}}{e}{\tau}}\\
% =& \sizeof{\cvalidE{\emptyset}{\emptyset}{\esceneU{\uDelta}{\uGamma}{\uPsi}{b}}{\ce}{e}{\tau}}\\
% =&\sizeof{b}\end{align*}

% The only case in the proof of part 2 that invokes part 1 is Case (\ref{rule:cvalidE-U-splicede}). There, we have that $\parseUExp{\bsubseq{b}{m}{n}}{\ue}$ and the IH is applied to the judgement $\expandsU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{\ue}{e}{\tau}$ where $\uDelta_\text{app}=\uDD{\uD}{\Delta_\text{app}}$ and $\uGamma_\text{app}=\uGG{\uG}{\Gamma_\text{app}}$ and $\uPsi=\uAS{\uA}{\Psi}$. Because the metric is stable when passing from part 1 to part 2, we must have that it is strictly decreasing in the other direction:
% \[\sizeof{\expandsU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{\ue}{e}{\tau}} < \sizeof{\cvalidE{\Delta}{\Gamma}{\esceneU{\uDelta_\text{app}}{\uGamma_\text{app}}{\uPsi}{b}}{\acesplicede{m}{n}{\ctau}}{e}{\tau}}\]
% i.e. by the definitions above, 
% \[\sizeof{\ue} < \sizeof{b}\]

% This is established by appeal to the following two conditions. The first condition states that an unexpanded expression constructed by parsing a textual sequence $b$ is strictly smaller, as measured by the metric defined above, than the length of $b$, because some characters must necessarily be used to invoke a TLM and delimit each literal body.
% \begingroup
% \def\thetheorem{\ref{condition:body-parsing}}
% \begin{condition}[Expression Parsing Monotonicity] If $\parseUExp{b}{\ue}$ then $\sizeof{\ue} < \sizeof{b}$.\end{condition}
% \endgroup
% The second condition simply states that subsequences of $b$ are no longer than $b$.
% \begingroup
% \def\thetheorem{\ref{condition:body-subsequences}}
% \begin{condition}[Body Subsequencing] If $\bsubseq{b}{m}{n}=b'$ then $\sizeof{b'} \leq \sizeof{b}$. \end{condition}
% \endgroup

% Combining these two conditions, we have that $\sizeof{\ue} < \sizeof{b}$ as needed.
% \end{proof}

% % We need to define the following theorem about proto-expression validation mutually with Theorem \ref{thm:typed-expansion-U}. 
% % \begin{theorem}[Proto-Expansion Expression Validation]\label{thm:candidate-expansion-validation-U}
% % If $\cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce}{e}{\tau}$ and $\uetsmenv{\Delta_\text{app}}{\uPsi}$ then $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$.
% % \end{theorem}
% % \begin{proof} By rule induction over Rules (\ref{rules:cvalidE-U}).
% % \begin{byCases}
% % \item[\text{(\ref{rule:cvalidE-U-var})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=x$ \BY{assumption}
% %   \item $e=x$ \BY{assumption}
% %   \item $\Gamma=\Gamma', \Ghyp{x}{\tau}$ \BY{assumption}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gamma', \Ghyp{x}{\tau}}{x}{\tau}$ \BY{Rule (\ref{rule:hastypeU-var})} \pflabel{hastypeU}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma', \Ghyp{x}{\tau}}{\Gamma_\text{app}}}{x}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} over $\Gamma_\text{app}$ to \pfref{hastypeU}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\text{(\ref{rule:cvalidE-U-lam})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=\acelam{\ctau_1}{x}{\ce'}$ \BY{assumption}
% %   \item $e=\aelam{\tau_1}{x}{e'}$ \BY{assumption}
% %   \item $\tau=\aparr{\tau_1}{\tau_2}$ \BY{assumption}
% %   \item $\cvalidT{\Delta}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ctau_1}{\tau_1}$ \BY{assumption} \pflabel{cvalidT}
% %   \item $\cvalidE{\Delta}{\Gamma, \Ghyp{x}{\tau_1}}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce'}{e'}{\tau_2}$ \BY{assumption} \pflabel{cvalidE}
% %   \item $\uetsmenv{\Delta_\text{app}}{\uPsi}$ \BY{assumption} \pflabel{uetsmenv}
% %   \item $\istypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\tau_1}$ \BY{Lemma \ref{lemma:candidate-expansion-type-validation} on \pfref{cvalidT}} \pflabel{istype}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma, \Ghyp{x}{\tau_1}}{\Gamma_\text{app}}}{e'}{\tau_2}$ \BY{IH on \pfref{cvalidE} and \pfref{uetsmenv}} \pflabel{hastype1}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}, \Ghyp{x}{\tau_1}}{e'}{\tau_2}$ \BY{exchange over $\Gamma_\text{app}$ on \pfref{hastype1}} \pflabel{hastype2}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aelam{\tau_1}{x}{e'}}{\aparr{\tau_1}{\tau_2}}$ \BY{Rule (\ref{rule:hastypeU-lam}) on \pfref{istype} and \pfref{hastype2}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\text{(\ref{rule:cvalidE-U-ap})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=\aceap{\ce_1}{\ce_2}$ \BY{assumption}
% %   \item $e=\aeap{e_1}{e_2}$ \BY{assumption}
% %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce_1}{e_1}{\aparr{\tau_1}{\tau}}$ \BY{assumption} \pflabel{cvalidE1}
% %   \item $\cvalidE{\Delta}{\Gamma}{\esceneU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{b}}{\ce_2}{e_2}{\tau_1}$ \BY{assumption} \pflabel{cvalidE2}
% %   \item $\uetsmenv{\Delta_\text{app}}{\uPsi}$ \BY{assumption} \pflabel{uetsmenv}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_1}{\aparr{\tau_1}{\tau}}$ \BY{IH on \pfref{cvalidE1} and \pfref{uetsmenv}} \pflabel{hastypeU1}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e_2}{\tau_1}$ \BY{IH on \pfref{cvalidE2} and \pfref{uetsmenv}} \pflabel{hastypeU2}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{\aeap{e_1}{e_2}}{\tau}$ \BY{Rule (\ref{rule:hastypeU-ap}) on \pfref{hastypeU1} and \pfref{hastypeU2}}
% % \end{pfsteps*}
% % \resetpfcounter

% % \item[\VExpof{\text{\ref{rule:hastypeU-tlam}}}~\text{through}~\VExpof{\text{\ref{rule:hastypeU-case}}}] These cases follow analagously, i.e. we apply the IH to all proto-expression validation premises, Lemma \ref{lemma:candidate-expansion-type-validation} to all proto-types validation premises, weakening and exchange as needed, and then apply the corresponding typing rule.
% % \\

% % \item[\text{(\ref{rule:cvalidE-U-splicede})}] ~
% % \begin{pfsteps*}
% %   \item $\ce=\acesplicede{m}{n}$ \BY{assumption}
% %   \item $\parseUExp{\bsubseq{b}{m}{n}}{\ue}$ \BY{assumption}
% %   \item $\expandsU{\Delta_\text{app}}{\Gamma_\text{app}}{\uPsi}{\ue}{e}{\tau}$ \BY{assumption} \pflabel{expands}
% %   \item $\uetsmenv{\Delta_\text{app}}{\uPsi}$ \BY{assumption} \pflabel{uetsmenv}
% %   \item $\hastypeU{\Delta_\text{app}}{\Gamma_\text{app}}{e}{\tau}$ \BY{Theorem \ref{thm:typed-expansion-U} on \pfref{expands} and \pfref{uetsmenv}} \pflabel{hastype}
% %   \item $\hastypeU{\Dcons{\Delta}{\Delta_\text{app}}}{\Gcons{\Gamma}{\Gamma_\text{app}}}{e}{\tau}$ \BY{Lemma \ref{lemma:weakening-U} on \pfref{hastype}}
% % \end{pfsteps*}
% % \resetpfcounter
% % \end{byCases}
% % \end{proof}


% %\qed


% \subsubsection{Abstract Reasoning Principles}\label{sec:uetsms-reasoning-principles}
% The following theorem summarizes the abstract reasoning principles that programmers can rely on when applying an seTLM. In particular, the programmer can be sure that:
% \begin{enumerate} 
% \item \textbf{Segmentation}: The segmentation determined by the proto-expansion actually segments the literal body (i.e. each segment is in-bounds and the segments are non-overlapping.)
% \item \textbf{Typing 1}: The type of the expansion is consistent with the type annotation on the seTLM definition.
% \item \textbf{Typing 2}: Each spliced type has a well-formed expansion at the application site.
% \item \textbf{Typing 3}: Each type annotation on a reference to a spliced expression has a well-formed expansion at the application site.
% \item \textbf{Typing 4}: Each spliced expression has a well-typed expansion consistent with its type annotation.
% \item \textbf{Capture Avoidance}: The final expansion can be decomposed into a  term with variables in place of each spliced type or expression. The expansions of these spliced types and expressions can be substituted into this term in the standard capture avoiding manner.
% \item \textbf{Context Independence}: The decomposed term is indeed well-typed independent of the application site contexts.

% \end{enumerate}

% \begingroup
% \def\thetheorem{\ref{thm:tsc-SES}}
% \begin{theorem}[seTLM Abstract Reasoning Principles]
% If $\expandsU{\uDD{\uD}{\Delta}}{\uGG{\uG}{\Gamma}}{\uPsi}{\utsmap{\tsmv}{b}}{e}{\tau}$ then:
% \begin{enumerate}
% \item (\textbf{Typing 1}) $\uPsi = \uPsi', \uShyp{\tsmv}{a}{\tau}{\eparse}$ and $\hastypeU{\Delta}{\Gamma}{e}{\tau}$
% \item $\encodeBody{b}{\ebody}$
% \item $\evalU{\ap{\eparse}{\ebody}}{\aein{\lbltxt{SuccessE}}{\ecand}}$
% \item $\decodeCondE{\ecand}{\ce}$
% \item (\textbf{Segmentation}) $\segOK{\segof{\ce}}{b}$
% \item $\segof{\ce} = \sseq{\acesplicedt{m'_i}{n'_i}}{\nty} \cup \sseq{\acesplicede{m_i}{n_i}{\ctau_i}}{\nexp}$
% \item \textbf{(Typing 2)} $\sseq{
%       \expandsTU{\uDD{\uD}{\Delta}}
%       {
%         \parseUTypF{\bsubseq{b}{m'_i}{n'_i}}
%       }{\tau'_i}
%     }{\nty}$ and $\sseq{\istypeU{\Delta}{\tau'_i}}{\nty}$
% \item \textbf{(Typing 3)} $\sseq{
%   \cvalidT{\emptyset}{
%     \tsceneUP
%       {\uDD
%         {\uD}{\Delta}
%       }{b}
%   }{
%     \ctau_i
%   }{\tau_i}
% }{\nexp}$ and $\sseq{\istypeU{\Delta}{\tau_i}}{\nexp}$
% \item \textbf{(Typing 4)} $\sseq{
%   \expandsU
%     {\uDD{\uD}{\Delta}}
%     {\uGG{\uG}{\Gamma}}
%     {\uPsi}
%     {\parseUExpF{\bsubseq{b}{m_i}{n_i}}}
%     {e_i}
%     {\tau_i}
% }{\nexp}$ and $\sseq{\hastypeU{\Delta}{\Gamma}{e_i}{\tau_i}}{\nexp}$
% \item (\textbf{Capture Avoidance}) $e = [\sseq{\tau'_i/t_i}{\nty}, \sseq{e_i/x_i}{\nexp}]e'$ for some $\sseq{t_i}{\nty}$ and $\sseq{x_i}{\nexp}$ and $e'$
% \item (\textbf{Context Independence}) $\mathsf{fv}(e') \subset \sseq{t_i}{\nty} \cup \sseq{x_i}{\nexp}$
%   % $\hastypeU
%   % {\sseq{\Dhyp{t_i}}{\nty}}
%   % {\sseq{x_i : \tau_i}{\nexp}}
%   % {e'}{\tau}$
% \end{enumerate}
% \end{theorem}
% \begin{proof} The proof, which involves auxiliary lemmas about the decomposition of proto-types and proto-expressions, is given in Appendix \ref{appendix:SES-reasoning-principles}.
% \end{proof}
% \endgroup

\vspace{-4px}
\newcommand{\pTLMsSec}{Parametric TLMs (pTLMs)}
\section{\protect\pTLMsSec}
\label{sec:ptsms}
\vspace{-2px}
Simple TLMs operate at a single specified type and the proto-expansions they generate must be closed (cf. the final premise of Rule \textsc{ee-ap-setsm} in the Sec. \ref{sec:s-TLM-ap}.) While this suited our purposes in the previous section because we intend $\miniVersePat$ to communicate the essential concepts, it is not realistic for an ML-like language to impose these limitations. This section introduces \emph{parametric TLMs}. Parametric TLMs can be defined over a type- and module-parameterized family of types, and the proto-expansions they generate can refer to supplied type and module parameters.

\vspace{-4px}
\subsection{Parametric TLMs By Example}\label{sec:ptsms-by-example}
\vspace{-2px}

Consider the following Reason module type (a.k.a. signature), which specifies an abstract data type \cite{liskov1974programming,harper1997programming} of string-keyed polymorphic immutable dictionaries:
\begin{lstlisting}[numbers=none]
  module type DICT = {
    type t('a); 
    let empty : t('a); 
    let extend : t('a) -> (string, 'a) -> t('a); 
    /* ... */ 
  };
\end{lstlisting}
Let us now define a TLM that is parametric over all implementations of this signature, \li{D : DICT}, and also over all choices of the range type, \li{'a}:
\begin{lstlisting}[numbers=none]
  syntax $dict (D : DICT) (type 'a) at D.t('a) by static {
    fun(b : body) -> parse_result(proto_expr) => /* ... */
  };
\end{lstlisting}
Assuming some module that implements this signature, \li{HashDict : DICT}, and some values \li{v1 : int} and \li{v2 : int}, we can apply \li{$dict} as follows:
\begin{lstlisting}[numbers=none]
  $dict HashDict int [| "key1"SURL => EURLv1SURL; EURL"key2"SURL => EURLv2 |]
\end{lstlisting}
Notice that the segmentation immediately reveals which punctuation is particular to this TLM and where the spliced key and value expressions appear. Because the context-free syntax of unexpanded terms is never modified, it is possible to reason modularly about syntactic determinism, i.e. we can reason above that \li{=>} does not appear in the follow set of unexpanded terms \cite{conf/pldi/SchwerdfegerW09}, so there can never be an ambiguity about where a key expression ends.

If we will use the \li{HashDict} implementation ubiquitously, we can abbreviate the partial application of \li{$dict} to \li{HashDict}, resulting in a TLM that is parametric over only the type \li{'a}, as follows:
\begin{lstlisting}[numbers=none]
  syntax $dict' = $dict HashDict; 
  $dict' int [| "key1"SURL => EURLv1SURL; EURL"key2"SURL => EURLv2 |];
\end{lstlisting}
The proto-expansion generated for the TLM application above might be:
\begin{lstlisting}[numbers=none]
  D.extend (D.extend D.empty (spliced<1; 6; string>, spliced<11; 12; 'a>)) 
    (spliced<15; 20; string>, spliced<25; 26; 'a>)
\end{lstlisting}
The generated proto-expansion must truly be parametric, i.e. it must be valid for all modules \li{D : DICT} and types \li{'a}. It is only after proto-expansion validation that we substitute the actual parameters, here \li{HashDict} for \li{D} and \li{int} for \li{'a}, into the final expansion. Additionally, substitution occurs on the type annotations on references to spliced terms before recursively expanding those terms (otherwise, the expressions \li{v1} and \li{v2} above would need to be well-typed for all types, \li{'a}.)

Module parameters are also useful for giving the expansion access to helper functions in a context-independent manner. For example, in Sec. \ref{sec:context-dependence} we discussed the problem of applying datatype constructors like \li{H1Element}, because they are introduced into the context as variables in most ML-like languages. With module parameters, it is possible to sidestep this problem by passing in a module containing the datatype constructors. Because this will be common in practice, we provide the shorthand \li{syntax $a at T using t1, (*...*), tn, X1, (*...*), Xm by static \{ ... \}} for the parameterization of \li{$a} by $n$ type variable parameters and $m$ module variable parameters, followed by the partial application of the given parameters. Additionally, for the type variable parameters that refer to datatypes, the constructors are bound automatically in the expansion. This explicit form of capture is reminiscent of work on explicit capture for distributed functions by \citet{DBLP:conf/ecoop/MillerHO14}. Although a bit cumbersome, asking TLM providers to make their intent explicit allows them to avoid having to reason about the computations that the parse function performs on syntax tree representations when refactoring code. Otherwise, proto-expansion representations would need to be built primitively into the binding structure of the language. We want to leave the semantics of OCaml alone when incorporating TLMs into Reason, so this would not be feasible.

It is important to note that module parameters are accessible by expansions, but not by parse functions directly, because the applied module parameters will not have been dynamically instantiated when typed expansion occurs. We will discuss a distinct mechanism for providing helper functions to parse functions in Sec. \ref{sec:static-eval}. 

\vspace{-4px}
\subsection{Parametric TLMs, Formally}
\vspace{-2px}
% \begin{figure}[p]
% \[\begin{array}{llcl}
% \mathsf{UMType} & \urho & ::= 
% %& \autype{\utau} 
% & \utau & \text{type annotation}\\
% % &&
% %& \aualltypes{\ut}{\urho} 
% % & \alltypes{\ut}{\urho} & \text{type parameterization}\\
% &&
% %& \auallmods{\usigma}{\uX}{\urho} 
% & \allmods{\uX}{\usigma}{\urho} & \text{module parameterization}\\
% \mathsf{UMExp} & \uepsilon & ::= 
% %& \abindref{\tsmv} 
% & \tsmv & \text{TLM identifier reference}\\
% % &&
% %& \auabstype{\ut}{\uepsilon} 
% % & \abstype{\ut}{\uepsilon} & \text{type abstraction}\\
% &&
% %& \auabsmod{\usigma}{\uX}{\uepsilon} 
% & \absmod{\uX}{\usigma}{\uepsilon} & \text{module abstraction}\\
% % &&
% %& \auaptype{\utau}{\uepsilon} 
% % & \aptype{\uepsilon}{\utau} & \text{type application}\\
% &&
% %& \auapmod{\uM}{\uepsilon} 
% & \apmod{\uepsilon}{\uX} & \text{module application}\ECC
% \end{array}
% \]
% \caption{Syntax of unexpanded TLM types and expressions.}
% \label{fig:P-macro-expressions-types-u}
% \end{figure}

We will now outline $\miniVerseParam$, a calculus that extends $\miniVersePat$ with parametric TLMs. This calculus is organized, like $\miniVersePat$, as an unexpanded language (UL) defined by typed expansion to an expanded language (XL). There is not enough space to describe $\miniVerseParam$ with the same level of detail as in Sec. \ref{sec:setsms-formally}, so we highlight only the most important concepts below. The details are in the supplement.

The XL consists of 1) module expressions, $M$, classified by signatures, $\sigma$; 2) constructions, $c$, classified by kinds, $\kappa$; and 3) expressions classified by types, which are constructions of kind $\akty$ (we use metavariables $\tau$ instead of $c$ for types by convention.) Metavariables $X$ ranges over module variables and $u$ or $t$ over construction variables. The module and construction languages are based closely on those defined by \citet{pfple1}, which in turn are based on early work by \citet{MacQueen:1984:MSM:800055.802036,DBLP:conf/popl/MacQueen86}, subsequent work on the phase splitting interpretation of modules \cite{harper1989higher} and on using dependent singleton kinds to track type identity \cite{stone2006extensional,DBLP:conf/lfmtp/Crary09}, and finally on formal developments by \citet{dreyer2005understanding} and \citet{conf/popl/LeeCH07}. A complete account of these developments is unfortunately beyond the scope of this paper. The expression language extends the language of $\miniVersePat$ only to allow projection out of modules.

The main conceptual difference between $\miniVersePat$ and $\miniVerseParam$ is that $\miniVerseParam$ introduces the notion of unexpanded and expanded TLM expressions and types, as shown in Figure \ref{fig:P-macro-expressions-types}. 

\begin{figure}[h]
\vspace{-6px}
\small
\begin{minipage}{0.38\textwidth}
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{UMType} & \urho & ::= & \utau ~\vert~ \allmods{\uX}{\usigma}{\urho}\\
\mathsf{UMExp} & \uepsilon & ::= & \tsmv ~\vert~ \absmod{\uX}{\usigma}{\uepsilon}~\vert~ \apmod{\uepsilon}{\uX}
\end{array}$
\end{minipage}
\begin{minipage}{0.6\textwidth}
$\arraycolsep=3pt\begin{array}{llcl}
\mathsf{MType} & \rho & ::= & \aetype{\tau} ~\vert~ \aeallmods{\sigma}{X}{\rho} \\
\mathsf{MExp} & \epsilon & ::= & \adefref{a} ~\vert~ \aeabsmod{\sigma}{X}{\epsilon} ~\vert~\aeapmod{X}{\epsilon} 
\end{array}$
\end{minipage}
\vspace{-10px}
\caption{Syntax of unexpanded and expanded TLM types and expressions in $\miniVerseParam$}
\label{fig:P-macro-expressions-types}
\vspace{-10px}
\end{figure}
The TLM type $\aeallmods{\sigma}{X}{\rho}$ classifies TLM expressions that have one module parameter matching $\sigma$. For simplicity, we formalize only module parameters. Type parameters can be expressed as module parameters having exactly one abstract type member.

The rule governing expression TLM application, reproduced below, touches all of the main ideas in $\miniVerseParam$, so we will refer to it throughout the remainder of this section.%It might be useful to compare this rule to Rule \textsc{ee-ap-sptsm}.
{\small\begin{mathpar}
\inferrule[ee-ap-petsm]{
  \uOmega = \uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}\\
  \uPsi=\uAS{\uA}{\Psi}\\\\
  \tsmexpExpandsExp{\uOmega}{\uPsi}{\uepsilon}{\epsilon}{\aetype{\tau_\text{final}}}\\
  \tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}\\\\
  \tsmdefof{\epsilon_\text{normal}}=a\\
  \Psi = \Psi', \petsmdefn{a}{\rho}{\eparse}\\\\
  \encodeBody{b}{\ebody}\\
  \evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{e_\text{pproto}}}\\
  \decodePCEExp{e_\text{pproto}}{\pce}\\\\
  \prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon_\text{normal}}{\aetype{\tau_\text{proto}}}{\omega}{\Omega_\text{params}}\\\\
  \segOK{\segof{\ce}}{b}\\
  \cvalidEP{\Omega_\text{params}}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\ce}{e}{\tau_\text{proto}}
}{
  \expandsP{\uOmega}{\uPsi}{\uPhi}{\utsmap{\uepsilon}{b}}{[\omega]e}{[\omega]\tau_\text{proto}}
}
\end{mathpar}}

The first two premises simply deconstruct the (unified) unexpanded context $\uOmega$ (which tracks the expansion of expression, constructor and module identifiers, as $\uDelta$ and $\uGamma$ did in $\miniVersePat$) and peTLM context, $\uPsi$. Next, we expand $\uepsilon$ according to straightforward unexpanded peTLM expression expansion rules. The resulting TLM expression, $\epsilon$, must be defined at a type (i.e. no quantification over modules must remain once the literal body is encountered.)

The fourth premise performs \emph{peTLM expression normalization}, $\tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}$. This is defined in terms of a structural operational semantics \cite{DBLP:journals/jlp/Plotkin04a} with two stepping rules:
% \begin{equation*}\tag{\ref{rule:tsmexpEvalsExp}}
% \inferrule{
%   \tsmexpMultistepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}\\
%   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon'}
% }{
%   \tsmexpEvalsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
% }
% \end{equation*}
% where the multistep judgement, $\tsmexpMultistepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}$, is defined as the reflexive, transitive closure of the stepping judgement defined by the following rules:
% \begin{equation*}\tag{\ref{rule:tsmexpStepsExp-aptype-1}}
% \inferrule{
%   \tsmexpStepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
% }{
%   \tsmexpStepsExp{\Omega}{\Psi}{\aeaptype{\tau}{\epsilon}}{\aeaptype{\tau}{\epsilon'}}
% }
% \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpStepsExp-aptype-2}}
% \inferrule{ }{
%   \tsmexpStepsExp{\Omega}{\Psi}{\aeaptype{\tau}{\aeabstype{t}{\epsilon}}}{[\tau/t]\epsilon}
% }
% \end{equation*}
{\small\begin{mathpar}
\inferrule[eps-dyn-apmod-subst-e]{ }{
  \tsmexpStepsExp{\Omega}{\Psi}{\aeapmod{X}{\aeabsmod{\sigma}{X'}{\epsilon}}}{[X/X']\epsilon}
}

\inferrule[eps-dyn-apmod-steps-e]{
  \tsmexpStepsExp{\Omega}{\Psi}{\epsilon}{\epsilon'}
}{
  \tsmexpStepsExp{\Omega}{\Psi}{\aeapmod{X}{\epsilon}}{\aeapmod{X}{\epsilon'}}
}
\end{mathpar}}
% The peTLM expression normal forms are defined as follows:
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-defref}}
% \inferrule{ }{
%   \tsmexpNormalExp{\Omega}{\Psi, \petsmdefn{a}{\rho}{\eparse}}{\adefref{a}}
% }
% \end{equation*}
% % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-abstype}}
% % \inferrule{ }{
% %   \tsmexpNormalExp{\Omega}{\Psi}{\aeabstype{t}{\epsilon}}
% % }
% % \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-absmod}}
% \inferrule{ }{
%   \tsmexpNormalExp{\Omega}{\Psi}{\aeabsmod{\sigma}{X}{\epsilon}}
% }
% \end{equation*}
% % \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-aptype}}
% % \inferrule{
% %   \epsilon \neq \aeabstype{t}{\epsilon'}\\
% %   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon}
% % }{
% %   \tsmexpNormalExp{\Omega}{\Psi}{\aeaptype{\tau}{\epsilon}}
% % }
% % \end{equation*}
% \begin{equation*}\tag{\ref{rule:tsmexpNormalExp-apmod}}
% \inferrule{
%   \epsilon \neq \aeabsmod{\sigma}{X'}{\epsilon'}\\
%   \tsmexpNormalExp{\Omega}{\Psi}{\epsilon}
% }{
%   \tsmexpNormalExp{\Omega}{\Psi}{\aeapmod{X}{\epsilon}}
% }
% \end{equation*}
\vspace{-5px}

Normalization eliminates parameters introduced in higher-order abbreviations, leaving only those parameter applications specified by the original TLM definition. Normal forms and progress and preservation theorems are established in the supplement.

The third row of premises looks up the applied TLM's definition by invoking a simple metafunction to extract its name, $a$, then looking up $a$ within the peTLM definition context, $\Psi$.
% \begin{align}
% \tsmdefof{\adefref{a}} & = a \tag{\ref{eqn:tsmdefof-adefref}}\\
% % \tsmdefof{\aeabstype{t}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-abstype}}\\
% \tsmdefof{\aeabsmod{\sigma}{X}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-absmod}}\\
% % \tsmdefof{\aeaptype{\tau}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-aptype}}\\
% \tsmdefof{\aeapmod{X}{\epsilon}} & = \tsmdefof{\epsilon} \tag{\ref{eqn:tsmdefof-apmod}}
% \end{align}


The fourth row of premises 1) encodes the body as a value of the type $\tBody$; 2) applies the parse function; and 3) decodes the result, producing a \emph{parameterized proto-expression}, $\pce$. Parameterized proto-expressions, $\pce$, are ABTs that serve simply to introduce the parameter bindings into an underlying proto-expression, $\ce$. The syntax of parameterized proto-expressions is given below.

\vspace{-4px}{\small\[\begin{array}{llcl}
% \textbf{Sort} & & & \textbf{Operational Form} & \textbf{Stylized Form} & \textbf{Description}\\
% \LCC \color{Yellow}&\color{Yellow}&\color{Yellow}& \color{Yellow} & \color{Yellow} & \color{Yellow}\\
\mathsf{PPrExp} & \pce & ::= & \apceexp{\ce} ~\vert~ \apcebindmod{X}{\pce}
\end{array}\]}%
\vspace{-6px}%

There must be one binder in $\pce$ for each TLM parameter specified by $a$. (In Reason, we can insert these binders automatically as a convenience.) 

The judgement on the fifth row of Rule \textsc{ee-ap-petsm} then \emph{deparameterizes} $\pce$ by peeling away these binders to produce 1) the underlying proto-expression, $\ce$, with the variables that stand for the parameters free; 2) a corresponding deparameterized type, $\tau_\text{proto}$, that uses the same free variables to stand for the parameters; 3) a \emph{substitution}, $\omega$, that pairs the applied parameters from $\epsilon_\text{normal}$ with the corresponding variables generated when peeling away the binders in $\pce$; and 4) a corresponding \emph{parameter context}, $\Omega_\text{params}$, that tracks the signatures of these variables. The two rules governing the proto-expression deparameterization judgement are below:
{\small\begin{mathpar}
\inferrule{ }{
  \prepce{\Omega_\text{app}}{\Psi, \petsmdefn{a}{\rho}{\eparse}}{\apceexp{\ce}}{\ce}{\adefref{a}}{\rho}{\emptyset}{\emptyset}
}

\vspace{-4px}\inferrule{
  \prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon}{\aeallmods{\sigma}{X}{\rho}}{\omega}{\Omega}\\
  X \notin \domof{\Omega_\text{app}}
}{
  \prepce{\Omega_\text{app}}{\Psi}{\apcebindmod{X}{\pce}}{\ce}{\aeapmod{X'}{\epsilon}}{\rho}{(\omega, X'/X)}{(\Omega, X : \sigma)}
}
\end{mathpar}}%
This judgement can be pronounced ``when applying peTLM $\epsilon$, $\pce$ has deparameterization $\ce$ leaving $\rho$ with parameter substitution $\omega$''. 
Notice based on the second rule that every module binding in $\pce$ must pair with a corresponding module parameter application. Moreover, the variables standing for parameters must not appear in $\Omega_\text{app}$, i.e. $\domof{\Omega_\text{params}}$ must be disjoint from $\domof{\Omega_\text{app}}$ (this requirement can always be discharged by alpha-variation.)

The final row of premises checks that the segmentation of $\ce$ is valid and  performs proto-expansion validation under the parameter context, $\Omega_\text{param}$ (rather than the empty context, as was the case in $\miniVersePat$.) The conclusion of the rule applies the parameter substitution, $\omega$, to the resulting expression and the deparameterized type.

Proto-expansion validation operates conceptually as in $\miniVersePat$. The only subtlety has to do with the type annotations on references to spliced terms. As described at the end of Sec. \ref{sec:ptsms-by-example}, these annotations might refer to the parameters, so the parameter substitution, $\omega$, which is tracked by the splicing scene, must be applied to the type annotation before proceeding recursively to expand the referenced unexpanded term. However, the spliced term itself must treat parameters parametrically, so the substitution is not applied in the conclusion of the following rule:
\begin{mathpar}\label{rule:cvalidE-P-splicede}
\inferrule{
  \parseUExp{\bsubseq{b}{m}{n}}{\ue}\\
    \cvalidC{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\ctau}{\tau}{\akty}\\
  \expandsP{\uOmega}{\uPsi}{\uPhi}{\ue}{e}{[\omega]\tau}\\\\
  \uOmega=\uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}\\
  \domof{\Omega} \cap \domof{\Omega_\text{app}} = \emptyset
}{
  \cvalidEP{\Omega}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\acesplicede{m}{n}{\ctau}}{e}{\tau}
}
\end{mathpar}
(This is only sensible because we maintain the invariant that $\Omega$ is always an extension of $\Omega_\text{params}$.)

The calculus enjoys metatheoretic properties analagous to those described in Sec. \ref{sec:s-metatheory}, modified to account for the presence of modules, kinds and parameterization. The following theorem establishes the abstract reasoning principles available when applying a parametric expression TLM. The clauses are directly analagous to those of Theorem \ref{thm:setlm-reasoning}, so for reasons of space we do not repeat the inline descriptions. The \textbf{Kinding} clauses can be understood by analogy to the \textbf{Typing} clauses. The details of parametric pattern TLMs (ppTLMs) are analagous (see supplement.)
\vspace{-3px}
\begin{theorem}[peTLM Reasoning Principles]
If $\expandsP{\uOmega}{\uPsi}{\uPhi}{\utsmap{\uepsilon}{b}}{e}{\tau}$ then:
\begin{enumerate}[nolistsep,noitemsep]
  \item $\uOmega=\uOmegaEx{\uD}{\uG}{\uMctx}{\Omega_\text{app}}$
  \item $\uPsi=\uAS{\uA}{\Psi}$
  \item (\textbf{Typing 1}) $\tsmexpExpandsExp{\uOmega}{\uPsi}{\uepsilon}{\epsilon}{\aetype{\tau}}$ and $\hastypeP{\Omega_\text{app}}{e}{\tau}$
  \item $\tsmexpEvalsExp{\Omega_\text{app}}{\Psi}{\epsilon}{\epsilon_\text{normal}}$
  \item $\tsmdefof{\epsilon_\text{normal}}=a$
  \item $\Psi = \Psi', \petsmdefn{a}{\rho}{\eparse}$
  \item $\encodeBody{b}{\ebody}$
    \item $\evalU{\ap{\eparse}{\ebody}}{\aein{\mathtt{SuccessE}}{e_\text{pproto}}}$
  \item $\decodePCEExp{e_\text{pproto}}{\pce}$
  \item $\prepce{\Omega_\text{app}}{\Psi}{\pce}{\ce}{\epsilon_\text{normal}}{\aetype{\tau_\text{proto}}}{\omega}{\Omega_\text{params}}$
  \item (\textbf{Segmentation}) $\segOK{\segof{\ce}}{b}$
  \item $\cvalidEP{\Omega_\text{params}}{\esceneP{\omega : \OParams}{\uOmega}{\uPsi}{\uPhi}{b}}{\ce}{e'}{\tau_\text{proto}}$
  \item $e = [\omega]e'$
  \item $\tau = [\omega]\tau_\text{proto}$
  \item $
    \segof{\ce} = \sseq{\acesplicedk{m_i}{n_i}}{\nkind} \cup \sseq{\acesplicedc{m'_i}{n'_i}{\cekappa'_i}}{\ncon} \cup $ \\
     ~~~~$          \sseq{\acesplicede{m''_i}{n''_i}{\ctau_i}}{\nexp}
    $
  \item (\textbf{Kinding 1}) $\sseq{\kExpands{\uOmega}{\parseUKindF{\bsubseq{b}{m_i}{n_i}}}{\kappa_i}}{\nkind}$ and \\ ~~~~$\sseq{\iskind{\Omega_\text{app}}{\kappa_i}}{\nkind}$
  \item (\textbf{Kinding 2}) $\sseq{\cvalidK{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\cekappa'_i}{\kappa'_i}}{\ncon}$ and $\sseq{\iskind{\Omega_\text{app}}{[\omega]\kappa'_i}}{\ncon}$
  \item (\textbf{Kinding 3}) $\sseq{\cExpands{\uOmega}{\parseUConF{\bsubseq{b}{m'_i}{n'_i}}}{c_i}{[\omega]\kappa'_i}}{\ncon}$ and $\sseq{\haskind{\Omega_\text{app}}{c_i}{[\omega]\kappa'_i}}{\ncon}$
  \item (\textbf{Kinding 4}) $\sseq{\cvalidC{\OParams}{\csceneP{\omega : \OParams}{\uOmega}{b}}{\ctau_i}{\tau_i}{\akty}}{\nexp}$ and $\sseq{\haskind{\Omega_\text{app}}{[\omega]\tau_i}{\akty}}{\nexp}$
  \item (\textbf{Typing 2}) $\sseq{\expandsP{\uOmega}{\uPsi}{\uPhi}{\parseUExpF{\bsubseq{b}{m''_i}{n''_i}}}{e_i}{[\omega]\tau_i}}{\nexp}$ and $\sseq{\hastypeP{\Omega_\text{app}}{e_i}{[\omega]\tau_i}}{\nexp}$
  \item (\textbf{Capture Avoidance}) $e = [\sseq{\kappa_i/k_i}{\nkind}, \sseq{c_i/u_i}{\ncon}, \sseq{e_i/x_i}{\nexp}, \omega]e''$ for some $e''$ and fresh $\sseq{k_i}{\nkind}$ and fresh $\sseq{u_i}{\ncon}$ and fresh $\sseq{x_i}{\nexp}$
  \item (\textbf{Context Independence}) $\mathsf{fv}(e'') \subset \sseq{k_i}{\nkind} \cup \sseq{u_i}{\ncon} \cup \sseq{x_i}{\nexp} \cup \domof{\OParams}$
  % $\hastypeP{\sseq{\Khyp{k_i}}{\nkind} \cup \sseq{u_i :: [\omega]\kappa'_i}{\ncon} \cup \sseq{x_i : [\omega]\tau_i}{\nexp}}{[\omega]e''}{\tau}$\todo{maybe restate this in terms of free variables of e'' here and elsewhere, because context isn't technically well-formed here?}
\end{enumerate}
\end{theorem}

\vspace{-6px}
\newcommand{\staticEvalSec}{Static Evaluation}
\section{\protect\staticEvalSec}
\label{sec:static-eval}
\vspace{-2px}

There is one more major impracticality that we can now address. So far, we have assumed that parse functions are closed expanded expressions. This assumption simplifies matters formally, but in practice it is quite difficult, because this means that TLM definitions cannot themselves access any libraries nor use TLMs internally. To address this problem, we introduce a \emph{static environment}. 

\vspace{-4px}
\subsection{The Static Environment}
\vspace{-2px}
Figure \ref{fig:static-module-example} shows an example of a module, \li{ParserCombos}, that defines a number of parser combinators following \citet{Hutton1992d}. The \li{static} qualifier indicates that this module is bound for use only within similarly qualified values, including in particular parse functions of subsequent TLM definitions.
\begin{figure}[h]
\vspace{-10px}
\begin{lstlisting}
  static module ParserCombos = { 
    type parser('c, 't) = list('c) -> list('t, list('c));
    let alt : parser('c, 't) -> parser('c, 't) -> parser('c, 't);
    /* ... */
  };
  syntax $a at t by static { fun(b) => ParserCombos.alt /* OK ... */ };
  static val y = ParserCombos.alt /* OK ... */;
  val z = /* ... neither ParserCombos nor y are available here */;
\end{lstlisting}
\vspace{-10px}
\caption{Binding static modules and values for use within parse functions.}
\label{fig:static-module-example}
\vspace{-10px}
\end{figure}

The values that arise when the static phase runs do not persist from ``compile-time'' to ``run-time'', so we do not need a full staged computation system, e.g. as described by \citet{Taha99multi-stageprogramming:}. Instead, a sequence of static bindings operates like a read-evaluate-print loop (REPL) scoped according to the program structure, in that each static expression is evaluated immediately and the evaluated values are tracked by a \emph{static environment}. The static environment is discarded after typed expansion.

A language designer might choose to restrict the external effects available to static terms in some way, e.g. to ensure deterministic builds. It might also be helpful to restrict mutable state shared between TLMs to prevent undesirable TLM application order dependencies. On the other hand, these features, if used for the purposes of caching, might speed up typed expansion. These are orthogonal design decisions.

\vspace{-6px}
\subsection{Applying TLMs Within TLM Definitions}\label{sec:tsms-for-tsms}
TLMs and TLM abbreviations can also be qualified with the \li{static} keyword, which marks them for use within subsequent static expressions and patterns. Let us consider some examples of relevance to TLM providers.

\vspace{-6px}
\subsubsection{Quasiquotation}
TLMs must construct values of type \li{proto_expr} or \li{proto_pat}. Constructing values of these types explicitly can have high syntactic cost. To decrease this cost, we can define TLMs that provide support for \emph{quasiquotation syntax} similar to that built in to languages like Lisp \cite{Bawd99a} and Scala \cite{shabalin2013quasiquotes}. The following TLM defines quasi-quotation for encodings of proto-expressions:
\begin{lstlisting}[numbers=none]
  static syntax $proto_expr at proto_expr by static { /* ... */ };
\end{lstlisting}
For example, \li{$proto_expr `SQTg x ^EQTx`} might have expansion \li{App(App(Var "g", Var "x"), x)}. Notice that prefixing a variable (or parenthesized expression) with \li{^} serves to splice in its value, which here must be of type \li{proto_expr} (though in other syntactic positions, it might be \li{proto_typ}.) This is also known as \emph{anti-quotation}.

% A similar approach can be taken for working with encodings of terms of other languages (e.g. when writing an interpretter or compiler in Reason.)

\vspace{-6px}
\subsubsection{Parser Generators}
Abstractly, a grammar-based parser generator is a module matching the signature \li{PARSEGEN} defined below:
\begin{lstlisting}
  module type PARSEGEN = { 
    type grammar('a);
    /* ... operations on grammars ... */
    val generate : grammar('a) -> (body -> parse_result('a));
  };
\end{lstlisting}

Rather than constructing a grammar equipped with semantic actions using the associated operations (whose specifications are elided in \li{PARSEGEN}), we wish to use a syntax for context-free grammars that follows standard conventions. We can do so by defining a static parametric TLM:
\begin{lstlisting}[numbers=none]
  static syntax $grammar(P : PARSEGEN)(type 'a) at P.grammar('a) /*...*/
\end{lstlisting}
To support splicing, we need non-terminals that recognize unexpanded terms and produce the corresponding splice references, rather than the AST itself. This requires that the parser generator keep track of location information (as most production-grade parser generators already do for error reporting.) A more detailed example of this mechanism being used to define a TLM for a modular encoding of regular expressions is given in the supplement.% For spliced expressions, this non-terminal would need to be a family of non-terminals indexed by a value of type \li{proto_typ}. 

A grammar containing such non-terminals can serve as a \emph{summary specification} -- a human can simply be take this grammar as a specification of what the segmentation will be for every recognized string, rather than relying on an editor to communicate this information. The associated semantic actions can be held abstract as long as the system performs a simple check to ensure that the proto-expansion does mention each spliced expression from the corresponding production.

% \begin{figure}[h!]
% \vspace{-5px}
% \begin{lstlisting}[deletekeywords={as}]
% syntax $rx(R : RX) at R.t by static 
%   P.generate ($grammar P proto_expr {|SHTML #\label{line:rx_parse_fn_start}#
%     start <- ""
%       EHTMLfn () => $proto_expr `SCSSR.EmptyECSS`SHTML
%     start <- "(" start ")"
%       EHTMLfn e => eSHTML
%     token str_tok #\label{line:str_tok_start}#
%       EHTMLRU.parse "SSTR[^(@$]+ESTR" /* cannot use $rx within its own def */SHTML #\label{line:str_tok_end}#
%     start <- str_tok
%       EHTMLfn s => $proto_expr `SCSSR.Str %(ECSSstr_to_proto_lit sSCSS)ECSS`SHTML
%     start <- start start
%       EHTMLfn e1 e2 => $proto_expr `SCSSR.Seq (%ECSSe1SCSS, %ECSSe2SCSS)ECSS`SHTML
%     start <- start "|" start 
%       EHTMLfn e1 e2 => $proto_expr `SCSSR.Or (%ECSSe1SCSS, %ECSSe2SCSS)ECSS`SHTML
%     start <- start "*"
%       EHTMLfn e => $proto_expr `SCSSR.Star %ECSSe`SHTML

%     using EHTMLspliced_uexp ($proto_typ `SCSSR.tECSS`) SHTML as spliced_rx #\label{line:splicede_using}#
%     start <- "${" spliced_rx "}" #\label{line:splicing-start}#
%       EHTMLfn e => eSHTML

%     using EHTMLspliced_uexp ($proto_typ `SCSSstringECSS`) SHTML as spliced_str
%     start <- "@{" spliced_str "}"
%       EHTMLfn e => $proto_expr `SCSSR.Str %(ECSSeSCSS)ECSS`SHTML #\label{line:splicing-end}#
%   EHTML|})
% end #\label{line:rx_parse_fn_end}#
% \end{lstlisting}
% \vspace{-12px}
% \caption{A grammar-based definition of \texttt{\$rx}.}
% \vspace{-15px}
% \label{fig:rx-grammar-based}
% \end{figure}

\vspace{-6px}
\subsection{Static Evaluation, Formally}
It is not difficult to extend $\miniVerseParam$ to account for static evaluation. Static environments, $\Sigma$, take the form $\staticenv{\omega}{\uOmega}{\uPsi}{\uPhi}$, where $\omega$ is a substitution. Each binding form is annotated with a \emph{phase}, $\phi$, either $\staticphase$ or $\standardphase$. The rules for binding forms annotated with $\standardphase$ are essentially unchanged, differing only in that $\Sigma$ passes through opaquely. The rules for binding forms annotated with $\staticphase$ are based on the corresponding $\standardphase$ phase rules, differing only in that 1) they operate on $\Sigma$ and 2) evaluation occurs immediately. Finally, the forms for TLM definition are modified so that the parse function is now an unexpanded, rather than an expanded expression. The substitution $\omega$ is applied to the parse function after it is expanded. The full details are defined as a small patch of $\miniVerseParam$ called $\miniVersePH$ in the supplement.

\vspace{-6px}
\subsection{Library Management}
In the examples above, and in our formal treatment, we explicitly qualified various definitions with the \li{static} keyword to make them available within static values. In practice, we would like to be able to use libraries within both static values and standard values as needed without duplicating code. This can be accomplished either by the package manager (e.g. SML/NJ's CM \cite{blume:smlnj-cm}, extended with phase annotations) or by allowing one to explicitly lower an instance of a module define in the standard phase for use also in  the static phase, as in a recent proposal for modular staging macros in OCaml \cite{Ocaml/macros}.

TLMs definitions can be exported from the top level of packages, but they cannot be exported from within ML-style modules because that would require that they also appear in signatures, and that, in turn, would complicate reasoning about signature equivalence, since TLM definitions contain arbitrary parse functions. 
%It would also bring in confusion about whether the generated expansions can use private knowledge about type identity. 
That said, it should be possible to export TLM \emph{abbreviations} from modules, since they refer to TLM definitions only through symbolic names. We have not yet formalized this intuition, but the work of \citet{culpepper2005syntactic,culpepper2007advanced} considered a closely related question: how should Typed Scheme's macros interact with its unit (i.e. package) system.



% For example, a language-external library manager for Reason similar to SML/NJ's CM \cite{blume:smlnj-cm} could support a \li{static} qualifier on imported libraries, which would place the definitions exported by the imported library into the static phase of the library being defined. In particular, a library definition in such a compilation manager might look like this:
% \begin{lstlisting}[numbers=none,morekeywords={Library,is}]
% Library 
%   /* ... exported module, signature and TLM names ... */
% is 
%   /* ... files defining those exports ... */

%   /* imports: */
%   static parsegen.cm 
% \end{lstlisting}

% A similar approach could be taken for languages the incorporate library management directly into the syntax of programs, e.g. Scala \cite{odersky2008programming}:
% \begin{lstlisting}[numbers=none]
% static import edu.cmu.comar.parsegen
% \end{lstlisting}


% \begin{equation}\label{rule:mExpandsPH-syntaxpe-standard}
% \inferrule{
%   \tsmtyExpands{\uOmega}{\urho}{\rho}\\
%   \Sigma = \staticenv{\omega}{\uOmega_S}{\uPsi_S}{\uPhi_S}\\\\
%   \expandsP{\uOmega_S}{\uPsi_S}{\uPhi_S}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultPCEExp}}\\\\
%   \evalU{[\omega]\eparse}{\eparse'}\\
%   \mExpandsPH{\uOmega}{\uAS{\uA \uplus \mapitem{\tsmv}{\adefref{a}}}{\Psi, \petsmdefn{a}{\rho}{\eparse'}}}{\uPhi}{\uM}{M}{\sigma}{\Sigma}
% }{
%   \mExpandsPH{\uOmega}{\uAS{\uA}{\Psi}}{\uPhi}{\defpetsmH{\standardphase}{\tsmv}{\urho}{\ueparse}{\uM}}{M}{\sigma}{\Sigma}
% }
% \end{equation}
% \begin{equation}\label{rule:mExpandsPH-syntaxpe-static}
% \inferrule{
%   \tsmtyExpands{\uOmega}{\urho}{\rho}\\
%   \Sigma = \staticenv{\omega}{\uOmega_S}{\uPsi_S}{\uPhi_S}\\
%   \uPsi_S = \uAS{\uA_S}{\Psi_S}\\\\
%   \expandsP{\uOmega_S}{\uPsi_S}{\uPhi_S}{\ueparse}{\eparse}{\aparr{\tBody}{\tParseResultPCEExp}}\\\\
%   \evalU{[\omega]\eparse}{\eparse'}\\
%   \mExpandsPH{\uOmega}{\uPsi}{\uPhi}{\uM}{M}{\sigma}{\staticenv{\omega}{\uOmega_S}{\uAS{\uA_S \uplus \mapitem{\tsmv}{\adefref{a}}}{\Psi_S, \petsmdefn{a}{\rho}{\eparse'}}}{\uPhi_S}}
% }{
%   \mExpandsPH{\uOmega}{\uPsi}{\uPhi}{\defpetsmH{\staticphase}{\tsmv}{\urho}{\ueparse}{\uM}}{M}{\sigma}{\Sigma}
% }
% \end{equation}

\vspace{-6px}
\newcommand{\rwSec}{Related Work}
\section{\protect\rwSec}
\label{sec:existing-approaches}
\subsection{Syntax Definition Systems}\label{sec:syntax-dialects-intro}
One approach available to library providers seeking to introduce new literal forms is to use a syntax definition system to construct a library-specific \emph{syntax dialect}: a new syntax definition that extends the syntax definition given in the language definition with new forms, including literal forms. For example, Ur/Web's syntax (Figure 1) is a library-specific dialect of Ur's syntax \cite{conf/pldi/Chlipala10,conf/popl/Chlipala15}.

 % Such dialects are sometimes qualitatively taxonomized as amongst the ``domain-specific language'' for this reason \cite{fowler2010domain}. %Syntactic cost is often assessed qualitatively \cite{green1996usability}, though quantitative metrics can be defined. 
% Syntax definition systems , which we will discuss in Sec. \ref{sec:syntax-dialects}, have simplified the task of defining ``library-specific'' (a.k.a. ``domain-specific'') syntax dialects like Ur/Web, and have thereby contributed to their ongoing proliferation.

% Many have argued that a proliferation of syntax dialects constructed using these tools is harmless or even desirable, because programmers can simply choose the right syntax dialect for each job at hand \cite{journals/stp/Ward94}. However, we argue that in fact 
There are hundreds of syntax definition systems of various design. Notable examples include grammar-oriented systems like Camlp4 \cite{ocaml-manual}, Copper \cite{conf/gpce/WykS07,conf/pldi/SchwerdfegerW09}, SugarJ/Sugar*/SoundExt \cite{erdweg2011sugarj,erdweg2013framework,conf/icfp/LorenzenE13,conf/popl/LorenzenE16}, as well as various parser combinator systems \cite{Hutton1992d}. The parsers generated by these systems can be invoked to preprocess program text in various ways, e.g. by invoking them from within a build script, by using a preprocessor-aware build system (e.g. \li{ocamlbuild}), or via language-integrated preprocessing directives (e.g. Racket's \li{#lang} directive or its reader macros \cite{Flatt:2012:CLR:2063176.2063195}, or the import mechanism of SugarJ/Sugar*/SoundExt.)
%library-specific (a.k.a. ``domain-specific'') 

The first problem with using a syntax definition system to directly extend the context-free syntax of a language is that clients cannot always deterministically combine the resulting extended syntax dialects when they want to use the new forms that they define together, i.e. there can be syntactic conflicts. This is a significant problem because large programs use many separately developed libraries \cite{DBLP:conf/sac/LammelPS11} and often do not fall cleanly into a single ``problem domain''.

One possible solution is found in Copper, which integrates a modular grammar analysis that guarantees that determinism is conserved when syntax dialects of a certain restricted class are combined \citet{conf/pldi/SchwerdfegerW09}. The caveat is that the constituent dialects must prefix all newly introduced forms with marking tokens drawn from disjoint sets. To be confident that the marking tokens used are disjoint, providers must base them on the domain name system or some other coordinating entity. Because the mechanism operates at the level of the context-free grammar, it is difficult for the client to define scoped abbreviations for these verbose marking tokens.

TLMs sidestep these complexities entirely because the context-free syntax of the language is fixed, and composition is via splicing rather than direct combination. The TLM provider can therefore modularly establish that the syntax definition that the TLM implements (possibly using a syntax definition system internally, as discussed in Sec. \ref{sec:static-eval}) is deterministic. There is no need for verbose marking tokens, and programmers can define scoped TLM abbreviations as discussed in Sec. \ref{sec:ptsms}.

% In some cases, the dialects in question cannot be combined simply because they have been defined under incompatible syntax definition systems. In other cases, the problem is one of \emph{conflict}: two dialects might define the same form, but determine different desugarings. For example, consider two syntax dialects defined under a system like Camlp4: $\mathcal{D}_1$ defines literal forms for sets, and $\mathcal{D}_2$ defines literal forms for finite sets, both delimited by \verb~{<~ and \verb~>}~. Each dialect is deterministic (i.e. there are no possible syntactic ambiguities), denoted $\mathrm{det}(\mathcal{D}_1)$ and $\mathrm{det}(\mathcal{D}_2)$. However, when the grammars are combined by Camlp4, it is not the case that $\mathrm{det}(\mathcal{D}_1 \cup \mathcal{D}_2)$ because \verb~{<>}~ can desugar to the empty set or the empty dictionary. Reasoning about determinism is therefore not modular. %(Indeed, this ambiguity arose when Python added set literals to its grammar.) %A recent version of Python added derived forms for mutable sets. Due to a conflict with dictionary syntax, however, there is no derived form for the empty set.)
 %A third syntax dialect might come along that uses the same forms that $\mathcal{D}_2$ defines, but for ordered finite maps.


Even putting aside the problem of syntactic conflict, there are questions about just how reasonable sprinkling many possibly unfamiliar library-specific literal forms  throughout a program may be. The intuition was given by example in Sec. \ref{sec:intro}. To reiterate, more generally:
\begin{enumerate}[noitemsep,nolistsep]
\item \textbf{Responsibility}: Clients using a combined syntax dialect cannot easily determine which constituent extension is responsible for a given form, whereas TLM clients can follow the binding structure of the language in the usual manner.
\item \textbf{Segmentation}: Clients of a syntax dialect cannot accurately determine which segments of the program text appear directly in the desugaring. In contrast, TLM clients can inspect the inferred segmentation (communicated via secondary notation.)
\item \textbf{Binding}: Clients of a context-free syntax dialect cannot be sure that the desugaring is context-independent and that spliced terms are capture avoiding, as TLM clients can.
\item \textbf{Typing}: Clients of a syntax dialect cannot reason abstractly about types. In contrast, TLM clients can determine the type of any expansion by referring to the parameter and type declarations on the TLM definition, and nothing else. The inferred segmentation also gives types for each spliced expression or pattern. %This information can be communicated upon request by the type inspection service of a program editor.
\end{enumerate}

The work of \citet{conf/icfp/LorenzenE13,conf/popl/LorenzenE16} introduces SoundExt, a grammar-based syntax extension system where extension providers can equip their new forms with derived typing rules. The system then attempts to automatically verify that the expansion logic (expressed using a rewrite system, rather than an arbitrary function) is sound relative to these derived rules. TLMs differ in several ways. First, as already discussed, we leave the context-free syntax fixed, so different TLMs cannot conflict. Second, SoundExt does not enforce hygiene, i.e. expansions might depend on the context and intentionally induce capture. Similarly, there is no abstract segmentation discipline. While the client can indirectly reason about binding (but not segmentation) by inspecting the derived typing rules, this is weaker than the strict hygiene and segmentation discipline that TLMs enforce. Another distinction is that TLMs do not support type-dependent expansions. The trade-off is that this allows expansions, and therefore segmentations, to be generated even when the program is ill-typed, as long as the static values are well-typed (this notion of \emph{untyped expansion} was not considered formally above, but it follows straightforwardly from the fact that all of the premises to the TLM application rules except the final proto-expansion validation premises are independent of the typing context.) Another important distinction is that TLMs rely on proto-expansion validation, rather than verification as in SoundExt. The trade-off is that TLMs do not require that the expansion logic be written using a restricted rewriting system, nor does the system require a fully mechanized language definition and heavyweight theorem proving machinery. Finally, it would be difficult to define syntax extensions in SoundExt that operate over module-parameterized families of types. Parameters can be approximated using splicing, but there is no clear notion of ``partial application''.



%In other words, encountering an unfamiliar derived form has made it difficult for the programmer to maintain the usual \emph{type discipline} and \emph{binding discipline}. %Compelling the programmer to examine the desugaring directly defeat the purpose of defining the derived form -- decreasing cognitive cost. Indeed, it substantially increases cognitive cost.

% In contrast, when a programmer encounters, for example, a function call like the call to \li{read_data} on Line 3, the analagous questions can be answered by following clear protocols that become ``cognitive reflexes'' after sufficient experience with the language, even if the programmer has no experience with the library defining \li{read_data}:
% \begin{enumerate}
% \item The language's syntax definition determines that \li{read_data(a)} is an expression of function application form.
% \item Similarly, \li{read_data} and \li{a} are definitively expressions of variable form.
% \item The variable \li{a} can only refer to the binding of \li{a} on Line 1.
% \item The variable \li{w} can be renamed without knowing anything about the values that \li{read_data} and \li{a} stand for.
% \item The type of \li{x} can be determined to be \li{B} by determining that the type of \li{read_data} is \li{A -> B} for some \li{A} and \li{B}, and checking that \li{a} has type \li{A}. Nothing else needs to be known about the values that \li{read_data} and \li{a} stand for. In Reynolds' words \cite{B304}:
% \begin{quote}
% \emph{Type structure is a syntactic discipline for enforcing levels of abstraction.}
% \end{quote}
% \end{enumerate}

\vspace{-6px}
\subsection{Term Rewriting Systems}
\vspace{-2px}
Another approach -- and the approach that TLMs are rooted in -- is to leave the context-free syntax of the language fixed, and instead contextually rewrite existing literal forms.

For example, OCaml's textual syntax now includes \emph{preprocessor extension (ppx) points} used to identify terms that some term rewriting preprocessor must rewrite \cite{ocaml-manual}. We could  mark a string literal containing Ur/Web-style HTML syntax with a ppx annotation, \li{xml}, as follows:
\begin{lstlisting}[numbers=none]
  [%xml "SSTR<h1>Hello, {[first_name]}!</h1>ESTR"]
\end{lstlisting}
There are problems reasoning modularly and abstractly about such constructions. More than one applied preprocessor might recognize this annotation (there are, in practice, many XML/HTML libraries), so the problems of \textbf{Responsibility} come up. It is also impossible to reason abstractly about \textbf{Segmentation}, \textbf{Capture}, \textbf{Context Dependence} and \textbf{Typing} because the code that the preprocessor generates is unconstrained.% For these reasons, users of this mechanism warn that they should be used sparingly.\todo{citation}

Term-rewriting macro systems are language-integrated local term rewriting systems that require that the client explicitly apply the intended rewriting, implemented by a macro, to the term that is to be rewritten. This addresses the issue of \textbf{Responsibility}. However, unhygienic, untyped macro systems, like the earliest variants of the Lisp macro system \cite{Hart63a}, Template Haskell \cite{SheardPeytonJones:Haskell-02} and GHC's quasiquotation system \cite{mainland2007s} (which is based on Template Haskell), do not allow clients to reason abstractly about the remaining issues, again because the expansion that they produce is unconstrained. (It is not enough that with Template Haskell / GHC quasiquotation, the generated expansion is typechecked -- to satisfy the \textbf{Typing} criterion, it must be possible to reason abstractly about \emph{what the type of the generated expansion is}.)  

\emph{Hygienic} macro systems prevent, or abstractly account for \cite{DBLP:conf/esop/HermanW08,Herman10:Theory}, \textbf{Capture}, and they enforce application-site \textbf{Context Independence} \cite{Kohlbecker86a,DBLP:conf/popl/Adams15,DBLP:conf/popl/ClingerR91,DBLP:journals/lisp/DybvigHB92}. However, this turns out to make it impossible to repurpose string literal forms to introduce compositional literal forms at other types. Consider again our running XHTML example, which we might try to realize by applying a hygienic macro, \li{xml!}, to a string literal that the macro parses:
\begin{lstlisting}[numbers=none]
  (xml! "SSTR<h1>Hello, {[first_name]}!</h1>ESTR")
\end{lstlisting}
The expansion of this macro will fail the check for context independence because \li{first_name} will appear as a free variable, no different from any other. The hygiene mechanism for TLMs addresses this problem by explicitly distinguishing spliced segments of the literal body in the proto-expansion. This also addresses the problem of \textbf{Segmentation} -- the segmentation abstractly communicates the fact that (only) \li{first_name} is a spliced expression of type \li{string}.

Much of the research on macro systems has been for languages in the LISP tradition \cite{mccarthy1978history} that do not have rich static type structure. The formal macro calculus studied by \citet{DBLP:conf/esop/HermanW08} uses types to encode the binding structure of the generated expansion (in particular, to indicate which identifiers passed as arguments can be captured by which other arguments), but the expanded language itself does not have rich type structure (nor is this calculus capable of expressing new literal forms, for the reasons already discussed.) Research on typed \emph{staging macro systems} like MetaML \cite{Sheard:1999:UMS} and MacroML \cite{ganz2001macros} is also not directly applicable to the problem of defining new literal forms -- the syntax tree of the arguments cannot be inspected at all (staging macros are useful mainly for performance reasons.) The Scala macro system is a hygienic macro system which supports ``black-box'' term rewriting macros. These support reasoning abstractly about \textbf{Typing} because type annotations constrain the macro arguments and the generated expansions, though the precise reasoning principles available are unclear because the Scala macro system has not been formally specified. In any case, black-box macros also cannot be used to repurpose string literal forms because they are hygienic.

  Some languages, including Scala \cite{odersky2008programming}, build in \emph{string splicing} (a.k.a. \emph{string interpolation}) forms, or similar but more general \emph{fragmentary quotation forms} \cite{conf/icfp/Slind91}, e.g. the SML/NJ dialect of ML. These designate a particular delimiter to escape out into the expression language. The problem with using these together with macros as vehicles to introduce literal forms at various other types is 1) there is no ``one-size-fits-all'' escape delimiter, and 2) typing is problematic because every escaped term is checked against the same type. In the XHTML example, we have splicing at two different types using two different delimiters. These forms are also not available in pattern position.%In general, e.g. when defining syntax for a programming language with many sorts of terms, the most appropriate choice of delimiters might depend on where each spliced term appears.


This brings us to the most closely related work, that of \citet{TSLs} on \emph{type-specific languages} (TSLs). Like simple expression TLMs (Sec. \ref{sec:setsms}), TSLs allow library providers to programmatically control the parsing of expressions of generalized literal form. Parse functions are associated directly with nominal types and invoked according to a bidirectionally typed protocol. In contrast, TLMs are separately defined and explicitly applied. Accordingly, different TLMs can be defined at the same type without conflict, and can operate at any type, not just at nominal (i.e. abstract) types. In a subsequent short paper, \citet{sac15} proposed explicit application of simple expression TLMs (there called \emph{typed syntax macros}) in a bidirectional typed setting \cite{Pierce:2000:LTI:345099.345100}, but this short proposal did not give any formal details. With TLMs, it is not necessary for the language to be bidirectionally typed -- context independence implies that ML-style type inference can be performed using only the segmentation (because the remainder of an expansion cannot mention the very variables whose types are being inferred.) It should be possible to implicitly invoke TLMs based on the expected type in situations where the inference engine had enough information, but we leave this as an interesting direction for future work.

 Another critical distinction is that the metatheory presented by \citet{TSLs} establishes only that generated expansions are of the expected type (i.e. a variant of the Typed Expression Expansion theorem from Sec. \ref{sec:s-metatheory}.) It does not establish the remaining abstract reasoning principles that have been a major focus of this paper. In particular, there is no formal hygiene theorem (though it is discussed informally), and the mechanism does not guarantee that a valid segmentation will exist, nor associate types with segments. Finally, this prior work did not consider pattern matching, type functions, ML-style modules, parameters or static evaluation. All of these are important for integration into an ML- or Scala-like language, and the focus of most of this paper.

% These existing forms normally have other meanings, so this can be confusing \cite{pane1996usability}.

\vspace{-6px}
\newcommand{\discussionSec}{Discussion}
\section{\protect\discussionSec}
\label{sec:discussion}
\label{sec:conclusion}
\vspace{-2px}

The importance of specialized notation as a ``tool for thought'' has long been recognized \cite{DBLP:journals/cacm/Iverson80}. According to Whitehead, a good notation ``relieves the brain of unnecessary work'' and ``sets it free to concentrate on more advanced problems'' \cite{cajori1928history}, and indeed, advances in mathematics, science and programming have often been accompanied by new notation. 

Of course, this desire to ``relieve the brain of unnecessary work'' has motivated not only the syntax but also the semantics of languages like ML and Scala -- these languages maintain a strong type and binding discipline so that programmers, and their tools, can hold certain implementation details abstract when reasoning about program behavior.  In the words of \citet{B304}, ``type structure is a syntactic discipline for enforcing levels of abstraction.''

Previously, these two relief mechanisms were in tension -- mechanisms  that allowed programmers to express new notation would obscure the type and binding structure of the program text. TLMs resolve this tension for the broad class of literal forms that generalized literal forms subsume. This class includes all of the examples enumerated in Sec. \ref{sec:intro} (up to the choice of outermost delimiter), the additional examples described in Sec. \ref{sec:static-eval}, the more detailed example in the supplement, and the many examples in the prior papers by \citet{TSLs,sac15}. 

Of course, not all possible literal notation will prove to be in good taste, and we make no empirical claims about particular notation in this paper. TLMs leave the burden of establishing the value of any particular literal notation to individual library providers, rather than to the language designer. The reasoning principles that TLMs provide, which are the specific contributions of this paper, allow clients to "reason around" poor literal designs, using principles analagous to those already familiar to programmers in languages like these. % We must leave a detailed empirical evaluation of the impact of particular TLMs on various quality attributes, like programmer productivity and program comprehensibility, as future work. 
The technical content of this paper, which formally establishes these reasoning principles, constitutes the first detailed type-theoretic account of a typed, hygienic macro system for an ML-like language, i.e. one with a rich static type system, support for pattern matching and an ML-like module system with type functions and abstract type members.  Our own language integration efforts have been focused on an emerging alternative front-end for OCaml called Reason, but Scala supports analagous features \cite{conf/oopsla/AminRO14} and would also be a suitable host for the TLM mechanism. The intended audience for this paper is language designers seeking a reasonable solution to the problem of user-defined literal notation.% We also plan to implement TLMs into \li{typy}, a typed functional language embedded into Python as a library \cite{gpce/Omar16}.%We have evaluated this claim by stating the available abstract reasoning principles formally and proving that they hold for the calculi that we have described.

The examples in this paper are written in an open source alternative syntactic front-end for OCaml called Reason because we are incorporating TLMs into Reason. However, the essential ideas are not Reason-, OCaml- or ML-specific -- it should be possible to adapt TLMs to other languages that take a similar approach to types and binding (e.g. Haskell, Scala and others.) Languages that have a disciplined binding structure but that lack a rich static type structure, e.g. standard Racket, would also benefit from TLMs -- they can apply these results by deploying the usual trick of viewing the language as statically ``{unityped}'' \cite{pfpl,scott1980lambda}.

% The intended audience for this paper is language designers who want to decentralize control over literal forms but seek a detailed understanding, rooted in the first principles of type theory, of a reasonable mechanism by which to achieve that goal. 


%(To summarize Sec. \ref{sec:existing-approaches}, the closest existing typed, hygienic macro system -- the Scala macro system \cite{ScalaMacros2013} -- is not formally specified, the work of \cite{DBLP:conf/esop/HermanW08} uses types only to reason abstractly about binding, and the specification of TSLs in the work of \citet{TSLs} did not formalize hygiene nor handle these advanced language features.)

% Haskell's \li{do}-notation at types with monadic structure cannot be expressed directly using TLMs because it needs to purposefully induce capture in spliced sub-terms, and TLMs prevent capture. This limitation could be resolved by allowing identifiers that appear in literal bodies to be explicitly designated as bound within spliced expressions in the segmentation. The prior work by \citet{DBLP:conf/esop/HermanW08} considered a similar approach in a setting where the locations of sub-terms is known ahead of time (building on the \emph{tree locations} of \citet{gorn1965explicit}, which are conceptually similar to our spliced segment locations.)  We leave as future work the details of this proposal and a consideration of whether this additional expressive power would justify the increased reasoning complexity for clients (it is presently the author's opinion that it would not.)  

There are several interesting avenues for future work beyond those already discussed. A correct parse function never returns an encoding of a proto-expansion that fails validation given well-typed splices, but this invariant cannot be enforced by the ML type system.
% Using a proof assistant, it would be possible to verify that
% a parse function generates only encodings of valid proto-expansions. 
Under a
richer type system, the return type of the parse function itself could be refined so as to
enforce this invariant intrinsically. This problem -- of typed first-class term representations -- has been studied in a variety of settings, e.g. in MetaML \cite{Sheard:1999:UMS} and in the modal logic tradition \cite{DBLP:conf/popl/DaviesP96}. We leave integration of these approaches into the TLM mechanism as future work. Our initial efforts aim to leave the semantics of OCaml unchanged, consistent with the philosophy of the Reason project.

%Another topic of interest has to do with intentional capture. 
Another direction has to do with automated refactoring. The unexpanded language does not come with context-free notions of renaming and substitution. However, given a segmentation, it should be possible to ``backpatch'' refactorings into literal bodies. Recent work by \citet{wand2017inferring} on tracking bindings ``backwards'' from an expansion to the source program is likely relevant.

At several points in the paper, we allude to editor integration. However, several important questions having to do with TLM-specific syntax highlighting, incremental parsing and error recovery \cite{graham1979practical} remain to be considered. Another interesting direction would be to generalize TLMs to support non-textual notation in the setting of a structure editor. The semantic foundations for structure editors proposed recently by \citet{DBLP:conf/popl/OmarVHAH17} may guide such an effort.

Let us conclude by considering the oft-uttered phrase ``syntax doesn't matter'', which is generally taken to mean that syntactic concerns are orthogonal to various more important semantic concerns. Our hope is that the reader will leave this paper with an understanding that this is a rather simplistic proclamation. Instead, let us proclaim that ``semantics matter for syntax too!'' % We hope that this work will lead to more ``reasonable'' variants of other syntactic conveniences, e.g. the ubiquitious ``deriving'' clauses on datatype definitions.

% Other directions for future work are given in the supplement\todo{do this?}.

% Another interesting question is what this mechanism would look like in the setting of a projectional structure editor, i.e. one where the syntax is not textual but rather tree-shaped and projected in an interative manner to the programmer. This paper taken together with work by \citet{Omar:2012:ACC:2337223.2337324,DBLP:conf/popl/OmarVHAH17} can serve to guide such an effort.

% To conclude, TLMs give substantial syntactic control to library providers while leaving programmers with strong abstract reasoning principles. We believe TLMs therefore occupy a ``sweet spot'' in the  design space. %This paper serves to guide those who hope to incorporate TLMs into their future language designs.





%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}

\clearpage
%% Bibliography
\bibliography{../../../papers/research}


\end{document}
